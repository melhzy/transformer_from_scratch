{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df828d",
   "metadata": {},
   "source": [
    "## 1. Encoder Layer üî∑\n",
    "\n",
    "An encoder layer consists of:\n",
    "1. **Multi-Head Self-Attention** (with residual + layer norm)\n",
    "2. **Position-wise Feed-Forward** (with residual + layer norm)\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "For input $X \\in \\mathbb{R}^{\\text{seq_len} \\times d_{\\text{model}}}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z &= \\text{LayerNorm}(X + \\text{MultiHeadAttn}(X, X, X)) \\\\\n",
    "\\text{Output} &= \\text{LayerNorm}(Z + \\text{FFN}(Z))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Note**: This is **Pre-LN** (Layer Norm before sub-layer). Post-LN is also common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention from Tutorial 3\"\"\"\n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and apply output projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise FFN from Tutorial 4\"\"\"\n",
    "    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8, d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sub-layer 1: Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Sub-layer 2: Position-wise FFN\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: (batch, 1, 1, seq_len) for padding mask\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Self-attention with residual + layer norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Sub-layer 2: FFN with residual + layer norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "encoder_layer = EncoderLayer(d_model=512, n_heads=8)\n",
    "x = torch.randn(2, 10, 512)  # (batch=2, seq_len=10, d_model=512)\n",
    "output = encoder_layer(x)\n",
    "print(f\"Encoder Layer Input: {x.shape}\")\n",
    "print(f\"Encoder Layer Output: {output.shape}\")\n",
    "print(\"‚úÖ Single encoder layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2363a",
   "metadata": {},
   "source": [
    "## 2. Stacking Encoder Layers üìö\n",
    "\n",
    "The original Transformer stacks **N=6 identical encoder layers**.\n",
    "\n",
    "### Why Stack Layers?\n",
    "\n",
    "Each layer enables **iterative refinement**:\n",
    "- **Layer 1**: Basic patterns (syntax, word relationships)\n",
    "- **Layer 2-3**: Intermediate patterns (phrases, local dependencies)\n",
    "- **Layer 4-6**: Abstract patterns (semantic meaning, long-range dependencies)\n",
    "\n",
    "**Empirical Finding**: 6 layers is standard for base models, but deeper models (12, 24, 48 layers) often perform better with more data.\n",
    "\n",
    "### üß† DeepSeek Insight: Layer Specialization\n",
    "\n",
    "Research shows different layers specialize:\n",
    "- **Early layers**: Syntactic features (POS tags, dependency parsing)\n",
    "- **Middle layers**: Entity recognition, coreference\n",
    "- **Late layers**: Semantic similarity, reasoning\n",
    "\n",
    "This **hierarchical representation learning** is key to Transformer power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Stack of N Encoder Layers\"\"\"\n",
    "    def __init__(self, n_layers: int = 6, d_model: int = 512, n_heads: int = 8, \n",
    "                 d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stack of N identical layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)  # Final layer norm\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) - embedded input\n",
    "            mask: (batch, 1, 1, seq_len) - padding mask\n",
    "        \n",
    "        Returns:\n",
    "            (batch, seq_len, d_model) - encoded representations\n",
    "        \"\"\"\n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        return self.norm(x)\n",
    "\n",
    "# Test encoder stack\n",
    "encoder = Encoder(n_layers=6, d_model=512, n_heads=8)\n",
    "x = torch.randn(2, 10, 512)\n",
    "encoded = encoder(x)\n",
    "print(f\"Encoder Input: {x.shape}\")\n",
    "print(f\"Encoder Output: {encoded.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "print(\"‚úÖ Encoder stack works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec7910",
   "metadata": {},
   "source": [
    "## 3. Decoder Layer üî∂\n",
    "\n",
    "A decoder layer is more complex, with **3 sub-layers**:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention** (on output sequence)\n",
    "2. **Multi-Head Cross-Attention** (attending to encoder outputs)\n",
    "3. **Position-wise Feed-Forward**\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "For decoder input $Y \\in \\mathbb{R}^{\\text{tgt_len} \\times d_{\\text{model}}}$ and encoder output $E \\in \\mathbb{R}^{\\text{src_len} \\times d_{\\text{model}}}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_1 &= \\text{LayerNorm}(Y + \\text{MaskedSelfAttn}(Y, Y, Y)) \\\\\n",
    "Z_2 &= \\text{LayerNorm}(Z_1 + \\text{CrossAttn}(Z_1, E, E)) \\\\\n",
    "\\text{Output} &= \\text{LayerNorm}(Z_2 + \\text{FFN}(Z_2))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Key Differences from Encoder\n",
    "\n",
    "1. **Masked Self-Attention**: Prevents attending to future tokens (causal)\n",
    "2. **Cross-Attention**: Query from decoder, Key/Value from encoder\n",
    "3. **Autoregressive**: Generates one token at a time\n",
    "\n",
    "### üß† DeepSeek Insight: Cross-Attention as Information Bridge\n",
    "\n",
    "Cross-attention is the **key mechanism** for encoder-decoder communication:\n",
    "- **Queries**: \"What information do I need?\" (from decoder)\n",
    "- **Keys/Values**: \"What information is available?\" (from encoder)\n",
    "\n",
    "This allows the decoder to **selectively attend** to relevant parts of the input while generating output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b54505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8, d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sub-layer 1: Masked multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Sub-layer 2: Multi-head cross-attention (to encoder output)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Sub-layer 3: Position-wise FFN\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, tgt_len, d_model) - decoder input\n",
    "            encoder_output: (batch, src_len, d_model) - encoder output\n",
    "            src_mask: (batch, 1, 1, src_len) - padding mask for source\n",
    "            tgt_mask: (batch, 1, tgt_len, tgt_len) - causal mask for target\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Masked self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Sub-layer 2: Cross-attention to encoder output\n",
    "        # Query from decoder, Key/Value from encoder\n",
    "        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        \n",
    "        # Sub-layer 3: FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test decoder layer\n",
    "decoder_layer = DecoderLayer(d_model=512, n_heads=8)\n",
    "tgt = torch.randn(2, 8, 512)  # (batch=2, tgt_len=8, d_model=512)\n",
    "encoder_out = torch.randn(2, 10, 512)  # (batch=2, src_len=10, d_model=512)\n",
    "\n",
    "# Create causal mask for target (prevent attending to future)\n",
    "tgt_len = 8\n",
    "tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "output = decoder_layer(tgt, encoder_out, tgt_mask=tgt_mask)\n",
    "print(f\"Decoder Input: {tgt.shape}\")\n",
    "print(f\"Encoder Output: {encoder_out.shape}\")\n",
    "print(f\"Decoder Layer Output: {output.shape}\")\n",
    "print(\"‚úÖ Single decoder layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5cabb",
   "metadata": {},
   "source": [
    "## 4. Stacking Decoder Layers üìö\n",
    "\n",
    "Like the encoder, we stack **N=6 identical decoder layers**.\n",
    "\n",
    "Each decoder layer:\n",
    "1. Refines output representation\n",
    "2. Attends to encoder output (via cross-attention)\n",
    "3. Maintains causal structure (can't see future)\n",
    "\n",
    "### üß† DeepSeek Insight: Autoregressive Generation\n",
    "\n",
    "The decoder is **autoregressive**: it generates one token at a time.\n",
    "\n",
    "At time step $t$:\n",
    "- Input: tokens $[1, 2, ..., t-1]$\n",
    "- Output: prediction for token $t$\n",
    "- Causal mask prevents \"peeking\" at tokens $[t, t+1, ...]$\n",
    "\n",
    "This is why GPT (decoder-only) can generate coherent long text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db977e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Stack of N Decoder Layers\"\"\"\n",
    "    def __init__(self, n_layers: int = 6, d_model: int = 512, n_heads: int = 8,\n",
    "                 d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stack of N identical layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)  # Final layer norm\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, tgt_len, d_model) - decoder input embeddings\n",
    "            encoder_output: (batch, src_len, d_model) - encoder outputs\n",
    "            src_mask: (batch, 1, 1, src_len) - padding mask for source\n",
    "            tgt_mask: (batch, 1, tgt_len, tgt_len) - causal mask for target\n",
    "        \n",
    "        Returns:\n",
    "            (batch, tgt_len, d_model) - decoded representations\n",
    "        \"\"\"\n",
    "        # Pass through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        return self.norm(x)\n",
    "\n",
    "# Test decoder stack\n",
    "decoder = Decoder(n_layers=6, d_model=512, n_heads=8)\n",
    "tgt = torch.randn(2, 8, 512)\n",
    "encoder_out = torch.randn(2, 10, 512)\n",
    "tgt_mask = torch.tril(torch.ones(8, 8)).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "decoded = decoder(tgt, encoder_out, tgt_mask=tgt_mask)\n",
    "print(f\"Decoder Input: {tgt.shape}\")\n",
    "print(f\"Encoder Output: {encoder_out.shape}\")\n",
    "print(f\"Decoder Output: {decoded.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
    "print(\"‚úÖ Decoder stack works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda192f",
   "metadata": {},
   "source": [
    "## 5. Visualizing Encoder-Decoder Flow üìä\n",
    "\n",
    "Let's visualize how information flows through the encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f497f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example\n",
    "batch_size = 1\n",
    "src_len = 6\n",
    "tgt_len = 5\n",
    "d_model = 512\n",
    "\n",
    "# Dummy encoder and decoder\n",
    "encoder = Encoder(n_layers=3, d_model=d_model, n_heads=8)\n",
    "decoder = Decoder(n_layers=3, d_model=d_model, n_heads=8)\n",
    "\n",
    "# Input sequences\n",
    "src = torch.randn(batch_size, src_len, d_model)\n",
    "tgt = torch.randn(batch_size, tgt_len, d_model)\n",
    "\n",
    "# Create causal mask for target\n",
    "tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "# Forward pass\n",
    "encoder_output = encoder(src)\n",
    "decoder_output = decoder(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä ENCODER-DECODER INFORMATION FLOW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1Ô∏è‚É£ INPUT TO ENCODER:\")\n",
    "print(f\"   Shape: {src.shape}\")\n",
    "print(f\"   Description: Source sequence (e.g., English sentence)\\n\")\n",
    "\n",
    "print(f\"2Ô∏è‚É£ ENCODER PROCESSING:\")\n",
    "print(f\"   Layers: 3\")\n",
    "print(f\"   Each layer: Self-Attention + FFN\")\n",
    "print(f\"   Output shape: {encoder_output.shape}\")\n",
    "print(f\"   Description: Rich contextualized representations\\n\")\n",
    "\n",
    "print(f\"3Ô∏è‚É£ INPUT TO DECODER:\")\n",
    "print(f\"   Shape: {tgt.shape}\")\n",
    "print(f\"   Description: Target sequence so far (e.g., French translation)\\n\")\n",
    "\n",
    "print(f\"4Ô∏è‚É£ DECODER PROCESSING:\")\n",
    "print(f\"   Layers: 3\")\n",
    "print(f\"   Each layer: Masked Self-Attn + Cross-Attn + FFN\")\n",
    "print(f\"   Cross-Attention: Decoder queries encoder output\")\n",
    "print(f\"   Output shape: {decoder_output.shape}\")\n",
    "print(f\"   Description: Contextualized output representations\\n\")\n",
    "\n",
    "print(f\"5Ô∏è‚É£ FINAL PREDICTION:\")\n",
    "print(f\"   Linear layer: {d_model} ‚Üí vocab_size\")\n",
    "print(f\"   Softmax: Probability distribution over vocabulary\")\n",
    "print(f\"   Output: Next token prediction\\n\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ded9e4",
   "metadata": {},
   "source": [
    "## 6. Visualizing Causal Mask üé≠\n",
    "\n",
    "The **causal mask** is crucial for autoregressive generation. Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(size: int):\n",
    "    \"\"\"Create a causal (lower triangular) mask\"\"\"\n",
    "    mask = torch.tril(torch.ones(size, size))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "seq_len = 8\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Causal mask\n",
    "sns.heatmap(mask.numpy(), annot=True, fmt=\".0f\", cmap=\"Blues\", \n",
    "            cbar=False, square=True, ax=ax1,\n",
    "            xticklabels=[f\"t{i+1}\" for i in range(seq_len)],\n",
    "            yticklabels=[f\"t{i+1}\" for i in range(seq_len)])\n",
    "ax1.set_title(\"Causal Mask (1 = can attend, 0 = masked)\\n\", fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(\"Key Position (being attended to)\")\n",
    "ax1.set_ylabel(\"Query Position (attending from)\")\n",
    "\n",
    "# Plot 2: Attention pattern explanation\n",
    "ax2.axis('off')\n",
    "explanation = \"\"\"\n",
    "üéØ CAUSAL MASK INTERPRETATION\n",
    "\n",
    "Reading Row by Row (Query Position):\n",
    "\n",
    "‚Ä¢ t1 can attend to: [t1] only\n",
    "  ‚Üí Sees only the first token\n",
    "\n",
    "‚Ä¢ t2 can attend to: [t1, t2]\n",
    "  ‚Üí Sees tokens 1-2\n",
    "\n",
    "‚Ä¢ t3 can attend to: [t1, t2, t3]\n",
    "  ‚Üí Sees tokens 1-3\n",
    "\n",
    "‚Ä¢ t8 can attend to: [t1, t2, ..., t8]\n",
    "  ‚Üí Sees all previous tokens + itself\n",
    "\n",
    "üö´ KEY CONSTRAINT:\n",
    "No token can attend to future tokens!\n",
    "This ensures autoregressive generation.\n",
    "\n",
    "üí° WHY IT MATTERS:\n",
    "Prevents information leakage during training.\n",
    "Token t should predict t+1 using only [1...t].\n",
    "\"\"\"\n",
    "ax2.text(0.1, 0.5, explanation, fontsize=11, family='monospace',\n",
    "         verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Causal mask ensures tokens can only attend to past and present!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b035f70",
   "metadata": {},
   "source": [
    "## 7. üß† DeepSeek Insights: Why 6 Layers?\n",
    "\n",
    "### The Magic Number?\n",
    "\n",
    "The original Transformer used **N=6 layers** for both encoder and decoder. Why?\n",
    "\n",
    "**Empirical Finding** (Vaswani et al., 2017):\n",
    "- 6 layers achieved good performance on translation tasks\n",
    "- More layers (8, 10) didn't significantly improve results on available data\n",
    "- Training was stable with 6 layers\n",
    "\n",
    "### Modern Perspective\n",
    "\n",
    "**Deeper is often better** (with more data and compute):\n",
    "- **BERT-base**: 12 layers\n",
    "- **GPT-3**: 96 layers (175B parameters)\n",
    "- **GPT-4**: Rumored 120+ layers\n",
    "- **DeepSeek-V3**: 61 layers with MoE\n",
    "\n",
    "### Layer-wise Specialization\n",
    "\n",
    "Research shows **hierarchical processing**:\n",
    "\n",
    "```\n",
    "Layer 1-2:  Syntax (POS tags, dependency parsing)\n",
    "Layer 3-4:  Entities, coreference, relations\n",
    "Layer 5-6:  Semantics, reasoning, world knowledge\n",
    "```\n",
    "\n",
    "**DeepSeek Insight**: More layers enable **multi-hop reasoning**:\n",
    "- Each layer = one \"hop\" through knowledge\n",
    "- Complex reasoning requires multiple hops\n",
    "- Depth correlates with reasoning capability\n",
    "\n",
    "### Training Deep Transformers\n",
    "\n",
    "Challenges:\n",
    "- **Gradient vanishing/exploding**: Residual connections help\n",
    "- **Training instability**: Pre-LN more stable than Post-LN\n",
    "- **Overfitting**: More layers need more data\n",
    "\n",
    "Solutions:\n",
    "- **Pre-LN**: Apply LayerNorm before sub-layer (more stable)\n",
    "- **Warm-up**: Gradually increase learning rate\n",
    "- **Gradient clipping**: Prevent exploding gradients\n",
    "- **Regularization**: Dropout, weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01878b9d",
   "metadata": {},
   "source": [
    "## 8. Encoder-Only vs Decoder-Only vs Encoder-Decoder ü§î\n",
    "\n",
    "### Three Architectures\n",
    "\n",
    "**1. Encoder-Only (BERT, RoBERTa)**\n",
    "- Bidirectional attention (no causal mask)\n",
    "- Good for understanding tasks: classification, NER, QA\n",
    "- Cannot generate text autoregressively\n",
    "\n",
    "**2. Decoder-Only (GPT, LLaMA, DeepSeek)**\n",
    "- Causal attention (cannot see future)\n",
    "- Good for generation: text completion, chat, code\n",
    "- Also surprisingly good at understanding (with prompting)\n",
    "\n",
    "**3. Encoder-Decoder (T5, BART, Original Transformer)**\n",
    "- Encoder: bidirectional, Decoder: causal\n",
    "- Good for seq2seq: translation, summarization\n",
    "- More parameters for same capacity\n",
    "\n",
    "### Modern Trend: Decoder-Only Dominates\n",
    "\n",
    "Why are GPT-style models winning?\n",
    "1. **Simplicity**: One architecture for everything\n",
    "2. **Scaling**: Easier to scale to billions of parameters\n",
    "3. **Versatility**: Can do both understanding and generation\n",
    "4. **Prompting**: In-context learning enables any task\n",
    "\n",
    "**DeepSeek Insight**: Decoder-only models with enough scale can **simulate** encoder-decoder behavior through attention patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9177a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter counts\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "encoder_only = Encoder(n_layers=n_layers, d_model=d_model, n_heads=n_heads)\n",
    "decoder_only = Decoder(n_layers=n_layers, d_model=d_model, n_heads=n_heads)\n",
    "\n",
    "# Note: Decoder has cross-attention, so slightly more parameters\n",
    "encoder_params = sum(p.numel() for p in encoder_only.parameters())\n",
    "decoder_params = sum(p.numel() for p in decoder_only.parameters())\n",
    "encoder_decoder_params = encoder_params + decoder_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1Ô∏è‚É£ Encoder-Only (BERT-style):\")\n",
    "print(f\"   Parameters: {encoder_params:,}\")\n",
    "print(f\"   Attention: Bidirectional\")\n",
    "print(f\"   Use case: Classification, NER, QA\\n\")\n",
    "\n",
    "print(f\"2Ô∏è‚É£ Decoder-Only (GPT-style):\")\n",
    "print(f\"   Parameters: {decoder_params:,}\")\n",
    "print(f\"   Attention: Causal (autoregressive)\")\n",
    "print(f\"   Use case: Text generation, chat, code\\n\")\n",
    "\n",
    "print(f\"3Ô∏è‚É£ Encoder-Decoder (T5-style):\")\n",
    "print(f\"   Parameters: {encoder_decoder_params:,}\")\n",
    "print(f\"   Attention: Encoder=bidirectional, Decoder=causal\")\n",
    "print(f\"   Use case: Translation, summarization, seq2seq\\n\")\n",
    "\n",
    "print(f\"üìà Parameter Comparison:\")\n",
    "print(f\"   Encoder-Decoder is ~{encoder_decoder_params/decoder_params:.1f}x larger than Decoder-Only\")\n",
    "print(f\"   (for same number of layers)\\n\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50f775",
   "metadata": {},
   "source": [
    "## 9. Summary & Key Takeaways üìù\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Encoder Layer**: Self-attention + FFN (2 sub-layers)  \n",
    "‚úÖ **Decoder Layer**: Masked self-attention + Cross-attention + FFN (3 sub-layers)  \n",
    "‚úÖ **Stacking**: N=6 layers enables hierarchical processing  \n",
    "‚úÖ **Cross-Attention**: Enables encoder-decoder communication  \n",
    "‚úÖ **Causal Masking**: Ensures autoregressive generation  \n",
    "‚úÖ **Architecture Variants**: Encoder-only, Decoder-only, Encoder-decoder  \n",
    "\n",
    "### Critical Insights\n",
    "\n",
    "1. **Depth enables reasoning**: More layers = more processing steps\n",
    "2. **Cross-attention is key**: Connects encoder and decoder\n",
    "3. **Causal masking is crucial**: Prevents information leakage\n",
    "4. **Layer specialization**: Different layers learn different features\n",
    "5. **Architecture choice matters**: Depends on task and scale\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Tutorial 6**, we'll:\n",
    "- Assemble the complete Transformer (encoder + decoder + embeddings)\n",
    "- Add input/output projections\n",
    "- Implement generation strategies\n",
    "- Train on a real task!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Exercises\n",
    "\n",
    "1. **Implement Pre-LN vs Post-LN**: Compare training stability\n",
    "2. **Visualize Layer-wise Representations**: Use t-SNE on layer outputs\n",
    "3. **Vary Number of Layers**: How does depth affect performance?\n",
    "4. **Implement Encoder-Only Model**: Remove decoder, test on classification\n",
    "5. **Implement Decoder-Only Model**: Remove encoder, test on generation\n",
    "6. **Analyze Cross-Attention**: Visualize which source tokens decoder attends to\n",
    "7. **Compare Architectures**: Train all 3 variants on same task\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! üéâ You now understand the complete encoder-decoder architecture!**\n",
    "\n",
    "Next: [Tutorial 6: Complete Transformer & Training](06_complete_transformer.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
