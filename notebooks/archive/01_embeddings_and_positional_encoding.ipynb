{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5e98c4",
   "metadata": {},
   "source": [
    "# üìö Tutorial 1: Embeddings & Positional Encoding\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand why we need embeddings in deep learning\n",
    "- Learn how token embeddings convert discrete symbols to continuous vectors\n",
    "- Explore positional encodings and why they matter for Transformers\n",
    "- Visualize embedding spaces and positional patterns\n",
    "- Incorporate DeepSeek-R1 insights on representation learning\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction: From Symbols to Vectors](#intro)\n",
    "2. [Token Embeddings: The Foundation](#embeddings)\n",
    "3. [Positional Encoding: Adding Order](#positional)\n",
    "4. [DeepSeek Insights: Why This Matters](#deepseek)\n",
    "5. [Hands-On Implementation](#implementation)\n",
    "6. [Visualization & Analysis](#visualization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546730ec",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.modules.embeddings import TokenEmbedding\n",
    "from src.modules.positional_encoding import PositionalEncoding, LearnedPositionalEncoding\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee3d2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction: From Symbols to Vectors <a id=\"intro\"></a>\n",
    "\n",
    "### The Problem: Neural Networks Need Numbers\n",
    "\n",
    "Neural networks can't directly process words, characters, or symbols. They work with **continuous numerical vectors** (tensors). This creates a fundamental challenge:\n",
    "\n",
    "**How do we convert discrete symbols into meaningful continuous representations?**\n",
    "\n",
    "Consider the sentence: `\"The cat sat on the mat\"`\n",
    "\n",
    "- **Input:** `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]` (discrete tokens)\n",
    "- **Need:** `[[0.2, -0.5, ...], [0.1, 0.8, ...], ...]` (continuous vectors)\n",
    "\n",
    "### Why Not Just Use One-Hot Encoding?\n",
    "\n",
    "Let's see why simple one-hot encoding fails for large vocabularies:\n",
    "\n",
    "```python\n",
    "# One-hot encoding example\n",
    "vocab = [\"cat\", \"dog\", \"bird\", \"fish\"]\n",
    "# \"cat\" = [1, 0, 0, 0]\n",
    "# \"dog\" = [0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "**Problems with One-Hot:**\n",
    "1. ‚ùå **Sparse & Inefficient:** For vocab_size=50,000, each word is a 50,000-dimensional vector (99.998% zeros!)\n",
    "2. ‚ùå **No Semantic Meaning:** \"cat\" and \"dog\" are equally different from each other as \"cat\" and \"democracy\"\n",
    "3. ‚ùå **Cannot Capture Relations:** No way to represent that \"king\" - \"man\" + \"woman\" ‚âà \"queen\"\n",
    "\n",
    "**Solution: Dense Embeddings** üéØ\n",
    "- Map each token to a **learned dense vector** (typically 256-1024 dimensions)\n",
    "- Similar words have similar vectors\n",
    "- Captures semantic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c443b9f",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Let's compare one-hot vs dense embeddings\n",
    "vocab_size = 10000\n",
    "d_model = 512  # embedding dimension\n",
    "\n",
    "# One-hot: 10000 dimensions per token (sparse)\n",
    "one_hot_size = vocab_size\n",
    "print(f\"One-hot encoding size: {one_hot_size:,} dimensions per token\")\n",
    "print(f\"  ‚Üí For a 100-token sentence: {one_hot_size * 100:,} total values\")\n",
    "print(f\"  ‚Üí Memory: ~{(one_hot_size * 100 * 4) / 1024:.1f} KB (float32)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Dense embedding: 512 dimensions per token (dense)\n",
    "dense_size = d_model\n",
    "print(f\"Dense embedding size: {dense_size} dimensions per token\")\n",
    "print(f\"  ‚Üí For a 100-token sentence: {dense_size * 100:,} total values\")\n",
    "print(f\"  ‚Üí Memory: ~{(dense_size * 100 * 4) / 1024:.1f} KB (float32)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(f\"‚úÖ Dense embeddings are {one_hot_size // dense_size:.1f}x more compact!\")\n",
    "print(f\"‚úÖ Plus they capture semantic meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c5dd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Token Embeddings: The Foundation <a id=\"embeddings\"></a>\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "An embedding layer is essentially a **learnable lookup table**:\n",
    "\n",
    "$$\\text{Embedding}: \\mathbb{Z}_{V} \\rightarrow \\mathbb{R}^{d_{model}}$$\n",
    "\n",
    "Where:\n",
    "- $V$ = vocabulary size (e.g., 50,000 words)\n",
    "- $d_{model}$ = embedding dimension (e.g., 512)\n",
    "\n",
    "For a token with index $i$, the embedding is the $i$-th row of the embedding matrix $E \\in \\mathbb{R}^{V \\times d_{model}}$:\n",
    "\n",
    "$$\\text{emb}(i) = E[i, :] \\in \\mathbb{R}^{d_{model}}$$\n",
    "\n",
    "### The Attention Paper's Scaling Factor\n",
    "\n",
    "In \"Attention Is All You Need\", embeddings are **scaled by** $\\sqrt{d_{model}}$:\n",
    "\n",
    "$$\\text{scaled\\_emb}(i) = \\sqrt{d_{model}} \\cdot E[i, :]$$\n",
    "\n",
    "**Why?** This scaling ensures:\n",
    "1. Embeddings and positional encodings have similar magnitudes\n",
    "2. Better gradient flow during training\n",
    "3. Prevents attention logits from growing too large\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "Let's see our `TokenEmbedding` class in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf1226",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple embedding layer\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "padding_idx = 0  # Reserve index 0 for padding\n",
    "\n",
    "embedding = TokenEmbedding(vocab_size=vocab_size, d_model=d_model, padding_idx=padding_idx)\n",
    "\n",
    "print(f\"üì¶ Created TokenEmbedding:\")\n",
    "print(f\"  - Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  - Embedding dimension: {d_model}\")\n",
    "print(f\"  - Padding index: {padding_idx}\")\n",
    "print(f\"  - Total parameters: {vocab_size * d_model:,}\")\n",
    "print(f\"  - Scaling factor: ‚àö{d_model} = {np.sqrt(d_model):.2f}\")\n",
    "\n",
    "# Test with a sequence of tokens\n",
    "token_ids = torch.tensor([[1, 42, 7, 99, 0],    # First sentence (0 is padding)\n",
    "                          [15, 88, 0, 0, 0]])    # Second sentence (padded)\n",
    "\n",
    "print(f\"\\nüìù Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Token IDs:\\n{token_ids}\")\n",
    "\n",
    "# Get embeddings\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(f\"\\nüéØ Output embeddings shape: {embedded.shape}\")\n",
    "print(f\"  - Batch size: {embedded.shape[0]}\")\n",
    "print(f\"  - Sequence length: {embedded.shape[1]}\")\n",
    "print(f\"  - Embedding dimension: {embedded.shape[2]}\")\n",
    "\n",
    "# Check padding is zeroed out\n",
    "print(f\"\\nüîç Padding check (should be all zeros):\")\n",
    "print(f\"  - Position [0, 4] (padding): {embedded[0, 4, :5].detach().numpy()}\")\n",
    "print(f\"  - Position [1, 2] (padding): {embedded[1, 2, :5].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7535e7",
   "metadata": {},
   "source": [
    "### Visualizing the Embedding Space\n",
    "\n",
    "Let's visualize how embeddings distribute in high-dimensional space. We'll use PCA to reduce dimensions for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d15fc",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get embedding matrix (excluding padding token)\n",
    "embedding_matrix = embedding.embedding.weight.detach().numpy()[1:500]  # Sample 500 tokens\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, s=30)\n",
    "plt.title(f'Token Embedding Space (PCA projection)\\n{vocab_size} tokens ‚Üí {d_model}D ‚Üí 2D', fontsize=14)\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight a few random tokens\n",
    "highlight_indices = np.random.choice(len(embeddings_2d), 10, replace=False)\n",
    "plt.scatter(embeddings_2d[highlight_indices, 0], \n",
    "           embeddings_2d[highlight_indices, 1], \n",
    "           c='red', s=100, marker='*', label='Sample tokens', zorder=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Total variance explained by 2 components: {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n",
    "print(f\"üí° The embeddings form a dense cloud in high-dimensional space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fac5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Positional Encoding: Adding Order <a id=\"positional\"></a>\n",
    "\n",
    "### The Problem: Transformers Have No Inherent Sense of Order\n",
    "\n",
    "Unlike RNNs which process sequences step-by-step, **Transformers process all tokens in parallel**. This creates a problem:\n",
    "\n",
    "```\n",
    "\"The cat chased the dog\" \n",
    "vs \n",
    "\"The dog chased the cat\"\n",
    "```\n",
    "\n",
    "Without positional information, these would look identical to the Transformer! ü§î\n",
    "\n",
    "### Solution: Positional Encoding\n",
    "\n",
    "We **add** positional information to the embeddings:\n",
    "\n",
    "$$\\text{input} = \\text{token\\_embedding} + \\text{positional\\_encoding}$$\n",
    "\n",
    "### Two Approaches\n",
    "\n",
    "#### 1. **Sinusoidal Positional Encoding** (Original Paper)\n",
    "\n",
    "Uses fixed sine/cosine functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $pos$ = position in sequence (0, 1, 2, ...)\n",
    "- $i$ = dimension index (0 to $d_{model}/2$)\n",
    "- Even dimensions use sine, odd dimensions use cosine\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ No learned parameters (model can generalize to longer sequences)\n",
    "- ‚úÖ Allows model to learn relative positions (due to linear properties of sin/cos)\n",
    "- ‚úÖ Each position has a unique encoding\n",
    "\n",
    "#### 2. **Learned Positional Encoding**\n",
    "\n",
    "Treats positional encodings as learnable parameters (like embeddings).\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Can adapt to task-specific patterns\n",
    "- ‚úÖ Sometimes performs better on fixed-length tasks\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Cannot extrapolate beyond max training length\n",
    "\n",
    "Let's implement and visualize both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25d05f",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Create sinusoidal positional encoding\n",
    "max_len = 100\n",
    "pe_sinusoidal = PositionalEncoding(d_model=d_model, max_len=max_len, dropout=0.0)\n",
    "\n",
    "# Create learned positional encoding\n",
    "pe_learned = LearnedPositionalEncoding(d_model=d_model, max_len=max_len, dropout=0.0)\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 50, d_model)  # (batch, seq_len, d_model)\n",
    "\n",
    "# Apply both types\n",
    "output_sin = pe_sinusoidal(dummy_input)\n",
    "output_learned = pe_learned(dummy_input)\n",
    "\n",
    "print(\"üìç Positional Encoding Comparison:\")\n",
    "print(f\"\\nSinusoidal PE:\")\n",
    "print(f\"  - Parameters: 0 (fixed)\")\n",
    "print(f\"  - Max sequence length: {max_len}\")\n",
    "print(f\"  - Can extrapolate: Yes\")\n",
    "\n",
    "print(f\"\\nLearned PE:\")\n",
    "print(f\"  - Parameters: {max_len * d_model:,}\")\n",
    "print(f\"  - Max sequence length: {max_len}\")\n",
    "print(f\"  - Can extrapolate: No\")\n",
    "\n",
    "print(f\"\\n‚úÖ Both add positional information to the {d_model}-dimensional embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec11098",
   "metadata": {},
   "source": [
    "### Visualizing Positional Encodings\n",
    "\n",
    "Let's visualize the sinusoidal patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062a95c",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Get the positional encoding matrix\n",
    "pe_matrix = pe_sinusoidal.pe[0].detach().numpy()  # Shape: (max_len, d_model)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Heatmap of positional encodings\n",
    "ax1 = axes[0, 0]\n",
    "im = ax1.imshow(pe_matrix[:50].T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('Position in Sequence')\n",
    "ax1.set_ylabel('Embedding Dimension')\n",
    "ax1.set_title('Sinusoidal Positional Encoding Heatmap\\n(First 50 positions)')\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# 2. Wave patterns for different dimensions\n",
    "ax2 = axes[0, 1]\n",
    "positions = np.arange(max_len)\n",
    "for dim in [0, 1, 8, 16, 32, 64]:\n",
    "    if dim < d_model:\n",
    "        ax2.plot(positions, pe_matrix[:, dim], label=f'Dim {dim}', alpha=0.7)\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Encoding Value')\n",
    "ax2.set_title('Positional Encoding Waves\\n(Different dimensions have different frequencies)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Encoding for specific positions\n",
    "ax3 = axes[1, 0]\n",
    "for pos in [0, 10, 25, 50]:\n",
    "    ax3.plot(pe_matrix[pos], label=f'Position {pos}', alpha=0.7)\n",
    "ax3.set_xlabel('Dimension')\n",
    "ax3.set_ylabel('Encoding Value')\n",
    "ax3.set_title('Positional Encoding Vectors\\n(Each position has unique pattern)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distance between positions (cosine similarity)\n",
    "ax4 = axes[1, 1]\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(pe_matrix[:50])\n",
    "im2 = ax4.imshow(similarities, cmap='viridis')\n",
    "ax4.set_xlabel('Position')\n",
    "ax4.set_ylabel('Position')\n",
    "ax4.set_title('Cosine Similarity Between Positions\\n(Darker = More similar)')\n",
    "plt.colorbar(im2, ax=ax4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations:\")\n",
    "print(\"  1. Different dimensions oscillate at different frequencies\")\n",
    "print(\"  2. Nearby positions have similar encodings (smooth gradient)\")\n",
    "print(\"  3. Each position gets a unique pattern across all dimensions\")\n",
    "print(\"  4. The pattern allows the model to learn relative positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1222d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. DeepSeek Insights: Why This Matters <a id=\"deepseek\"></a>\n",
    "\n",
    "### üî¨ DeepSeek-R1 Perspective on Embeddings\n",
    "\n",
    "**DeepSeek-R1**, with its advanced reasoning capabilities, highlights several key insights about embeddings and positional encodings:\n",
    "\n",
    "#### 1. **Representation Learning is Foundation**\n",
    "\n",
    "> \"High-quality representations are the bedrock of reasoning. Without rich, semantic embeddings, downstream attention mechanisms cannot discover meaningful patterns.\"\n",
    "\n",
    "**What this means:**\n",
    "- Embeddings create the \"vocabulary\" of features that attention operates on\n",
    "- Poor embeddings ‚Üí Poor attention ‚Üí Poor reasoning\n",
    "- The $\\sqrt{d_{model}}$ scaling prevents gradient vanishing\n",
    "\n",
    "#### 2. **Positional Encodings Enable Relational Reasoning**\n",
    "\n",
    "> \"Absolute position is less important than relative relationships. Sinusoidal encodings allow the model to learn: 'this word is 3 positions after that word' regardless of their absolute positions.\"\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "\"The cat sat on the mat\"\n",
    "     ‚Üì   ‚Üì\n",
    "\"cat\" and \"sat\" are always adjacent\n",
    "\"cat\" and \"mat\" are always 4 positions apart\n",
    "```\n",
    "\n",
    "The model can learn these **relative patterns** because:\n",
    "\n",
    "$$PE_{pos+k} = f(PE_{pos}, k)$$\n",
    "\n",
    "Due to sine/cosine addition formulas.\n",
    "\n",
    "#### 3. **Emergent Structure in Embedding Space**\n",
    "\n",
    "> \"Well-trained embeddings naturally cluster by semantic similarity, part-of-speech, and syntactic function‚Äîeven though we never explicitly trained for this!\"\n",
    "\n",
    "This is **representation learning** at work:\n",
    "- Similar contexts ‚Üí Similar embeddings\n",
    "- The model discovers linguistic structure automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5f583",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate relative position learning\n",
    "# The key property: PE(pos + k) can be represented as a linear function of PE(pos)\n",
    "\n",
    "pos1 = 10\n",
    "pos2 = 13  # 3 positions later\n",
    "\n",
    "pe1 = pe_matrix[pos1]\n",
    "pe2 = pe_matrix[pos2]\n",
    "\n",
    "# Calculate the \"difference\" in positional encoding\n",
    "diff = pe2 - pe1\n",
    "\n",
    "print(\"üîç DeepSeek Insight: Relative Position Encoding\")\n",
    "print(f\"\\nPosition {pos1} encoding (first 10 dims): {pe1[:10]}\")\n",
    "print(f\"Position {pos2} encoding (first 10 dims): {pe2[:10]}\")\n",
    "print(f\"\\nDifference (first 10 dims): {diff[:10]}\")\n",
    "\n",
    "# Check another pair with same relative distance\n",
    "pos3 = 25\n",
    "pos4 = 28  # Also 3 positions later\n",
    "\n",
    "pe3 = pe_matrix[pos3]\n",
    "pe4 = pe_matrix[pos4]\n",
    "diff2 = pe4 - pe3\n",
    "\n",
    "print(f\"\\n‚ú® Same relative distance (3 positions):\")\n",
    "print(f\"Position {pos3} ‚Üí {pos4} difference (first 10 dims): {diff2[:10]}\")\n",
    "\n",
    "# The differences should have similar patterns (not identical due to non-linearity, but related)\n",
    "correlation = np.corrcoef(diff, diff2)[0, 1]\n",
    "print(f\"\\nüìä Correlation between the two 3-step differences: {correlation:.4f}\")\n",
    "print(f\"üí° The model can learn: 'words that are 3 apart have this relationship'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf506e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hands-On Implementation <a id=\"implementation\"></a>\n",
    "\n",
    "### Building a Complete Input Pipeline\n",
    "\n",
    "Let's put it all together: **Embeddings + Positional Encoding = Transformer Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b1d17",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerInputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete input embedding for Transformer:\n",
    "    1. Token embedding (with scaling)\n",
    "    2. Positional encoding\n",
    "    3. Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len=5000, dropout=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model, padding_idx)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token IDs [batch_size, seq_len]\n",
    "        Returns:\n",
    "            Embedded input [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Step 1: Convert tokens to embeddings (includes ‚àöd_model scaling)\n",
    "        token_emb = self.token_embedding(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        output = self.positional_encoding(token_emb)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create the complete input embedding\n",
    "input_embedding = TransformerInputEmbedding(\n",
    "    vocab_size=10000,\n",
    "    d_model=512,\n",
    "    max_len=200,\n",
    "    dropout=0.1,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "# Test with a sample sentence\n",
    "# Let's simulate: \"The cat sat on the mat <PAD> <PAD>\"\n",
    "sample_tokens = torch.tensor([[45, 123, 87, 56, 89, 234, 0, 0]])  # [1, 8]\n",
    "\n",
    "print(\"üé¨ Complete Input Pipeline Demo\")\n",
    "print(f\"\\nInput tokens shape: {sample_tokens.shape}\")\n",
    "print(f\"Input tokens: {sample_tokens[0].tolist()}\")\n",
    "\n",
    "# Pass through the pipeline\n",
    "embedded_input = input_embedding(sample_tokens)\n",
    "\n",
    "print(f\"\\n‚ú® Final embedded input shape: {embedded_input.shape}\")\n",
    "print(f\"   - Ready for Transformer encoder/decoder!\")\n",
    "\n",
    "# Verify the components\n",
    "print(f\"\\nüì¶ What happened:\")\n",
    "print(f\"   1. Tokens ‚Üí Dense vectors ({input_embedding.d_model}D)\")\n",
    "print(f\"   2. Scaled by ‚àö{input_embedding.d_model} = {np.sqrt(input_embedding.d_model):.2f}\")\n",
    "print(f\"   3. Added position information\")\n",
    "print(f\"   4. Applied dropout (p=0.1)\")\n",
    "\n",
    "# Show first few dimensions of first two tokens\n",
    "print(f\"\\nüîç First token embedding (first 8 dims):\")\n",
    "print(f\"   {embedded_input[0, 0, :8].detach().numpy()}\")\n",
    "print(f\"\\nüîç Second token embedding (first 8 dims):\")\n",
    "print(f\"   {embedded_input[0, 1, :8].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8711695",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualization & Analysis <a id=\"visualization\"></a>\n",
    "\n",
    "### Comparing Token Embeddings vs. Token + Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff5d8c",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Create a sequence and compare before/after positional encoding\n",
    "test_tokens = torch.tensor([[10, 20, 30, 40, 50]])  # 5 different tokens\n",
    "\n",
    "# Get token embeddings only (no positional encoding)\n",
    "token_emb_only = input_embedding.token_embedding(test_tokens)\n",
    "\n",
    "# Get complete embeddings (token + position)\n",
    "input_embedding.eval()  # Turn off dropout for visualization\n",
    "with torch.no_grad():\n",
    "    complete_emb = input_embedding(test_tokens)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Before: Token embeddings only\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(token_emb_only[0].T.numpy(), aspect='auto', cmap='coolwarm')\n",
    "ax1.set_xlabel('Position in Sequence')\n",
    "ax1.set_ylabel('Embedding Dimension')\n",
    "ax1.set_title('Token Embeddings Only\\n(No position information)')\n",
    "ax1.set_xticks(range(5))\n",
    "ax1.set_xticklabels(['Pos 0', 'Pos 1', 'Pos 2', 'Pos 3', 'Pos 4'])\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# After: Token + Positional embeddings\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(complete_emb[0].T.numpy(), aspect='auto', cmap='coolwarm')\n",
    "ax2.set_xlabel('Position in Sequence')\n",
    "ax2.set_ylabel('Embedding Dimension')\n",
    "ax2.set_title('Token + Positional Embeddings\\n(Position information encoded)')\n",
    "ax2.set_xticks(range(5))\n",
    "ax2.set_xticklabels(['Pos 0', 'Pos 1', 'Pos 2', 'Pos 3', 'Pos 4'])\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Differences:\")\n",
    "print(\"  Left: Only semantic (token) information\")\n",
    "print(\"  Right: Semantic + positional information\")\n",
    "print(\"  ‚Üí Now each position has a unique signature!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea2f42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Token Embeddings**\n",
    "   - Convert discrete tokens to continuous vectors\n",
    "   - Much more efficient and expressive than one-hot encoding\n",
    "   - Scaled by ‚àöd_model to maintain gradient flow\n",
    "   - Learn semantic relationships automatically\n",
    "\n",
    "2. **Positional Encoding**\n",
    "   - Essential for Transformers (which process in parallel)\n",
    "   - Sinusoidal: Fixed, generalizes to any length, enables relative position learning\n",
    "   - Learned: Adaptive but limited to training length\n",
    "   - Added to token embeddings, not concatenated\n",
    "\n",
    "3. **DeepSeek Insights**\n",
    "   - High-quality representations enable reasoning\n",
    "   - Relative positions more important than absolute\n",
    "   - Emergent structure from optimization\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\\text{TransformerInput}(x) = \\text{TokenEmb}(x) \\cdot \\sqrt{d_{model}} + \\text{PosEnc}(x)$$\n",
    "\n",
    "Where:\n",
    "- $x \\in \\mathbb{Z}^{B \\times L}$ (batch of token sequences)\n",
    "- Output $\\in \\mathbb{R}^{B \\times L \\times d_{model}}$\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Tutorial 2: Attention Mechanisms**, we'll see how these rich embeddings are used by:\n",
    "- **Scaled Dot-Product Attention**: Computing relevance between positions\n",
    "- **Multi-Head Attention**: Parallel attention with different perspectives\n",
    "- **Self-Attention vs Cross-Attention**: Different attention patterns\n",
    "\n",
    "The embeddings we created here are the **input** to those attention mechanisms!\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Exercises\n",
    "\n",
    "Try these experiments to deepen your understanding:\n",
    "\n",
    "1. **Change d_model**: Try 256, 512, 1024. How does it affect the representation capacity?\n",
    "\n",
    "2. **Visualize Learned PE**: Train the `LearnedPositionalEncoding` on a dummy task and visualize the learned patterns\n",
    "\n",
    "3. **Embedding Similarity**: Use `TokenEmbedding.compute_embedding_similarity()` to find which tokens are most similar\n",
    "\n",
    "4. **Longer Sequences**: Test sinusoidal PE with sequences longer than `max_len` during training\n",
    "\n",
    "5. **2D Positional Encoding**: Extend to 2D (for images) by encoding both row and column positions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
