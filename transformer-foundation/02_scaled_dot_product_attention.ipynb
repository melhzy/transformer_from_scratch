{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3181cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.modules.attention import ScaledDotProductAttention\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b0326",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Attention Revolution <a id=\"revolution\"></a>\n",
    "\n",
    "### The Problem with RNNs\n",
    "\n",
    "Before Transformers, sequence modeling was dominated by RNNs (LSTM, GRU):\n",
    "\n",
    "```\n",
    "Input:  \"The cat sat on the mat\"\n",
    "RNN:    ‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üí \n",
    "        h‚ÇÅ h‚ÇÇ h‚ÇÉ h‚ÇÑ h‚ÇÖ h‚ÇÜ\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "1. ‚ùå **Sequential Processing**: Must process tokens one-by-one (slow, can't parallelize)\n",
    "2. ‚ùå **Long-Range Dependencies**: Information from early tokens gets diluted\n",
    "3. ‚ùå **Fixed Context**: Hidden state must compress entire history\n",
    "\n",
    "### The Attention Solution\n",
    "\n",
    "**Key Idea:** Let each token directly attend to (look at) all other tokens!\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat\"\n",
    " ‚Üï   ‚Üï   ‚Üï   ‚Üï   ‚Üï   ‚Üï\n",
    " ‚Üê‚Üí  ‚Üê‚Üí  ‚Üê‚Üí  ‚Üê‚Üí  ‚Üê‚Üí  ‚Üê‚Üí  (Each token can attend to any other)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Parallel Processing**: All tokens processed simultaneously\n",
    "- ‚úÖ **Direct Connections**: Any token can attend to any other (no information loss)\n",
    "- ‚úÖ **Dynamic Context**: Different queries attend differently\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "Attention answers: **\"Where should I look to understand this word?\"**\n",
    "\n",
    "Example: *\"The animal didn't cross the street because **it** was too tired\"*\n",
    "\n",
    "When processing \"**it**\", attention might:\n",
    "- Look strongly at \"**animal**\" (high attention weight)\n",
    "- Look weakly at \"street\" (low attention weight)\n",
    "- Determine \"it\" = \"animal\", not \"street\"\n",
    "\n",
    "This is **learned automatically** from data! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b21ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between RNN and Attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RNN: Sequential processing\n",
    "ax1 = axes[0]\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "for i, token in enumerate(tokens):\n",
    "    ax1.text(i, 0.5, token, ha='center', va='center', fontsize=14, weight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "    if i < len(tokens) - 1:\n",
    "        ax1.arrow(i+0.3, 0.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black')\n",
    "\n",
    "ax1.set_xlim(-0.5, len(tokens))\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('RNN: Sequential Processing\\n(One token at a time)', fontsize=14, weight='bold')\n",
    "\n",
    "# Attention: All-to-all connections\n",
    "ax2 = axes[1]\n",
    "positions = np.arange(len(tokens))\n",
    "for i, token in enumerate(tokens):\n",
    "    ax2.text(i, 0.5, token, ha='center', va='center', fontsize=14, weight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "    # Draw connections to all other tokens\n",
    "    for j in range(len(tokens)):\n",
    "        if i != j:\n",
    "            ax2.plot([i, j], [0.5, 0.5], 'gray', alpha=0.2, linewidth=1)\n",
    "\n",
    "ax2.set_xlim(-0.5, len(tokens))\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Attention: All-to-All Connections\\n(Parallel processing)', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîë Key Difference:\")\n",
    "print(\"  RNN: Token 5 must pass through tokens 1-4 to see token 0\")\n",
    "print(\"  Attention: Token 5 directly attends to token 0 (no intermediary!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d232492",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Query, Key, Value Intuition <a id=\"qkv\"></a>\n",
    "\n",
    "### The Database Analogy\n",
    "\n",
    "Think of attention like a **database lookup**:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\" (your search query)\n",
    "- **Key (K)**: \"What does each item offer?\" (database indices)\n",
    "- **Value (V)**: \"What information does each item contain?\" (database content)\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "Imagine searching a library:\n",
    "\n",
    "```python\n",
    "# You want to learn about \"machine learning\"\n",
    "Query = \"machine learning concepts\"\n",
    "\n",
    "# Books in the library:\n",
    "Keys = [\n",
    "    \"deep learning and neural networks\",  # High similarity!\n",
    "    \"cooking recipes for beginners\",       # Low similarity\n",
    "    \"artificial intelligence overview\",    # Medium similarity\n",
    "    \"gardening tips and tricks\"            # Low similarity\n",
    "]\n",
    "\n",
    "Values = [\n",
    "    \"<content of deep learning book>\",\n",
    "    \"<content of cooking book>\",\n",
    "    \"<content of AI book>\",\n",
    "    \"<content of gardening book>\"\n",
    "]\n",
    "\n",
    "# Attention computes:\n",
    "# 1. Similarity between Query and each Key\n",
    "# 2. Weighted sum of Values based on similarities\n",
    "```\n",
    "\n",
    "### In Transformers\n",
    "\n",
    "For a sequence \"The cat sat\":\n",
    "\n",
    "When processing \"sat\":\n",
    "- **Query**: \"sat\" asks \"what words are relevant to me?\"\n",
    "- **Keys**: Each word (\"The\", \"cat\", \"sat\") offers what it represents\n",
    "- **Values**: The actual semantic content of each word\n",
    "\n",
    "**Attention determines:** \"sat\" should pay attention to \"cat\" (the subject!)\n",
    "\n",
    "### Mathematical Projection\n",
    "\n",
    "Q, K, V are **learned linear projections** of the input:\n",
    "\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "Where:\n",
    "- $X \\in \\mathbb{R}^{n \\times d_{model}}$ (input sequence)\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d_{model} \\times d_k}$ (learned weight matrices)\n",
    "- The model learns **what to query**, **what to key on**, **what to value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5040e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Q, K, V projections\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "d_k = 64\n",
    "\n",
    "# Simulated input (e.g., embedded tokens)\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Learnable projection matrices\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "# Project to Q, K, V\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(\"üîß Query, Key, Value Projections\\n\")\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"  ‚Üí [batch_size, seq_len, d_model]\\n\")\n",
    "\n",
    "print(f\"Query Q shape: {Q.shape}\")\n",
    "print(f\"Key K shape: {K.shape}\")\n",
    "print(f\"Value V shape: {V.shape}\")\n",
    "print(f\"  ‚Üí All: [batch_size, seq_len, d_k]\\n\")\n",
    "\n",
    "print(\"üí° Interpretation:\")\n",
    "print(\"  - Each position has a Query: 'What do I need?'\")\n",
    "print(\"  - Each position has a Key: 'What do I offer?'\")\n",
    "print(\"  - Each position has a Value: 'Here's my content'\")\n",
    "print(\"\\n  The model learns these projections during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fb461",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Scaled Dot-Product Attention Formula <a id=\"formula\"></a>\n",
    "\n",
    "### The Complete Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "### Step 1: Compute Attention Scores\n",
    "\n",
    "$$\\text{scores} = QK^T$$\n",
    "\n",
    "- Dot product between queries and keys\n",
    "- High score = high similarity\n",
    "- Shape: $[n \\times n]$ (every query to every key)\n",
    "\n",
    "### Step 2: Scale the Scores\n",
    "\n",
    "$$\\text{scaled\\_scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "**Why scale?**\n",
    "- Dot products grow with dimension $d_k$\n",
    "- Large values ‚Üí softmax saturates ‚Üí tiny gradients\n",
    "- Dividing by $\\sqrt{d_k}$ normalizes variance\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "d_k = 64\n",
    "raw_score = 100  # Too large!\n",
    "scaled_score = 100 / ‚àö64 = 100 / 8 = 12.5  # Better range\n",
    "```\n",
    "\n",
    "### Step 3: Apply Softmax\n",
    "\n",
    "$$\\text{weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "- Converts scores to probabilities\n",
    "- Each row sums to 1\n",
    "- High scores ‚Üí high attention weights\n",
    "\n",
    "### Step 4: Weighted Sum of Values\n",
    "\n",
    "$$\\text{output} = \\text{weights} \\cdot V$$\n",
    "\n",
    "- Multiply attention weights by values\n",
    "- Each position becomes a weighted combination of all values\n",
    "- Positions with high attention contribute more\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Dot product** measures similarity (Q ¬∑ K)\n",
    "2. **Scaling** prevents gradient issues\n",
    "3. **Softmax** creates a distribution (interpretable weights)\n",
    "4. **Weighted sum** aggregates relevant information\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- Attention matrix: $O(n^2 \\cdot d_k)$\n",
    "- Space: $O(n^2)$ (stores attention weights)\n",
    "- This is why very long sequences are challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation to understand each step\n",
    "def manual_attention_step_by_step(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Implement scaled dot-product attention with detailed output\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    print(\"üìê Step-by-Step Attention Computation\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"\\n1Ô∏è‚É£ Compute scores: Q @ K^T\")\n",
    "    print(f\"   Shape: {scores.shape}\")\n",
    "    print(f\"   Range: [{scores.min():.2f}, {scores.max():.2f}]\")\n",
    "    print(f\"   Mean: {scores.mean():.2f}, Std: {scores.std():.2f}\")\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    print(f\"\\n2Ô∏è‚É£ Scale by ‚àö{d_k} = {np.sqrt(d_k):.2f}\")\n",
    "    print(f\"   Range: [{scaled_scores.min():.2f}, {scaled_scores.max():.2f}]\")\n",
    "    print(f\"   Mean: {scaled_scores.mean():.2f}, Std: {scaled_scores.std():.2f}\")\n",
    "    print(f\"   ‚úì Variance normalized!\")\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scaled_scores = scaled_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        print(f\"\\n3Ô∏è‚É£ Apply mask (set masked positions to -inf)\")\n",
    "        print(f\"   Masked positions will get 0 attention after softmax\")\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attn_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    print(f\"\\n{'4Ô∏è‚É£' if mask is None else '4Ô∏è‚É£'} Apply softmax (convert to probabilities)\")\n",
    "    print(f\"   Shape: {attn_weights.shape}\")\n",
    "    print(f\"   Range: [0.0, 1.0]\")\n",
    "    print(f\"   Each row sums to: {attn_weights[0].sum(dim=-1).mean():.4f} (‚âà 1.0)\")\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    print(f\"\\n5Ô∏è‚É£ Compute weighted sum: attention_weights @ V\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Each position is now a weighted combination of all values!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "# Create example data\n",
    "batch_size = 1\n",
    "seq_len = 6\n",
    "d_k = 64\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Run manual attention\n",
    "output, weights = manual_attention_step_by_step(Q, K, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4e557",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation from Scratch <a id=\"implementation\"></a>\n",
    "\n",
    "Now let's implement the `ScaledDotProductAttention` class and use our module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our implementation\n",
    "attention = ScaledDotProductAttention(dropout=0.1)\n",
    "\n",
    "# Test it\n",
    "output, attn_weights = attention(Q, K, V)\n",
    "\n",
    "print(\"\\nüîß Using ScaledDotProductAttention Module\\n\")\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Q: {Q.shape}\")\n",
    "print(f\"  K: {K.shape}\")\n",
    "print(f\"  V: {V.shape}\")\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Attention successfully computed!\")\n",
    "print(f\"   Each position now contains information from all positions\")\n",
    "print(f\"   weighted by their relevance (attention weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50957722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement from scratch for learning\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Minimal attention implementation for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Get dimension\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # 1. Compute scores: Q @ K^T\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # 2. Scale by sqrt(d_k)\n",
    "        scores = scores / np.sqrt(d_k)\n",
    "        \n",
    "        # 3. Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 4. Apply softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 5. Weighted sum of values\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Test our implementation\n",
    "simple_attn = SimpleAttention()\n",
    "output2, weights2 = simple_attn(Q, K, V)\n",
    "\n",
    "print(\"‚úÖ Custom attention implementation works!\")\n",
    "print(f\"   Output shape: {output2.shape}\")\n",
    "print(f\"   Weights shape: {weights2.shape}\")\n",
    "\n",
    "# Verify it matches our module (approximately, due to dropout)\n",
    "print(f\"\\nüìä Consistency check:\")\n",
    "print(f\"   Both implementations produce same shapes: {output.shape == output2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb4657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualization & Analysis <a id=\"visualization\"></a>\n",
    "\n",
    "Let's visualize attention patterns to understand what the model is learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d66a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more interpretable example\n",
    "# Simulate a sentence: \"The cat sat on the mat\"\n",
    "seq_len = 6\n",
    "d_k = 64\n",
    "\n",
    "# Create Q, K, V with some structure\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(1, seq_len, d_k)\n",
    "K = torch.randn(1, seq_len, d_k)\n",
    "V = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "# Make \"cat\" and \"sat\" more similar (subject-verb relationship)\n",
    "K[0, 2] = K[0, 1] * 0.7 + K[0, 2] * 0.3  # \"sat\" key similar to \"cat\" key\n",
    "\n",
    "# Compute attention\n",
    "attention = ScaledDotProductAttention(dropout=0.0)  # No dropout for visualization\n",
    "output, attn_weights = attention(Q, K, V)\n",
    "\n",
    "# Visualize\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), \n",
    "            annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "\n",
    "plt.title('Attention Weight Matrix\\n(Each row shows where that token attends)', fontsize=14, weight='bold')\n",
    "plt.xlabel('Keys (attending TO)', fontsize=12)\n",
    "plt.ylabel('Queries (attending FROM)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Interpretation:\")\n",
    "print(\"  - Darker colors = stronger attention\")\n",
    "print(\"  - Each ROW is a query's attention distribution\")\n",
    "print(\"  - Each row sums to 1.0 (probability distribution)\")\n",
    "print(f\"\\n  Example: '{tokens[2]}' attends most to '{tokens[attn_weights[0, 2].argmax().item()]}'\")\n",
    "print(f\"  (Attention weight: {attn_weights[0, 2].max():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how scaling affects attention\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Create scores with different scales\n",
    "Q_test = torch.randn(1, 4, 64)\n",
    "K_test = torch.randn(1, 4, 64)\n",
    "\n",
    "scores = torch.matmul(Q_test, K_test.transpose(-2, -1))[0]\n",
    "\n",
    "# Without scaling\n",
    "ax1 = axes[0]\n",
    "weights_no_scale = F.softmax(scores, dim=-1)\n",
    "sns.heatmap(weights_no_scale.detach().numpy(), annot=True, fmt='.3f', cmap='YlOrRd', ax=ax1, vmin=0, vmax=1)\n",
    "ax1.set_title('No Scaling\\n(Softmax on raw scores)', fontsize=12, weight='bold')\n",
    "ax1.set_xlabel('Key')\n",
    "ax1.set_ylabel('Query')\n",
    "\n",
    "# With proper scaling\n",
    "ax2 = axes[1]\n",
    "weights_scaled = F.softmax(scores / np.sqrt(64), dim=-1)\n",
    "sns.heatmap(weights_scaled.detach().numpy(), annot=True, fmt='.3f', cmap='YlOrRd', ax=ax2, vmin=0, vmax=1)\n",
    "ax2.set_title(f'Scaled by ‚àö64 = 8\\n(Better distribution)', fontsize=12, weight='bold')\n",
    "ax2.set_xlabel('Key')\n",
    "ax2.set_ylabel('Query')\n",
    "\n",
    "# Show the difference\n",
    "ax3 = axes[2]\n",
    "difference = (weights_scaled - weights_no_scale).detach().numpy()\n",
    "sns.heatmap(difference, annot=True, fmt='.3f', cmap='RdBu_r', ax=ax3, center=0)\n",
    "ax3.set_title('Difference\\n(Scaled - Unscaled)', fontsize=12, weight='bold')\n",
    "ax3.set_xlabel('Key')\n",
    "ax3.set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight:\")\n",
    "print(\"  - Scaling prevents attention from becoming too 'peaky' (dominated by one position)\")\n",
    "print(\"  - Allows more balanced attention distribution\")\n",
    "print(\"  - Better gradients during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a684714",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. DeepSeek Insights <a id=\"deepseek\"></a>\n",
    "\n",
    "### üî¨ DeepSeek-R1 Perspective on Scaled Dot-Product Attention\n",
    "\n",
    "**DeepSeek-R1** research reveals deep insights about how attention enables reasoning:\n",
    "\n",
    "#### 1. **Attention as Differentiable Memory Access**\n",
    "\n",
    "> \"Attention is not just a weighted average - it's a differentiable way to read from memory. The Query is the 'read address', Keys are 'memory indices', and Values are 'memory content'.\"\n",
    "\n",
    "**Why this matters:**\n",
    "- Traditional neural networks have fixed computation\n",
    "- Attention allows **dynamic, input-dependent** computation\n",
    "- The model learns **where to look** based on **what it needs**\n",
    "\n",
    "#### 2. **The Scaling Factor is Critical**\n",
    "\n",
    "> \"Without proper scaling, attention patterns become overconfident early in training, leading to mode collapse and poor generalization.\"\n",
    "\n",
    "**Mathematical reasoning:**\n",
    "- Dot product variance grows with dimension: $\\text{Var}(QK^T) \\propto d_k$\n",
    "- Large values ‚Üí softmax saturates ‚Üí gradients vanish\n",
    "- Scaling by $\\sqrt{d_k}$ ensures: $\\text{Var}(\\frac{QK^T}{\\sqrt{d_k}}) \\approx 1$\n",
    "\n",
    "#### 3. **Information Routing**\n",
    "\n",
    "> \"Attention is the Transformer's way of routing information. Each layer decides: 'Which information from which positions should flow where?'\"\n",
    "\n",
    "**In practice:**\n",
    "- Early layers: Local patterns (adjacent words)\n",
    "- Middle layers: Syntactic relationships (subject-verb)\n",
    "- Late layers: Semantic relationships (reasoning steps)\n",
    "\n",
    "#### 4. **Why Dot Product?**\n",
    "\n",
    "There are other similarity measures (cosine, Euclidean), but dot product wins because:\n",
    "\n",
    "1. **Efficient**: Matrix multiplication is highly optimized on GPUs\n",
    "2. **Differentiable**: Smooth gradients for learning\n",
    "3. **Expressive**: Can represent both similarity AND magnitude\n",
    "4. **Stable**: With proper scaling\n",
    "\n",
    "#### 5. **The Softmax Distribution**\n",
    "\n",
    "> \"Softmax creates a 'soft' selection mechanism. Instead of picking the single best match (hard attention), we get a distribution over all matches. This is crucial for gradient flow.\"\n",
    "\n",
    "**Benefits:**\n",
    "- Differentiable (hard attention isn't)\n",
    "- Allows weighted combinations\n",
    "- Temperature-like behavior (sharper vs softer)\n",
    "\n",
    "---\n",
    "\n",
    "### DeepSeek's Training Insights\n",
    "\n",
    "During DeepSeek-R1 training, researchers observed:\n",
    "\n",
    "1. **Attention patterns emerge gradually**\n",
    "   - Random at initialization\n",
    "   - Local patterns first (adjacent tokens)\n",
    "   - Long-range patterns later (complex reasoning)\n",
    "\n",
    "2. **Different heads specialize**\n",
    "   - Some focus on syntax\n",
    "   - Some focus on semantics\n",
    "   - Some focus on specific linguistic phenomena\n",
    "\n",
    "3. **Reasoning requires multi-hop attention**\n",
    "   - Single attention layer: direct associations\n",
    "   - Multiple layers: chains of reasoning\n",
    "   - Example: A‚ÜíB (layer 1), B‚ÜíC (layer 2), conclude A‚ÜíC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of different similarity measures\n",
    "def compare_similarity_measures(Q, K):\n",
    "    \"\"\"\n",
    "    Compare dot product, cosine similarity, and Euclidean distance\n",
    "    \"\"\"\n",
    "    # Dot product (what we use)\n",
    "    dot_product = torch.matmul(Q, K.transpose(-2, -1))[0]\n",
    "    \n",
    "    # Cosine similarity\n",
    "    Q_norm = Q / Q.norm(dim=-1, keepdim=True)\n",
    "    K_norm = K / K.norm(dim=-1, keepdim=True)\n",
    "    cosine_sim = torch.matmul(Q_norm, K_norm.transpose(-2, -1))[0]\n",
    "    \n",
    "    # Euclidean distance (convert to similarity)\n",
    "    Q_expanded = Q.unsqueeze(2)  # [1, n, 1, d]\n",
    "    K_expanded = K.unsqueeze(1)  # [1, 1, n, d]\n",
    "    euclidean_dist = torch.norm(Q_expanded - K_expanded, dim=-1)[0]\n",
    "    euclidean_sim = -euclidean_dist  # Negative because smaller distance = more similar\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    measures = [\n",
    "        (dot_product, 'Dot Product\\n(Used in Transformers)', 'RdBu_r'),\n",
    "        (cosine_sim, 'Cosine Similarity', 'RdBu_r'),\n",
    "        (euclidean_sim, 'Negative Euclidean Distance', 'RdBu_r')\n",
    "    ]\n",
    "    \n",
    "    for ax, (scores, title, cmap) in zip(axes, measures):\n",
    "        # Apply softmax for fair comparison\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        sns.heatmap(weights.detach().numpy(), annot=True, fmt='.3f', \n",
    "                   cmap='YlOrRd', ax=ax, vmin=0, vmax=1)\n",
    "        ax.set_title(title, fontsize=12, weight='bold')\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with small example\n",
    "Q_test = torch.randn(1, 4, 64)\n",
    "K_test = torch.randn(1, 4, 64)\n",
    "\n",
    "compare_similarity_measures(Q_test, K_test)\n",
    "\n",
    "print(\"üîç Comparison:\")\n",
    "print(\"  - Dot product: Fast, expressive, but needs scaling\")\n",
    "print(\"  - Cosine: Normalized, but loses magnitude information\")\n",
    "print(\"  - Euclidean: Distance-based, less efficient for high dimensions\")\n",
    "print(\"\\n  ‚úÖ Dot product + scaling wins for Transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c446b56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The Attention Revolution**\n",
    "   - Replaced sequential RNNs with parallel attention\n",
    "   - Direct connections between all positions\n",
    "   - No information bottleneck\n",
    "\n",
    "2. **Query, Key, Value**\n",
    "   - Q: \"What am I looking for?\"\n",
    "   - K: \"What do I offer?\"\n",
    "   - V: \"Here's my content\"\n",
    "   - Learned projections of input\n",
    "\n",
    "3. **The Formula**\n",
    "   $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "   - Dot product for similarity\n",
    "   - Scale to prevent gradient issues\n",
    "   - Softmax for probability distribution\n",
    "   - Weighted sum for output\n",
    "\n",
    "4. **Why It Works**\n",
    "   - Dynamic information routing\n",
    "   - Differentiable memory access\n",
    "   - Learned attention patterns\n",
    "   - Enables reasoning through information flow\n",
    "\n",
    "5. **DeepSeek Insights**\n",
    "   - Attention = differentiable read from memory\n",
    "   - Scaling is critical for training stability\n",
    "   - Patterns emerge from simple to complex\n",
    "   - Multi-hop reasoning requires depth\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Tutorial 3: Multi-Head Attention & Masking**, we'll learn:\n",
    "- How multiple attention heads provide different perspectives\n",
    "- Parallel attention computation\n",
    "- Masking strategies (padding, causal)\n",
    "- Different attention patterns (self, cross)\n",
    "\n",
    "The scaled dot-product attention we learned is the **building block** for multi-head attention!\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Exercises\n",
    "\n",
    "1. **Implement Without Scaling**: Remove the $\\sqrt{d_k}$ scaling and observe the effect on attention patterns\n",
    "\n",
    "2. **Experiment with Dimensions**: Try different $d_k$ values (16, 32, 128, 256) and see how it affects attention\n",
    "\n",
    "3. **Create Structured Patterns**: Design Q, K, V matrices to create specific attention patterns (e.g., each position attends only to itself)\n",
    "\n",
    "4. **Visualize Gradients**: Compute gradients and visualize how they flow through the attention mechanism\n",
    "\n",
    "5. **Alternative Similarity**: Implement attention using cosine similarity instead of dot product - what changes?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
