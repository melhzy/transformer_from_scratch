{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c21f17",
   "metadata": {},
   "source": [
    "# üöÄ Run in Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/melhzy/transformer_from_scratch/blob/main/transformer-foundation/04_feed_forward_networks.ipynb)\n",
    "\n",
    "**Note:** If you're running this in Google Colab, execute the setup cell below to clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup (run this cell only if you're in Colab)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete! Ready to run the tutorial.\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.modules.feed_forward import PositionwiseFeedForward, GLUFeedForward\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4744ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Position-wise Feed-Forward Networks <a id=\"ffn\"></a>\n",
    "\n",
    "### What is a Position-wise FFN?\n",
    "\n",
    "After attention aggregates information, we need to **transform** it. The feed-forward network (FFN) applies the same transformation to each position **independently**.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ (expand)\n",
    "- $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ (contract)\n",
    "- Typically: $d_{ff} = 4 \\times d_{model}$\n",
    "\n",
    "### Why \"Position-wise\"?\n",
    "\n",
    "The same FFN is applied to **each position independently**:\n",
    "\n",
    "```python\n",
    "# NOT like this (sharing across sequence):\n",
    "output = FFN(entire_sequence)\n",
    "\n",
    "# But like this (independent per position):\n",
    "for position in sequence:\n",
    "    output[position] = FFN(input[position])\n",
    "```\n",
    "\n",
    "In practice, we process all positions in parallel using batch operations.\n",
    "\n",
    "### The Expand-Contract Pattern\n",
    "\n",
    "```\n",
    "Input:  512 dimensions\n",
    "   ‚Üì Linear + ReLU\n",
    "Hidden: 2048 dimensions (4√ó expansion!)\n",
    "   ‚Üì Linear\n",
    "Output: 512 dimensions\n",
    "```\n",
    "\n",
    "**Why expand?**\n",
    "- More capacity for complex transformations\n",
    "- Non-linear mixing in higher dimensions\n",
    "- Think of it as a \"bottleneck\" layer in reverse\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Original paper used **ReLU**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "Modern variants use:\n",
    "- **GELU** (Gaussian Error Linear Unit): Smoother, better gradients\n",
    "- **SwiGLU**: Combines Swish and GLU (Gated Linear Unit)\n",
    "- **GLU variants**: Gating mechanism for better control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d75616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test a position-wise FFN\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "ffn = PositionwiseFeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "# Test input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = ffn(x)\n",
    "\n",
    "print(\"üîß Position-wise Feed-Forward Network\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Input dimension (d_model): {d_model}\")\n",
    "print(f\"  - Hidden dimension (d_ff): {d_ff} ({d_ff//d_model}√ó expansion)\")\n",
    "print(f\"  - Output dimension: {d_model}\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nüìê Shapes:\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  ‚úì Shape preserved (important for residual connections!)\")\n",
    "\n",
    "print(f\"\\nüí° Key Points:\")\n",
    "print(f\"  - Applied independently to each position\")\n",
    "print(f\"  - Same parameters shared across all positions\")\n",
    "print(f\"  - Adds non-linear transformation capacity\")\n",
    "print(f\"  - The 4√ó expansion allows rich feature mixing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformation\n",
    "# Show input and output distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Input distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(x.flatten().detach().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "ax1.set_title('Input Distribution', fontsize=14, weight='bold')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Get intermediate (after first linear + ReLU)\n",
    "with torch.no_grad():\n",
    "    intermediate = F.relu(ffn.linear1(x))\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.hist(intermediate.flatten().detach().numpy(), bins=50, alpha=0.7, color='green')\n",
    "ax2.set_title(f'After Expansion & ReLU\\n({d_ff} dimensions)', fontsize=14, weight='bold')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "print(f\"\\nüìä Note: ReLU sets all negative values to 0\")\n",
    "\n",
    "# Output distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(output.flatten().detach().numpy(), bins=50, alpha=0.7, color='purple')\n",
    "ax3.set_title('Output Distribution', fontsize=14, weight='bold')\n",
    "ax3.set_xlabel('Value')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Distribution Analysis:\")\n",
    "print(f\"  Input - Mean: {x.mean():.3f}, Std: {x.std():.3f}\")\n",
    "print(f\"  Intermediate - Mean: {intermediate.mean():.3f}, Std: {intermediate.std():.3f}\")\n",
    "print(f\"  Output - Mean: {output.mean():.3f}, Std: {output.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4757a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Layer Normalization <a id=\"layernorm\"></a>\n",
    "\n",
    "### Why Normalization?\n",
    "\n",
    "Deep networks suffer from **internal covariate shift**: as parameters update, the distribution of layer inputs changes. This makes training unstable.\n",
    "\n",
    "**Layer Normalization** solves this by normalizing across features:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu = \\frac{1}{d}\\sum_{i=1}^d x_i$ (mean across features)\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^d (x_i - \\mu)^2$ (variance)\n",
    "- $\\gamma, \\beta$ are learnable scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "### Layer Norm vs Batch Norm\n",
    "\n",
    "**Batch Normalization**: Normalizes across the batch dimension\n",
    "```\n",
    "For each feature:\n",
    "  Compute mean & variance across all samples in batch\n",
    "```\n",
    "\n",
    "**Layer Normalization**: Normalizes across the feature dimension\n",
    "```\n",
    "For each sample:\n",
    "  Compute mean & variance across all features\n",
    "```\n",
    "\n",
    "**Why Layer Norm for Transformers?**\n",
    "1. Works with variable sequence lengths\n",
    "2. Independent of batch size (important for generation)\n",
    "3. More stable for NLP tasks\n",
    "4. No need to track running statistics\n",
    "\n",
    "### Effect of Layer Normalization\n",
    "\n",
    "- **Stabilizes training**: Prevents activations from exploding/vanishing\n",
    "- **Faster convergence**: Can use higher learning rates\n",
    "- **Better generalization**: Acts as regularization\n",
    "- **Gradient flow**: Ensures gradients don't get too large/small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ea076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate layer normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "# Create input with varying statistics\n",
    "x_unnormalized = torch.randn(2, 10, d_model) * 5 + 10  # Mean=10, Std=5\n",
    "\n",
    "# Apply layer norm\n",
    "x_normalized = layer_norm(x_unnormalized)\n",
    "\n",
    "print(\"üìè Layer Normalization Effect\\n\")\n",
    "print(\"Before normalization:\")\n",
    "print(f\"  Shape: {x_unnormalized.shape}\")\n",
    "print(f\"  Mean (across features): {x_unnormalized.mean(dim=-1)[0, 0]:.3f}\")\n",
    "print(f\"  Std (across features): {x_unnormalized.std(dim=-1)[0, 0]:.3f}\")\n",
    "print(f\"  Global range: [{x_unnormalized.min():.3f}, {x_unnormalized.max():.3f}]\")\n",
    "\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(f\"  Shape: {x_normalized.shape} (unchanged)\")\n",
    "print(f\"  Mean (across features): {x_normalized.mean(dim=-1)[0, 0]:.3f} (‚âà 0)\")\n",
    "print(f\"  Std (across features): {x_normalized.std(dim=-1)[0, 0]:.3f} (‚âà 1)\")\n",
    "print(f\"  Global range: [{x_normalized.min():.3f}, {x_normalized.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.hist(x_unnormalized[0].flatten().detach().numpy(), bins=50, alpha=0.7, color='red')\n",
    "ax1.set_title('Before Layer Normalization\\n(Varying mean & std)', fontsize=14, weight='bold')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(x_unnormalized[0].mean(), color='black', linestyle='--', label='Mean')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.hist(x_normalized[0].flatten().detach().numpy(), bins=50, alpha=0.7, color='green')\n",
    "ax2.set_title('After Layer Normalization\\n(Mean‚âà0, Std‚âà1)', fontsize=14, weight='bold')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(x_normalized[0].mean(), color='black', linestyle='--', label='Mean‚âà0')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Layer normalization ensures stable, normalized activations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b11c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Residual Connections <a id=\"residual\"></a>\n",
    "\n",
    "### The Residual (Skip Connection) Concept\n",
    "\n",
    "Instead of:\n",
    "$$\\text{output} = F(x)$$\n",
    "\n",
    "We do:\n",
    "$$\\text{output} = F(x) + x$$\n",
    "\n",
    "Where $F(x)$ is the sub-layer (attention or FFN).\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "**1. Gradient Flow**\n",
    "```python\n",
    "# Without residual:\n",
    "‚àÇLoss/‚àÇx = ‚àÇLoss/‚àÇF(x) ¬∑ ‚àÇF(x)/‚àÇx  # Can vanish!\n",
    "\n",
    "# With residual:\n",
    "‚àÇLoss/‚àÇx = ‚àÇLoss/‚àÇ(F(x)+x) ¬∑ (‚àÇF(x)/‚àÇx + 1)  # Always has gradient from '+1'!\n",
    "```\n",
    "\n",
    "**2. Identity Mapping**\n",
    "- Model can learn to ignore a layer by setting $F(x) \\approx 0$\n",
    "- Makes it easier to train very deep networks\n",
    "- The network can always \"fall back\" to the identity\n",
    "\n",
    "**3. Ensemble Behavior**\n",
    "- Residual networks behave like ensembles of shallower networks\n",
    "- Each path from input to output contributes\n",
    "- More robust and better generalization\n",
    "\n",
    "### In Transformers\n",
    "\n",
    "Every sub-layer (attention, FFN) has a residual connection:\n",
    "\n",
    "```python\n",
    "# After attention:\n",
    "x = x + attention(x)\n",
    "\n",
    "# After FFN:\n",
    "x = x + ffn(x)\n",
    "```\n",
    "\n",
    "This allows information to flow directly through the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate residual connections\n",
    "class SubLayerWithResidual(nn.Module):\n",
    "    \"\"\"Demonstrates a sub-layer with residual connection\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout=0.0)\n",
    "    \n",
    "    def forward(self, x, use_residual=True):\n",
    "        ffn_output = self.ffn(x)\n",
    "        if use_residual:\n",
    "            return x + ffn_output  # Residual connection\n",
    "        else:\n",
    "            return ffn_output  # No residual\n",
    "\n",
    "# Test\n",
    "sublayer = SubLayerWithResidual(d_model=512, d_ff=2048)\n",
    "x = torch.randn(1, 10, 512)\n",
    "\n",
    "output_with_residual = sublayer(x, use_residual=True)\n",
    "output_without_residual = sublayer(x, use_residual=False)\n",
    "\n",
    "print(\"üîó Residual Connection Effect\\n\")\n",
    "print(f\"Input norm: {x.norm():.3f}\")\n",
    "print(f\"\\nWithout residual:\")\n",
    "print(f\"  Output norm: {output_without_residual.norm():.3f}\")\n",
    "print(f\"  Difference from input: {(output_without_residual - x).norm():.3f}\")\n",
    "\n",
    "print(f\"\\nWith residual:\")\n",
    "print(f\"  Output norm: {output_with_residual.norm():.3f}\")\n",
    "print(f\"  Difference from input: {(output_with_residual - x).norm():.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Notice:\")\n",
    "print(f\"  - With residual, output preserves input information\")\n",
    "print(f\"  - The FFN learns to add a 'delta' to the input\")\n",
    "print(f\"  - Gradients can flow directly through the '+' operation\")\n",
    "\n",
    "# Visualize gradient flow\n",
    "x_input = torch.randn(1, 1, 512, requires_grad=True)\n",
    "output = sublayer(x_input, use_residual=True)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nüéØ Gradient Flow:\")\n",
    "print(f\"  Input gradient norm: {x_input.grad.norm():.3f}\")\n",
    "print(f\"  ‚úì Gradients flow back easily thanks to residual!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a830c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The Complete Sub-Layer Pattern <a id=\"pattern\"></a>\n",
    "\n",
    "### The Transformer's Universal Pattern\n",
    "\n",
    "Every sub-layer in a Transformer follows this pattern:\n",
    "\n",
    "```python\n",
    "# Pre-LN (Pre-Normalization) - Modern approach:\n",
    "output = x + SubLayer(LayerNorm(x))\n",
    "\n",
    "# Post-LN (Post-Normalization) - Original paper:\n",
    "output = LayerNorm(x + SubLayer(x))\n",
    "```\n",
    "\n",
    "Where `SubLayer` can be:\n",
    "- Multi-head attention\n",
    "- Feed-forward network\n",
    "\n",
    "### Pre-LN vs Post-LN\n",
    "\n",
    "**Post-LN (Original):**\n",
    "```\n",
    "x ‚Üí SubLayer ‚Üí Add (residual) ‚Üí LayerNorm ‚Üí output\n",
    "```\n",
    "\n",
    "**Pre-LN (Modern):**\n",
    "```\n",
    "x ‚Üí LayerNorm ‚Üí SubLayer ‚Üí Add (residual) ‚Üí output\n",
    "```\n",
    "\n",
    "**Why Pre-LN is better:**\n",
    "- More stable training\n",
    "- Can train deeper models\n",
    "- Doesn't require learning rate warmup\n",
    "- Residual path is always normalized\n",
    "\n",
    "### Complete Encoder Layer\n",
    "\n",
    "```python\n",
    "# 1. Self-Attention sub-layer\n",
    "x = x + MultiHeadAttention(LayerNorm(x))\n",
    "\n",
    "# 2. Feed-Forward sub-layer  \n",
    "x = x + FeedForward(LayerNorm(x))\n",
    "```\n",
    "\n",
    "Two sub-layers, each with:\n",
    "- Layer normalization\n",
    "- The actual computation\n",
    "- Residual connection\n",
    "- Dropout (not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the complete pattern\n",
    "class SubLayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete sub-layer pattern:\n",
    "    LayerNorm ‚Üí SubLayer ‚Üí Dropout ‚Üí Residual\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"Apply: x + dropout(sublayer(norm(x)))\"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Test with FFN as sublayer\n",
    "d_model = 512\n",
    "connection = SubLayerConnection(d_model, dropout=0.1)\n",
    "ffn = PositionwiseFeedForward(d_model, 2048, dropout=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = connection(x, ffn)\n",
    "\n",
    "print(\"üéØ Complete Sub-Layer Pattern\\n\")\n",
    "print(\"Flow: Input ‚Üí LayerNorm ‚Üí SubLayer ‚Üí Dropout ‚Üí Add(Residual) ‚Üí Output\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"  Input - Mean: {x.mean():.3f}, Std: {x.std():.3f}\")\n",
    "print(f\"  Output - Mean: {output.mean():.3f}, Std: {output.std():.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ This pattern is used for EVERY sub-layer in Transformers!\")\n",
    "print(f\"   - Makes architecture clean and modular\")\n",
    "print(f\"   - Ensures stable training\")\n",
    "print(f\"   - Enables very deep networks (100+ layers possible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete pattern\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.axis('off')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# Draw the flow\n",
    "components = [\n",
    "    (5, 9, \"Input x\", 'lightblue'),\n",
    "    (5, 7.5, \"LayerNorm(x)\", 'lightyellow'),\n",
    "    (5, 6, \"SubLayer\\n(Attention or FFN)\", 'lightgreen'),\n",
    "    (5, 4.5, \"Dropout\", 'lightcoral'),\n",
    "    (5, 3, \"Add Residual\\n(+ x)\", 'plum'),\n",
    "    (5, 1.5, \"Output\", 'lightblue'),\n",
    "]\n",
    "\n",
    "for x, y, text, color in components:\n",
    "    box = FancyBboxPatch((x-1.5, y-0.3), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                         edgecolor='black', facecolor=color, linewidth=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=11, weight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "for i in range(len(components)-1):\n",
    "    ax.annotate('', xy=(5, components[i+1][1]+0.3), \n",
    "               xytext=(5, components[i][1]-0.3),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "# Draw residual connection (skip connection)\n",
    "ax.annotate('', xy=(7.5, 3), xytext=(7.5, 9),\n",
    "           arrowprops=dict(arrowstyle='->', lw=3, color='red', \n",
    "                         connectionstyle=\"arc3,rad=.3\"))\n",
    "ax.text(8.5, 6, 'Residual\\nConnection', fontsize=10, color='red', \n",
    "       weight='bold', ha='center')\n",
    "\n",
    "plt.title('Complete Sub-Layer Pattern in Transformers', \n",
    "         fontsize=16, weight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìã This pattern appears in:\")\n",
    "print(\"  - Encoder self-attention\")\n",
    "print(\"  - Encoder feed-forward\")\n",
    "print(\"  - Decoder masked self-attention\")\n",
    "print(\"  - Decoder cross-attention\")\n",
    "print(\"  - Decoder feed-forward\")\n",
    "print(\"\\n  ‚Üí 5 times per encoder-decoder layer pair!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb7b56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. DeepSeek Insights <a id=\"deepseek\"></a>\n",
    "\n",
    "### üî¨ DeepSeek-R1 Perspective on FFN & Normalization\n",
    "\n",
    "#### 1. **FFN as Memory Storage**\n",
    "\n",
    "> \"While attention routes information, the feed-forward layers **store** knowledge. They're like the 'facts' memory of the network.\"\n",
    "\n",
    "**Research findings:**\n",
    "- FFN weights encode factual knowledge\n",
    "- Different neurons activate for different concepts\n",
    "- Editing FFN weights can change model's knowledge\n",
    "- The 4√ó expansion creates rich representational space\n",
    "\n",
    "#### 2. **The Expand-Contract is Key**\n",
    "\n",
    "> \"The expansion to 4√ó dimensions isn't arbitrary - it's the sweet spot between capacity and efficiency.\"\n",
    "\n",
    "**Why 4√ó?**\n",
    "- Less than 4√ó: Insufficient capacity, bottleneck\n",
    "- More than 4√ó: Diminishing returns, wasted compute\n",
    "- 4√ó empirically optimal across many tasks\n",
    "\n",
    "#### 3. **Layer Norm Enables Scale**\n",
    "\n",
    "> \"Without layer normalization, training transformers beyond 12 layers is nearly impossible. With it, we scale to 100+ layers.\"\n",
    "\n",
    "**Why it works:**\n",
    "- Prevents activation explosion/vanishing\n",
    "- Makes optimization landscape smoother\n",
    "- Reduces sensitivity to initialization\n",
    "- Enables higher learning rates\n",
    "\n",
    "#### 4. **Pre-LN vs Post-LN**\n",
    "\n",
    "> \"The shift from post-LN to pre-LN was crucial for scaling. Pre-LN is strictly superior for deep models.\"\n",
    "\n",
    "**DeepSeek-R1 uses Pre-LN** because:\n",
    "- Trains 3-5√ó faster\n",
    "- More stable with high learning rates\n",
    "- Scales better to 90+ layers\n",
    "- No warmup needed\n",
    "\n",
    "#### 5. **Residual Connections are Essential**\n",
    "\n",
    "> \"Without residuals, deep transformers simply don't train. The gradient flow is crucial for learning multi-hop reasoning.\"\n",
    "\n",
    "**Reasoning connection:**\n",
    "- Each layer adds a \"reasoning step\"\n",
    "- Residuals allow building on previous steps\n",
    "- Gradients flow back through all steps\n",
    "- Enables learning complex multi-step logic\n",
    "\n",
    "#### 6. **GLU Variants for Better Performance**\n",
    "\n",
    "> \"Modern transformers use gated variants (SwiGLU, GeGLU) instead of plain ReLU. The gating mechanism provides better control.\"\n",
    "\n",
    "**SwiGLU formula:**\n",
    "$$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\odot (xW_2)$$\n",
    "\n",
    "Where $\\odot$ is element-wise multiplication (gating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289011df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different activation functions\n",
    "x = torch.linspace(-3, 3, 1000)\n",
    "\n",
    "# Different activations\n",
    "relu = F.relu(x)\n",
    "gelu = F.gelu(x)\n",
    "swish = x * torch.sigmoid(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, relu, label='ReLU (Original Transformer)', linewidth=2)\n",
    "plt.plot(x, gelu, label='GELU (BERT, GPT-2)', linewidth=2)\n",
    "plt.plot(x, swish, label='Swish (Modern)', linewidth=2)\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Input', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('Activation Functions in Feed-Forward Networks', fontsize=14, weight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Activation Function Comparison:\")\n",
    "print(\"\\nReLU: max(0, x)\")\n",
    "print(\"  ‚úì Simple, fast\")\n",
    "print(\"  ‚úó Hard cutoff at 0, dead neurons\")\n",
    "\n",
    "print(\"\\nGELU: x¬∑Œ¶(x) where Œ¶ is Gaussian CDF\")\n",
    "print(\"  ‚úì Smooth, probabilistic\")\n",
    "print(\"  ‚úì Better gradients\")\n",
    "\n",
    "print(\"\\nSwish: x¬∑œÉ(x)\")\n",
    "print(\"  ‚úì Smooth, self-gated\")\n",
    "print(\"  ‚úì Works well in practice\")\n",
    "\n",
    "print(\"\\nüéØ DeepSeek-R1 uses variants of these in gated form (SwiGLU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4fc6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Implementation <a id=\"implementation\"></a>\n",
    "\n",
    "Let's see how to use our modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GLU variant\n",
    "glu_ffn = GLUFeedForward(d_model=512, d_ff=2048, dropout=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "output_glu = glu_ffn(x)\n",
    "\n",
    "print(\"üîß GLU Feed-Forward Network\\n\")\n",
    "print(\"Formula: GLU(x) = (xW1 + b1) ‚äô œÉ(xW2 + b2)\")\n",
    "print(\"  where ‚äô is element-wise multiplication (gating)\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Input dimension: 512\")\n",
    "print(f\"  Hidden dimension: 2048\")\n",
    "print(f\"  Gating mechanism: Sigmoid\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in glu_ffn.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Shape: {output_glu.shape}\")\n",
    "print(f\"  ‚úì GLU variants often perform better than plain FFN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1aa3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Position-wise Feed-Forward**\n",
    "   - Two linear layers with activation\n",
    "   - 4√ó expansion (512 ‚Üí 2048 ‚Üí 512)\n",
    "   - Applied independently to each position\n",
    "   - Stores factual knowledge\n",
    "\n",
    "2. **Layer Normalization**\n",
    "   - Normalizes across features (not batch)\n",
    "   - Mean ‚âà 0, Std ‚âà 1\n",
    "   - Critical for training stability\n",
    "   - Enables deep networks\n",
    "\n",
    "3. **Residual Connections**\n",
    "   - output = input + sublayer(input)\n",
    "   - Ensures gradient flow\n",
    "   - Enables identity mapping\n",
    "   - Essential for depth\n",
    "\n",
    "4. **Complete Pattern**\n",
    "   - Pre-LN: x + SubLayer(LayerNorm(x))\n",
    "   - Used for every sub-layer\n",
    "   - Clean, modular architecture\n",
    "\n",
    "5. **DeepSeek Insights**\n",
    "   - FFN stores knowledge, attention routes it\n",
    "   - 4√ó expansion is empirically optimal\n",
    "   - Pre-LN superior to Post-LN\n",
    "   - Gated variants (GLU) improve performance\n",
    "\n",
    "### The Formula\n",
    "\n",
    "Complete sub-layer with all components:\n",
    "\n",
    "$$\\text{output} = x + \\text{Dropout}(\\text{SubLayer}(\\text{LayerNorm}(x)))$$\n",
    "\n",
    "Where SubLayer can be:\n",
    "- Multi-head attention\n",
    "- Feed-forward network\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Tutorial 5: Encoder & Decoder Architecture**, we'll see:\n",
    "- How to stack these components into encoder layers\n",
    "- Decoder layers with cross-attention\n",
    "- Complete encoder and decoder stacks\n",
    "- How information flows through the full model\n",
    "\n",
    "These building blocks are now ready to be assembled! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
