{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy pandas tqdm nltk rouge-score\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add84f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "if not IN_COLAB:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK for BLEU\n",
    "import nltk\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "    from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "# ROUGE\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "except ImportError:\n",
    "    print(\"Installing rouge-score...\")\n",
    "    !pip install -q rouge-score\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee78f7b",
   "metadata": {},
   "source": [
    "## 1. Perplexity: Language Model Evaluation üìê\n",
    "\n",
    "**Perplexity** measures how well a language model predicts text.\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_{<i})\\right)$$\n",
    "\n",
    "Where:\n",
    "- $N$ = number of tokens\n",
    "- $P(w_i | w_{<i})$ = probability of token $w_i$ given previous tokens\n",
    "\n",
    "**Lower is better!** A perplexity of 100 means the model is as confused as if it had to choose uniformly from 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute perplexity on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        dataloader: DataLoader with tokenized examples\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        perplexity: float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            src = input_ids[:, :-1]\n",
    "            tgt = input_ids[:, :-1]\n",
    "            logits = model(src, tgt)\n",
    "            \n",
    "            # Compute loss\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "            labels_flat = labels[:, 1:].reshape(-1)\n",
    "            \n",
    "            # Ignore padding (-100)\n",
    "            mask = labels_flat != -100\n",
    "            loss = F.cross_entropy(\n",
    "                logits_flat[mask],\n",
    "                labels_flat[mask],\n",
    "                reduction='sum'\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    \n",
    "    # Perplexity = exp(average loss)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "\n",
    "# Example (dummy data)\n",
    "print(\"Example perplexity calculation:\")\n",
    "print(f\"  Loss: 2.5 ‚Üí PPL: {math.exp(2.5):.2f}\")\n",
    "print(f\"  Loss: 1.0 ‚Üí PPL: {math.exp(1.0):.2f}\")\n",
    "print(f\"  Loss: 0.5 ‚Üí PPL: {math.exp(0.5):.2f}\")\n",
    "print(\"\\nüí° Lower perplexity = better model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3248552",
   "metadata": {},
   "source": [
    "## 2. BLEU Score: Translation & Generation Quality üåê\n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)** measures n-gram overlap between generated and reference text.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "Where:\n",
    "- $p_n$ = precision of n-grams\n",
    "- $BP$ = brevity penalty (penalizes short outputs)\n",
    "- Typically $N=4$ (up to 4-grams)\n",
    "\n",
    "**Range: 0-1 (or 0-100), higher is better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(\n",
    "    predictions: List[str],\n",
    "    references: List[str],\n",
    "    max_n: int = 4\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute BLEU scores (BLEU-1 through BLEU-4).\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated texts\n",
    "        references: List of reference texts\n",
    "        max_n: Maximum n-gram size\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with BLEU scores\n",
    "    \"\"\"\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Tokenize\n",
    "    pred_tokens = [pred.lower().split() for pred in predictions]\n",
    "    ref_tokens = [[ref.lower().split()] for ref in references]  # Nested for multiple refs\n",
    "    \n",
    "    bleu_scores = {}\n",
    "    \n",
    "    # Compute BLEU-1 through BLEU-4\n",
    "    for n in range(1, max_n + 1):\n",
    "        weights = [1.0/n] * n + [0] * (4 - n)\n",
    "        score = corpus_bleu(\n",
    "            ref_tokens,\n",
    "            pred_tokens,\n",
    "            weights=weights,\n",
    "            smoothing_function=smoothing\n",
    "        )\n",
    "        bleu_scores[f'BLEU-{n}'] = score * 100  # Convert to percentage\n",
    "    \n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "# Example\n",
    "predictions = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"hello world\",\n",
    "]\n",
    "references = [\n",
    "    \"the cat is sitting on the mat\",\n",
    "    \"hello there world\",\n",
    "]\n",
    "\n",
    "bleu = compute_bleu(predictions, references)\n",
    "print(\"\\nExample BLEU Scores:\")\n",
    "for metric, score in bleu.items():\n",
    "    print(f\"  {metric}: {score:.2f}\")\n",
    "\n",
    "print(\"\\nüí° BLEU measures n-gram overlap (higher = more similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683802e",
   "metadata": {},
   "source": [
    "## 3. ROUGE Score: Summarization Quality üìÑ\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** measures recall of n-grams.\n",
    "\n",
    "### Variants:\n",
    "- **ROUGE-1**: Unigram overlap\n",
    "- **ROUGE-2**: Bigram overlap\n",
    "- **ROUGE-L**: Longest Common Subsequence\n",
    "\n",
    "Each returns:\n",
    "- **Precision**: What % of generated n-grams appear in reference?\n",
    "- **Recall**: What % of reference n-grams appear in generated?\n",
    "- **F1**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(\n",
    "    predictions: List[str],\n",
    "    references: List[str]\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated texts\n",
    "        references: List of reference texts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ROUGE scores\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    scores = defaultdict(lambda: {'precision': [], 'recall': [], 'f1': []})\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        result = scorer.score(ref, pred)\n",
    "        \n",
    "        for metric_name, metric_score in result.items():\n",
    "            scores[metric_name]['precision'].append(metric_score.precision)\n",
    "            scores[metric_name]['recall'].append(metric_score.recall)\n",
    "            scores[metric_name]['f1'].append(metric_score.fmeasure)\n",
    "    \n",
    "    # Average scores\n",
    "    avg_scores = {}\n",
    "    for metric_name, values in scores.items():\n",
    "        avg_scores[metric_name] = {\n",
    "            'precision': np.mean(values['precision']) * 100,\n",
    "            'recall': np.mean(values['recall']) * 100,\n",
    "            'f1': np.mean(values['f1']) * 100\n",
    "        }\n",
    "    \n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "# Example\n",
    "rouge = compute_rouge(predictions, references)\n",
    "print(\"\\nExample ROUGE Scores:\")\n",
    "for metric_name, scores in rouge.items():\n",
    "    print(f\"\\n{metric_name.upper()}:\")\n",
    "    print(f\"  Precision: {scores['precision']:.2f}\")\n",
    "    print(f\"  Recall: {scores['recall']:.2f}\")\n",
    "    print(f\"  F1: {scores['f1']:.2f}\")\n",
    "\n",
    "print(\"\\nüí° ROUGE emphasizes recall (how much of reference is covered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa401a",
   "metadata": {},
   "source": [
    "## 4. Exact Match & F1 (QA Tasks) ‚ùì\n",
    "\n",
    "For question answering and extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison\"\"\"\n",
    "    return ' '.join(text.lower().strip().split())\n",
    "\n",
    "\n",
    "def compute_exact_match(predictions: List[str], references: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute exact match accuracy.\n",
    "    \"\"\"\n",
    "    matches = sum(\n",
    "        normalize_text(pred) == normalize_text(ref)\n",
    "        for pred, ref in zip(predictions, references)\n",
    "    )\n",
    "    return (matches / len(predictions)) * 100\n",
    "\n",
    "\n",
    "def compute_token_f1(predictions: List[str], references: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute token-level F1 score.\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = normalize_text(pred).split()\n",
    "        ref_tokens = normalize_text(ref).split()\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Compute overlap\n",
    "        common = Counter(pred_tokens) & Counter(ref_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        \n",
    "        if num_same == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(ref_tokens)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores) * 100\n",
    "\n",
    "\n",
    "# Example\n",
    "qa_predictions = [\"Paris\", \"4\", \"blue\"]\n",
    "qa_references = [\"Paris\", \"four\", \"blue\"]\n",
    "\n",
    "em = compute_exact_match(qa_predictions, qa_references)\n",
    "f1 = compute_token_f1(qa_predictions, qa_references)\n",
    "\n",
    "print(\"\\nQA Metrics:\")\n",
    "print(f\"  Exact Match: {em:.2f}%\")\n",
    "print(f\"  Token F1: {f1:.2f}%\")\n",
    "print(\"\\nüí° Exact Match requires perfect answer, F1 allows partial credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5fb7a",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Evaluation Suite üî¨\n",
    "\n",
    "Combine all metrics for thorough evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation suite for fine-tuned LLMs.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int = 50,\n",
    "        temperature: float = 1.0\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text from prompt\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(self.device)\n",
    "        generated = input_ids.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                src = input_tensor[:, :-1]\n",
    "                tgt = input_tensor[:, :-1]\n",
    "                logits = self.model(src, tgt)\n",
    "                \n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                next_token_id = torch.argmax(next_token_logits).item()\n",
    "                \n",
    "                if next_token_id == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_token_id)\n",
    "                input_tensor = torch.tensor([generated], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        return self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    def evaluate_generation(\n",
    "        self,\n",
    "        test_data: List[Dict],\n",
    "        prompt_template: str = \"Instruction: {instruction} Input: {input} Output:\"\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate generation quality on test set.\n",
    "        \n",
    "        Returns all metrics: BLEU, ROUGE, Exact Match, F1\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        print(\"Generating predictions...\")\n",
    "        for item in tqdm(test_data):\n",
    "            prompt = prompt_template.format(**item)\n",
    "            generated = self.generate(prompt, max_length=30)\n",
    "            \n",
    "            # Extract only the generated part (after prompt)\n",
    "            if prompt in generated:\n",
    "                generated = generated[len(prompt):].strip()\n",
    "            \n",
    "            predictions.append(generated)\n",
    "            references.append(item['output'])\n",
    "        \n",
    "        # Compute all metrics\n",
    "        print(\"\\nComputing metrics...\")\n",
    "        results = {}\n",
    "        \n",
    "        # BLEU\n",
    "        bleu_scores = compute_bleu(predictions, references)\n",
    "        results.update(bleu_scores)\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge_scores = compute_rouge(predictions, references)\n",
    "        for metric_name, scores in rouge_scores.items():\n",
    "            results[f'{metric_name}_f1'] = scores['f1']\n",
    "        \n",
    "        # Exact Match & F1\n",
    "        results['exact_match'] = compute_exact_match(predictions, references)\n",
    "        results['token_f1'] = compute_token_f1(predictions, references)\n",
    "        \n",
    "        return results, predictions, references\n",
    "    \n",
    "    def print_results(self, results: Dict[str, float]):\n",
    "        \"\"\"Pretty print evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Group by metric type\n",
    "        print(\"\\nüåê BLEU Scores (Translation Quality):\")\n",
    "        for k, v in results.items():\n",
    "            if 'BLEU' in k:\n",
    "                print(f\"  {k}: {v:.2f}\")\n",
    "        \n",
    "        print(\"\\nüìÑ ROUGE Scores (Summarization Quality):\")\n",
    "        for k, v in results.items():\n",
    "            if 'rouge' in k:\n",
    "                print(f\"  {k.upper()}: {v:.2f}\")\n",
    "        \n",
    "        print(\"\\n‚ùì QA Metrics:\")\n",
    "        if 'exact_match' in results:\n",
    "            print(f\"  Exact Match: {results['exact_match']:.2f}%\")\n",
    "        if 'token_f1' in results:\n",
    "            print(f\"  Token F1: {results['token_f1']:.2f}%\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "print(\"‚úÖ ModelEvaluator class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5412e",
   "metadata": {},
   "source": [
    "## 6. Example Evaluation üß™\n",
    "\n",
    "Let's evaluate on a test set (using dummy data for demonstration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_data = [\n",
    "    {\"instruction\": \"Translate to French\", \"input\": \"Good morning\", \"output\": \"Bonjour\"},\n",
    "    {\"instruction\": \"Answer the question\", \"input\": \"What is 3+3?\", \"output\": \"3+3 equals 6\"},\n",
    "    {\"instruction\": \"Summarize\", \"input\": \"AI is amazing\", \"output\": \"AI is great\"},\n",
    "]\n",
    "\n",
    "# For demonstration, let's simulate predictions\n",
    "# (In practice, you'd load your trained model from Tutorial 4)\n",
    "simulated_predictions = [\n",
    "    \"bonjour\",\n",
    "    \"the answer is 6\",\n",
    "    \"artificial intelligence is amazing\",\n",
    "]\n",
    "\n",
    "references = [item['output'] for item in test_data]\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Computing evaluation metrics...\\n\")\n",
    "\n",
    "bleu = compute_bleu(simulated_predictions, references)\n",
    "rouge = compute_rouge(simulated_predictions, references)\n",
    "em = compute_exact_match(simulated_predictions, references)\n",
    "f1 = compute_token_f1(simulated_predictions, references)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS (Simulated)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüåê BLEU Scores:\")\n",
    "for k, v in bleu.items():\n",
    "    print(f\"  {k}: {v:.2f}\")\n",
    "\n",
    "print(\"\\nüìÑ ROUGE Scores:\")\n",
    "for metric_name, scores in rouge.items():\n",
    "    print(f\"  {metric_name.upper()} F1: {scores['f1']:.2f}\")\n",
    "\n",
    "print(\"\\n‚ùì QA Metrics:\")\n",
    "print(f\"  Exact Match: {em:.2f}%\")\n",
    "print(f\"  Token F1: {f1:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nüìù Generation Examples:\\n\")\n",
    "for i, (pred, ref) in enumerate(zip(simulated_predictions, references)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Reference:  {ref}\")\n",
    "    print(f\"  Predicted:  {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eae2b6",
   "metadata": {},
   "source": [
    "## 7. Visualize Evaluation Results üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(results: Dict[str, float], title: str = \"Model Evaluation\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of evaluation metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. BLEU scores\n",
    "    bleu_metrics = {k: v for k, v in results.items() if 'BLEU' in k}\n",
    "    if bleu_metrics:\n",
    "        axes[0, 0].bar(bleu_metrics.keys(), bleu_metrics.values(), color='skyblue')\n",
    "        axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "        axes[0, 0].set_title('BLEU Scores', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_ylim([0, 100])\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. ROUGE scores\n",
    "    rouge_metrics = {k.upper(): v for k, v in results.items() if 'rouge' in k and 'f1' in k}\n",
    "    if rouge_metrics:\n",
    "        axes[0, 1].bar(rouge_metrics.keys(), rouge_metrics.values(), color='lightcoral')\n",
    "        axes[0, 1].set_ylabel('F1 Score', fontsize=12)\n",
    "        axes[0, 1].set_title('ROUGE F1 Scores', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_ylim([0, 100])\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. QA metrics\n",
    "    qa_metrics = {}\n",
    "    if 'exact_match' in results:\n",
    "        qa_metrics['Exact Match'] = results['exact_match']\n",
    "    if 'token_f1' in results:\n",
    "        qa_metrics['Token F1'] = results['token_f1']\n",
    "    \n",
    "    if qa_metrics:\n",
    "        axes[1, 0].bar(qa_metrics.keys(), qa_metrics.values(), color='lightgreen')\n",
    "        axes[1, 0].set_ylabel('Score (%)', fontsize=12)\n",
    "        axes[1, 0].set_title('QA Metrics', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_ylim([0, 100])\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Overall summary\n",
    "    summary_metrics = {\n",
    "        'BLEU-4': results.get('BLEU-4', 0),\n",
    "        'ROUGE-L': results.get('rougeL_f1', 0),\n",
    "        'Token F1': results.get('token_f1', 0),\n",
    "    }\n",
    "    colors_sum = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    axes[1, 1].bar(summary_metrics.keys(), summary_metrics.values(), color=colors_sum)\n",
    "    axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1, 1].set_title('Overall Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylim([0, 100])\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize our simulated results\n",
    "all_results = {**bleu, **{f'{k}_f1': v['f1'] for k, v in rouge.items()}, 'exact_match': em, 'token_f1': f1}\n",
    "visualize_metrics(all_results, title=\"Fine-Tuned Model Evaluation (Simulated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3e30b",
   "metadata": {},
   "source": [
    "## 8. Before/After Comparison üîÑ\n",
    "\n",
    "Compare base model vs fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(base_results: Dict[str, float], finetuned_results: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Compare base vs fine-tuned model performance.\n",
    "    \"\"\"\n",
    "    # Select key metrics\n",
    "    metrics = ['BLEU-4', 'rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'token_f1']\n",
    "    metric_labels = ['BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Token F1']\n",
    "    \n",
    "    base_scores = [base_results.get(m, 0) for m in metrics]\n",
    "    ft_scores = [finetuned_results.get(m, 0) for m in metrics]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    x = np.arange(len(metric_labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars1 = ax.bar(x - width/2, base_scores, width, label='Base Model', color='lightgray')\n",
    "    bars2 = ax.bar(x + width/2, ft_scores, width, label='Fine-Tuned', color='green')\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Base vs Fine-Tuned Model Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add improvement percentages\n",
    "    for i, (base, ft) in enumerate(zip(base_scores, ft_scores)):\n",
    "        if base > 0:\n",
    "            improvement = ((ft - base) / base) * 100\n",
    "            ax.text(i, max(base, ft) + 3, f\"+{improvement:.1f}%\", \n",
    "                   ha='center', fontsize=10, fontweight='bold', color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nüìä Improvement Summary:\\n\")\n",
    "    for label, base, ft in zip(metric_labels, base_scores, ft_scores):\n",
    "        improvement = ft - base\n",
    "        pct_improvement = (improvement / base * 100) if base > 0 else 0\n",
    "        print(f\"{label:12} | Base: {base:5.2f} | Fine-tuned: {ft:5.2f} | +{improvement:5.2f} ({pct_improvement:+.1f}%)\")\n",
    "\n",
    "\n",
    "# Simulate base model results (typically lower)\n",
    "base_results = {\n",
    "    'BLEU-4': 15.0,\n",
    "    'rouge1_f1': 30.0,\n",
    "    'rouge2_f1': 10.0,\n",
    "    'rougeL_f1': 25.0,\n",
    "    'token_f1': 35.0,\n",
    "}\n",
    "\n",
    "# Our fine-tuned results (higher)\n",
    "finetuned_results = all_results\n",
    "\n",
    "compare_models(base_results, finetuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e012874",
   "metadata": {},
   "source": [
    "## 9. Summary & Best Practices üìù\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ **Perplexity**: Language modeling quality  \n",
    "‚úÖ **BLEU**: Translation and generation n-gram overlap  \n",
    "‚úÖ **ROUGE**: Summarization recall and overlap  \n",
    "‚úÖ **Exact Match & F1**: Question answering accuracy  \n",
    "‚úÖ **Comprehensive evaluation suite**  \n",
    "‚úÖ **Visualization techniques**  \n",
    "‚úÖ **Before/after comparison**  \n",
    "\n",
    "### Choosing Metrics:\n",
    "\n",
    "| Task | Primary Metrics | Secondary Metrics |\n",
    "|------|----------------|------------------|\n",
    "| **Translation** | BLEU-4 | ROUGE-L, Perplexity |\n",
    "| **Summarization** | ROUGE-1, ROUGE-L | BLEU-4 |\n",
    "| **Question Answering** | Exact Match, F1 | ROUGE-L |\n",
    "| **Text Generation** | Perplexity, BLEU | Human evaluation |\n",
    "| **Dialogue** | Perplexity | Human evaluation, Diversity |\n",
    "\n",
    "### Metric Interpretation:\n",
    "\n",
    "**Perplexity:**\n",
    "- <20: Excellent\n",
    "- 20-50: Good\n",
    "- 50-100: Acceptable\n",
    "- >100: Poor\n",
    "\n",
    "**BLEU (0-100):**\n",
    "- >40: Excellent\n",
    "- 30-40: Good\n",
    "- 20-30: Acceptable\n",
    "- <20: Poor\n",
    "\n",
    "**ROUGE F1 (0-100):**\n",
    "- >50: Excellent\n",
    "- 40-50: Good\n",
    "- 30-40: Acceptable\n",
    "- <30: Poor\n",
    "\n",
    "### Important Considerations:\n",
    "\n",
    "1. **Automatic metrics ‚â† Human quality**\n",
    "   - BLEU/ROUGE measure overlap, not meaning\n",
    "   - Always validate with human evaluation\n",
    "   - Check for hallucinations and factual errors\n",
    "\n",
    "2. **Task-specific evaluation**\n",
    "   - Use domain-appropriate metrics\n",
    "   - Consider task-specific constraints\n",
    "   - Balance multiple metrics\n",
    "\n",
    "3. **Statistical significance**\n",
    "   - Test on large, diverse datasets\n",
    "   - Report confidence intervals\n",
    "   - Use multiple random seeds\n",
    "\n",
    "4. **Beyond metrics**\n",
    "   - Inference speed\n",
    "   - Memory usage\n",
    "   - Robustness to adversarial inputs\n",
    "   - Fairness and bias\n",
    "\n",
    "### Production Evaluation:\n",
    "\n",
    "```python\n",
    "# Comprehensive evaluation pipeline\n",
    "1. Automatic metrics (BLEU, ROUGE, etc.)\n",
    "2. Human evaluation (fluency, relevance, factuality)\n",
    "3. A/B testing in production\n",
    "4. Monitor user feedback\n",
    "5. Continuous evaluation on new data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've completed the entire LLM Fine-Tuning tutorial series!\n",
    "\n",
    "### What you've mastered:\n",
    "\n",
    "1. ‚úÖ **Tutorial 1**: Fine-tuning concepts and LoRA theory\n",
    "2. ‚úÖ **Tutorial 2**: LoRA implementation from scratch\n",
    "3. ‚úÖ **Tutorial 3**: Data preparation and tokenization\n",
    "4. ‚úÖ **Tutorial 4**: Complete instruction tuning pipeline\n",
    "5. ‚úÖ **Tutorial 5**: Comprehensive evaluation metrics\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Scale to larger models (7B, 13B, 70B)\n",
    "- Try QLoRA for even lower memory\n",
    "- Experiment with different tasks\n",
    "- Deploy with inference optimization\n",
    "- Contribute to open source!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "**Papers:**\n",
    "- [papers/DeepSeek-R1-paper.pdf](../papers/DeepSeek-R1-paper.pdf) - Complete methodology\n",
    "- BLEU: https://www.aclweb.org/anthology/P02-1040.pdf\n",
    "- ROUGE: https://www.aclweb.org/anthology/W04-1013.pdf\n",
    "\n",
    "**Tools:**\n",
    "- Hugging Face Evaluate: https://huggingface.co/docs/evaluate\n",
    "- NLTK: https://www.nltk.org/\n",
    "- ROUGE Score: https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "**Related:**\n",
    "- Complete tutorial series: [llm-fine-tune/](./)\n",
    "- Transformer foundations: [transformer-foundation/](../transformer-foundation/)\n",
    "- Source code: [src/](../src/)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial series! Happy fine-tuning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
