{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cde291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy pandas tqdm\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "if not IN_COLAB:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our implementations\n",
    "from src.modules.multi_head_attention import MultiHeadAttention\n",
    "from src.modules.encoder import TransformerEncoder, EncoderLayer\n",
    "from src.modules.feed_forward import PositionWiseFeedForward\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf378f",
   "metadata": {},
   "source": [
    "## 1. LoRA Layer Implementation üîß\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For a linear layer with weight matrix $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "\n",
    "**Standard forward pass:**\n",
    "$$y = Wx$$\n",
    "\n",
    "**LoRA forward pass:**\n",
    "$$y = Wx + \\frac{\\alpha}{r}BAx$$\n",
    "\n",
    "Where:\n",
    "- $W$ is frozen (pre-trained)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable\n",
    "- $r$ is the rank (typically 4, 8, 16, 32)\n",
    "- $\\alpha$ is a scaling factor (typically 16 or 32)\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09074aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) Layer\n",
    "    \n",
    "    Adds trainable low-rank decomposition to a frozen linear layer.\n",
    "    Based on: https://arxiv.org/abs/2106.09685\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Frozen pre-trained weight\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_features, in_features),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        # Bias (optional)\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.zeros(out_features),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        # LoRA matrices (trainable)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Dropout for LoRA path\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize LoRA matrices\"\"\"\n",
    "        # Initialize A with Kaiming uniform (like nn.Linear)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        # Initialize B with zeros (so LoRA starts as identity)\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: y = Wx + dropout(BAx) * scaling\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (..., in_features)\n",
    "        Returns:\n",
    "            Output tensor (..., out_features)\n",
    "        \"\"\"\n",
    "        # Original frozen path\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "        \n",
    "        # LoRA path: x @ A^T @ B^T\n",
    "        x_lora = self.dropout(x)\n",
    "        lora_result = F.linear(F.linear(x_lora, self.lora_A), self.lora_B)\n",
    "        \n",
    "        # Scale and add\n",
    "        result = result + lora_result * self.scaling\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def merge_weights(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Merge LoRA weights into base weights for inference.\n",
    "        \n",
    "        Returns:\n",
    "            Merged weight matrix\n",
    "        \"\"\"\n",
    "        # W' = W + BA * scaling\n",
    "        delta_w = (self.lora_B @ self.lora_A) * self.scaling\n",
    "        return self.weight + delta_w\n",
    "    \n",
    "    def unmerge_weights(self, merged_weight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Restore original weights from merged weights.\n",
    "        Useful for switching between tasks.\n",
    "        \"\"\"\n",
    "        delta_w = (self.lora_B @ self.lora_A) * self.scaling\n",
    "        self.weight.data = merged_weight - delta_w\n",
    "\n",
    "\n",
    "# Test LoRA layer\n",
    "print(\"Testing LoRA Layer...\\n\")\n",
    "lora = LoRALayer(512, 512, rank=8, alpha=16.0)\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = lora(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Frozen: {sum(p.numel() for p in lora.parameters() if not p.requires_grad):,}\")\n",
    "print(f\"  Trainable (LoRA): {sum(p.numel() for p in lora.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Ratio: {sum(p.numel() for p in lora.parameters() if p.requires_grad) / sum(p.numel() for p in lora.parameters()) * 100:.2f}%\")\n",
    "print(\"\\n‚úÖ LoRA layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb18a5",
   "metadata": {},
   "source": [
    "## 2. Applying LoRA to Multi-Head Attention üéØ\n",
    "\n",
    "### Reference to transformer-foundation\n",
    "\n",
    "From [transformer-foundation/03_multi_head_attention.ipynb](../transformer-foundation/03_multi_head_attention.ipynb), we learned that multi-head attention has 4 linear projections:\n",
    "\n",
    "```python\n",
    "W_q: Query projection  (d_model ‚Üí d_model)\n",
    "W_k: Key projection    (d_model ‚Üí d_model)\n",
    "W_v: Value projection  (d_model ‚Üí d_model)\n",
    "W_o: Output projection (d_model ‚Üí d_model)\n",
    "```\n",
    "\n",
    "**Common LoRA configurations:**\n",
    "1. **Q + V only** (most efficient)\n",
    "2. **Q + K + V + O** (maximum adaptation)\n",
    "3. **Q + V + O** (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with LoRA adapters.\n",
    "    \n",
    "    Based on src/modules/multi_head_attention.py with LoRA modifications.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        lora_rank: int = 8,\n",
    "        lora_alpha: float = 16.0,\n",
    "        lora_dropout: float = 0.0,\n",
    "        apply_lora_to: list = ['q', 'v']  # Which projections to apply LoRA\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.apply_lora_to = [x.lower() for x in apply_lora_to]\n",
    "        \n",
    "        # Create projections (with or without LoRA)\n",
    "        self.W_q = self._create_projection('q', lora_rank, lora_alpha, lora_dropout)\n",
    "        self.W_k = self._create_projection('k', lora_rank, lora_alpha, lora_dropout)\n",
    "        self.W_v = self._create_projection('v', lora_rank, lora_alpha, lora_dropout)\n",
    "        self.W_o = self._create_projection('o', lora_rank, lora_alpha, lora_dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _create_projection(self, name: str, rank: int, alpha: float, dropout: float):\n",
    "        \"\"\"Create projection with or without LoRA\"\"\"\n",
    "        if name in self.apply_lora_to:\n",
    "            return LoRALayer(self.d_model, self.d_model, rank, alpha, dropout)\n",
    "        else:\n",
    "            # Standard linear layer (frozen for fine-tuning)\n",
    "            layer = nn.Linear(self.d_model, self.d_model)\n",
    "            layer.weight.requires_grad = False\n",
    "            layer.bias.requires_grad = False\n",
    "            return layer\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention (from transformer-foundation/02)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and apply output projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# Test LoRA Multi-Head Attention\n",
    "print(\"Testing LoRA Multi-Head Attention...\\n\")\n",
    "lora_mha = LoRAMultiHeadAttention(\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    lora_rank=8,\n",
    "    apply_lora_to=['q', 'v']  # Most efficient configuration\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "output, attn_weights = lora_mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nParameter counts:\")\n",
    "trainable = sum(p.numel() for p in lora_mha.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lora_mha.parameters())\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable (LoRA): {trainable:,}\")\n",
    "print(f\"  Ratio: {trainable / total * 100:.2f}%\")\n",
    "print(\"\\n‚úÖ LoRA Multi-Head Attention works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977dce74",
   "metadata": {},
   "source": [
    "## 3. Memory Comparison: Full vs LoRA üìä\n",
    "\n",
    "Let's compare memory usage for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d62cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "# Create models for comparison\n",
    "d_model = 768  # BERT-base size\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# Full fine-tuning attention\n",
    "full_attn = nn.ModuleList([\n",
    "    MultiHeadAttention(d_model, n_heads)\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "# LoRA attention (Q+V only, rank=8)\n",
    "lora_attn_qv = nn.ModuleList([\n",
    "    LoRAMultiHeadAttention(d_model, n_heads, lora_rank=8, apply_lora_to=['q', 'v'])\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "# LoRA attention (all projections, rank=8)\n",
    "lora_attn_all = nn.ModuleList([\n",
    "    LoRAMultiHeadAttention(d_model, n_heads, lora_rank=8, apply_lora_to=['q', 'k', 'v', 'o'])\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "# LoRA attention (Q+V, rank=16)\n",
    "lora_attn_r16 = nn.ModuleList([\n",
    "    LoRAMultiHeadAttention(d_model, n_heads, lora_rank=16, apply_lora_to=['q', 'v'])\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "# Count parameters\n",
    "results = {}\n",
    "for name, model in [\n",
    "    ('Full Fine-Tuning', full_attn),\n",
    "    ('LoRA (Q+V, r=8)', lora_attn_qv),\n",
    "    ('LoRA (Q+K+V+O, r=8)', lora_attn_all),\n",
    "    ('LoRA (Q+V, r=16)', lora_attn_r16),\n",
    "]:\n",
    "    trainable, total = count_parameters(model)\n",
    "    results[name] = {\n",
    "        'trainable': trainable,\n",
    "        'total': total,\n",
    "        'ratio': trainable / total * 100\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Parameter Comparison (12-layer BERT-base size)\\n\")\n",
    "print(f\"{'Strategy':<25} {'Trainable':>12} {'Total':>12} {'Ratio':>10}\")\n",
    "print(\"-\" * 65)\n",
    "for name, data in results.items():\n",
    "    print(f\"{name:<25} {data['trainable']:>12,} {data['total']:>12,} {data['ratio']:>9.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of trainable parameters\n",
    "names = list(results.keys())\n",
    "trainable_counts = [results[n]['trainable'] / 1e6 for n in names]\n",
    "\n",
    "ax1.bar(range(len(names)), trainable_counts, color=['red', 'green', 'blue', 'orange'])\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Trainable Parameters (Millions)', fontsize=12)\n",
    "ax1.set_title('Trainable Parameters Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart showing ratio\n",
    "ratios = [results[n]['ratio'] for n in names]\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "ax2.bar(range(len(names)), ratios, color=colors)\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels(names, rotation=15, ha='right')\n",
    "ax2.set_ylabel('Trainable Percentage (%)', fontsize=12)\n",
    "ax2.set_title('Parameter Efficiency', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Memory estimate\n",
    "full_trainable = results['Full Fine-Tuning']['trainable']\n",
    "lora_trainable = results['LoRA (Q+V, r=8)']['trainable']\n",
    "memory_saving = full_trainable / lora_trainable\n",
    "\n",
    "print(f\"\\nüí° Memory Insights:\")\n",
    "print(f\"  LoRA (Q+V, r=8) uses {memory_saving:.1f}x FEWER trainable parameters!\")\n",
    "print(f\"  For gradient storage: ~{memory_saving:.1f}x less GPU memory required\")\n",
    "print(f\"  Training speedup: ~{memory_saving/2:.1f}x faster (approximate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd953f",
   "metadata": {},
   "source": [
    "## 4. Weight Merging for Inference ‚ö°\n",
    "\n",
    "During inference, we can merge LoRA weights into the base weights to avoid any additional computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create LoRA layer\n",
    "lora_layer = LoRALayer(512, 512, rank=8)\n",
    "x = torch.randn(100, 512).to(device)\n",
    "lora_layer = lora_layer.to(device)\n",
    "\n",
    "# Time LoRA inference (with separate paths)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = lora_layer(x)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "lora_time = time.time() - start\n",
    "\n",
    "# Merge weights\n",
    "merged_weight = lora_layer.merge_weights()\n",
    "\n",
    "# Create standard linear layer with merged weights\n",
    "merged_layer = nn.Linear(512, 512, bias=True).to(device)\n",
    "merged_layer.weight.data = merged_weight\n",
    "merged_layer.bias.data = lora_layer.bias\n",
    "\n",
    "# Time merged inference\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = merged_layer(x)\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "merged_time = time.time() - start\n",
    "\n",
    "# Compare outputs (should be identical)\n",
    "lora_output = lora_layer(x)\n",
    "merged_output = merged_layer(x)\n",
    "max_diff = torch.max(torch.abs(lora_output - merged_output)).item()\n",
    "\n",
    "print(\"\\n‚ö° Inference Speed Comparison\\n\")\n",
    "print(f\"LoRA (separate paths): {lora_time*1000:.2f} ms\")\n",
    "print(f\"Merged weights:        {merged_time*1000:.2f} ms\")\n",
    "print(f\"Speedup:               {lora_time/merged_time:.2f}x\")\n",
    "print(f\"\\nOutput difference: {max_diff:.2e} (should be ~0)\")\n",
    "print(\"\\nüí° For production: merge weights to eliminate overhead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577f6c8",
   "metadata": {},
   "source": [
    "## 5. Applying LoRA to Complete Encoder üèóÔ∏è\n",
    "\n",
    "Let's apply LoRA to a complete encoder stack, referencing our implementation from `src/modules/encoder.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer with LoRA adapters.\n",
    "    Based on src/modules/encoder.py with LoRA modifications.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        lora_rank: int = 8,\n",
    "        lora_alpha: float = 16.0,\n",
    "        apply_lora_to_attn: list = ['q', 'v'],\n",
    "        apply_lora_to_ffn: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention with LoRA\n",
    "        self.self_attn = LoRAMultiHeadAttention(\n",
    "            d_model, n_heads, dropout, \n",
    "            lora_rank, lora_alpha, 0.0, \n",
    "            apply_lora_to_attn\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-forward network (with optional LoRA)\n",
    "        if apply_lora_to_ffn:\n",
    "            self.ffn = nn.Sequential(\n",
    "                LoRALayer(d_model, d_ff, lora_rank, lora_alpha),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                LoRALayer(d_ff, d_model, lora_rank, lora_alpha)\n",
    "            )\n",
    "        else:\n",
    "            # Standard FFN (frozen)\n",
    "            ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "            for param in ffn.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.ffn = ffn\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # FFN with residual\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class LoRAEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete encoder with LoRA adapters.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int = 6,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        lora_rank: int = 8,\n",
    "        lora_alpha: float = 16.0,\n",
    "        apply_lora_to_attn: list = ['q', 'v'],\n",
    "        apply_lora_to_ffn: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            LoRAEncoderLayer(\n",
    "                d_model, n_heads, d_ff, dropout,\n",
    "                lora_rank, lora_alpha,\n",
    "                apply_lora_to_attn, apply_lora_to_ffn\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Test LoRA Encoder\n",
    "print(\"Testing LoRA Encoder...\\n\")\n",
    "lora_encoder = LoRAEncoder(\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    lora_rank=8,\n",
    "    apply_lora_to_attn=['q', 'v'],\n",
    "    apply_lora_to_ffn=False\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "output = lora_encoder(x)\n",
    "\n",
    "trainable, total = count_parameters(lora_encoder)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable (LoRA): {trainable:,}\")\n",
    "print(f\"  Ratio: {trainable / total * 100:.2f}%\")\n",
    "print(\"\\n‚úÖ LoRA Encoder works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0452b7",
   "metadata": {},
   "source": [
    "## 6. Practical Training Example üéì\n",
    "\n",
    "Let's see how to set up a simple training loop with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and dummy data\n",
    "model = LoRAEncoder(\n",
    "    n_layers=3,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    lora_rank=8\n",
    ").to(device)\n",
    "\n",
    "# Only optimize LoRA parameters!\n",
    "lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n",
    "\n",
    "print(f\"Optimizing {len(lora_params)} LoRA parameter groups\")\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in lora_params):,}\\n\")\n",
    "\n",
    "# Dummy training loop\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for step in tqdm(range(100), desc=\"Training\"):\n",
    "    # Generate dummy data\n",
    "    x = torch.randn(8, 20, 256).to(device)  # (batch, seq_len, d_model)\n",
    "    target = torch.randn(8, 20, 256).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Simple MSE loss (for demonstration)\n",
    "    loss = F.mse_loss(output, target)\n",
    "    \n",
    "    # Backward pass (only updates LoRA parameters)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LoRA Training Loss', fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"\\nüí° Note: Only LoRA parameters were updated during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db9145",
   "metadata": {},
   "source": [
    "## 7. Summary & Best Practices üìù\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ Implemented LoRA layer from scratch  \n",
    "‚úÖ Applied LoRA to multi-head attention  \n",
    "‚úÖ Compared memory usage vs full fine-tuning  \n",
    "‚úÖ Learned weight merging for efficient inference  \n",
    "‚úÖ Built complete LoRA encoder  \n",
    "‚úÖ Set up training loop with LoRA parameters  \n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start with Q+V projections**: Most efficient, good performance\n",
    "2. **Use rank 8-16**: Sweet spot for most tasks\n",
    "3. **Set alpha = 2 √ó rank**: Standard scaling\n",
    "4. **Merge weights for production**: Eliminates inference overhead\n",
    "5. **Monitor trainable param ratio**: Aim for <1% for large models\n",
    "\n",
    "### Configuration Guide:\n",
    "\n",
    "```python\n",
    "# For experimentation (fastest)\n",
    "apply_lora_to = ['q', 'v']\n",
    "rank = 8\n",
    "\n",
    "# For better performance\n",
    "apply_lora_to = ['q', 'v', 'o']\n",
    "rank = 16\n",
    "\n",
    "# For maximum adaptation\n",
    "apply_lora_to = ['q', 'k', 'v', 'o']\n",
    "rank = 32\n",
    "apply_lora_to_ffn = True\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Tutorial 3**: Data preparation for fine-tuning\n",
    "- **Tutorial 4**: Instruction tuning with LoRA\n",
    "- **Tutorial 5**: Evaluation and metrics\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "**Papers:**\n",
    "- LoRA: https://arxiv.org/abs/2106.09685\n",
    "- QLoRA: https://arxiv.org/abs/2305.14314\n",
    "\n",
    "**Code:**\n",
    "- Our implementation: [src/modules/](../src/modules/)\n",
    "- Hugging Face PEFT: https://github.com/huggingface/peft\n",
    "\n",
    "**Related:**\n",
    "- [transformer-foundation/03_multi_head_attention.ipynb](../transformer-foundation/03_multi_head_attention.ipynb)\n",
    "- [transformer-foundation/04_feed_forward_networks.ipynb](../transformer-foundation/04_feed_forward_networks.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for data preparation? Continue to Tutorial 3! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
