{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy pandas tqdm datasets transformers\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "if not IN_COLAB:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab778f51",
   "metadata": {},
   "source": [
    "## 1. Data Formats for Fine-Tuning üìù\n",
    "\n",
    "### Common Formats:\n",
    "\n",
    "1. **Prompt-Completion** (Simple)\n",
    "```json\n",
    "{\"prompt\": \"Translate to French: Hello\", \"completion\": \"Bonjour\"}\n",
    "```\n",
    "\n",
    "2. **Instruction Format** (Alpaca-style)\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Summarize the following text\",\n",
    "  \"input\": \"[long text]\",\n",
    "  \"output\": \"[summary]\"\n",
    "}\n",
    "```\n",
    "\n",
    "3. **Chat Format** (Multi-turn)\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Let's create sample datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datasets in different formats\n",
    "\n",
    "# 1. Prompt-Completion\n",
    "prompt_completion_data = [\n",
    "    {\"prompt\": \"Translate to French: Hello\", \"completion\": \"Bonjour\"},\n",
    "    {\"prompt\": \"Translate to French: Thank you\", \"completion\": \"Merci\"},\n",
    "    {\"prompt\": \"Translate to Spanish: Good morning\", \"completion\": \"Buenos d√≠as\"},\n",
    "    {\"prompt\": \"What is the capital of France?\", \"completion\": \"The capital of France is Paris.\"},\n",
    "]\n",
    "\n",
    "# 2. Instruction Format (Alpaca-style)\n",
    "instruction_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept in simple terms\",\n",
    "        \"input\": \"Quantum entanglement\",\n",
    "        \"output\": \"Quantum entanglement is when two particles become connected so that the state of one instantly affects the other, no matter how far apart they are.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about the topic\",\n",
    "        \"input\": \"Spring\",\n",
    "        \"output\": \"Cherry blossoms fall,\\nGentle breeze whispers softly,\\nSpring awakens joy.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize this text\",\n",
    "        \"input\": \"The Transformer architecture, introduced in 2017, revolutionized natural language processing by replacing recurrent connections with self-attention mechanisms.\",\n",
    "        \"output\": \"Transformers (2017) replaced RNNs with self-attention for NLP tasks.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# 3. Chat Format\n",
    "chat_data = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in math.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the derivative of x^2?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The derivative of x^2 is 2x.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a coding tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"How do I reverse a string in Python?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"You can reverse a string using slicing: `s[::-1]` or `''.join(reversed(s))`.\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Sample datasets created!\")\n",
    "print(f\"  Prompt-Completion: {len(prompt_completion_data)} examples\")\n",
    "print(f\"  Instruction: {len(instruction_data)} examples\")\n",
    "print(f\"  Chat: {len(chat_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d6257",
   "metadata": {},
   "source": [
    "## 2. Simple Tokenizer Implementation üî§\n",
    "\n",
    "For demonstration, we'll implement a simple BPE-like tokenizer. In production, use Hugging Face tokenizers.\n",
    "\n",
    "Reference: [transformer-foundation/01_embeddings_and_positional_encoding.ipynb](../transformer-foundation/01_embeddings_and_positional_encoding.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Simple word-based tokenizer for demonstration.\n",
    "    In production, use Hugging Face tokenizers (BPE, WordPiece, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        # Special tokens (standard for LLMs)\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.bos_token = \"<BOS>\"  # Beginning of sequence\n",
    "        self.eos_token = \"<EOS>\"  # End of sequence\n",
    "        \n",
    "        # Initialize vocab with special tokens\n",
    "        self.token2id = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1,\n",
    "            self.bos_token: 2,\n",
    "            self.eos_token: 3,\n",
    "        }\n",
    "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
    "        self.next_id = 4\n",
    "        \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from corpus\"\"\"\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        # Add most frequent words to vocab\n",
    "        for word, _ in word_freq.most_common(self.vocab_size - 4):\n",
    "            if word not in self.token2id:\n",
    "                self.token2id[word] = self.next_id\n",
    "                self.id2token[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "        \n",
    "        print(f\"Built vocabulary: {len(self.token2id)} tokens\")\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        words = text.lower().split()\n",
    "        ids = [self.token2id.get(w, self.token2id[self.unk_token]) for w in words]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            ids = [self.token2id[self.bos_token]] + ids + [self.token2id[self.eos_token]]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        special_ids = {self.token2id[t] for t in [self.pad_token, self.bos_token, self.eos_token]}\n",
    "        \n",
    "        words = []\n",
    "        for id in ids:\n",
    "            if skip_special_tokens and id in special_ids:\n",
    "                continue\n",
    "            words.append(self.id2token.get(id, self.unk_token))\n",
    "        \n",
    "        return \" \".join(words)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.token2id[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self.token2id[self.eos_token]\n",
    "\n",
    "\n",
    "# Build tokenizer from our sample data\n",
    "all_texts = []\n",
    "for item in prompt_completion_data:\n",
    "    all_texts.extend([item['prompt'], item['completion']])\n",
    "for item in instruction_data:\n",
    "    all_texts.extend([item['instruction'], item['input'], item['output']])\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=1000)\n",
    "tokenizer.build_vocab(all_texts)\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"Hello, how are you?\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTokenizer Test:\")\n",
    "print(f\"  Original: {test_text}\")\n",
    "print(f\"  Encoded: {encoded}\")\n",
    "print(f\"  Decoded: {decoded}\")\n",
    "print(\"\\n‚úÖ Tokenizer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b02c7",
   "metadata": {},
   "source": [
    "## 3. Dataset Classes for Different Formats üóÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCompletionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for prompt-completion format.\n",
    "    Used for: simple Q&A, translation, etc.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: List[Dict],\n",
    "        tokenizer: SimpleTokenizer,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Concatenate prompt and completion\n",
    "        # Format: <BOS> prompt <EOS> completion <EOS>\n",
    "        prompt_ids = self.tokenizer.encode(item['prompt'], add_special_tokens=False)\n",
    "        completion_ids = self.tokenizer.encode(item['completion'], add_special_tokens=False)\n",
    "        \n",
    "        # Add special tokens manually for more control\n",
    "        input_ids = (\n",
    "            [self.tokenizer.token2id[self.tokenizer.bos_token]] +\n",
    "            prompt_ids +\n",
    "            [self.tokenizer.token2id[self.tokenizer.eos_token]] +\n",
    "            completion_ids +\n",
    "            [self.tokenizer.token2id[self.tokenizer.eos_token]]\n",
    "        )\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        \n",
    "        # Labels: -100 for prompt (don't compute loss), actual IDs for completion\n",
    "        labels = [-100] * (len(prompt_ids) + 2) + completion_ids + [self.tokenizer.eos_token_id]\n",
    "        if len(labels) > self.max_length:\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for instruction format (Alpaca-style).\n",
    "    Used for: instruction tuning, task adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        tokenizer: SimpleTokenizer,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format: <BOS> Instruction: {inst} Input: {input} Output: {output} <EOS>\n",
    "        prompt = f\"Instruction: {item['instruction']} Input: {item['input']} Output:\"\n",
    "        completion = item['output']\n",
    "        \n",
    "        prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        completion_ids = self.tokenizer.encode(completion, add_special_tokens=False)\n",
    "        \n",
    "        input_ids = (\n",
    "            [self.tokenizer.token2id[self.tokenizer.bos_token]] +\n",
    "            prompt_ids +\n",
    "            completion_ids +\n",
    "            [self.tokenizer.token2id[self.tokenizer.eos_token]]\n",
    "        )\n",
    "        \n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        \n",
    "        # Only compute loss on output\n",
    "        labels = ([-100] * (len(prompt_ids) + 1) + \n",
    "                 completion_ids + \n",
    "                 [self.tokenizer.eos_token_id])\n",
    "        if len(labels) > self.max_length:\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Test datasets\n",
    "pc_dataset = PromptCompletionDataset(prompt_completion_data, tokenizer, max_length=128)\n",
    "inst_dataset = InstructionDataset(instruction_data, tokenizer, max_length=128)\n",
    "\n",
    "print(\"Testing datasets...\\n\")\n",
    "print(f\"Prompt-Completion Dataset: {len(pc_dataset)} examples\")\n",
    "sample = pc_dataset[0]\n",
    "print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  Decoded input: {tokenizer.decode(sample['input_ids'].tolist())}\")\n",
    "\n",
    "print(f\"\\nInstruction Dataset: {len(inst_dataset)} examples\")\n",
    "sample = inst_dataset[0]\n",
    "print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Datasets work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c356df0",
   "metadata": {},
   "source": [
    "## 4. Data Collator with Padding üì¶\n",
    "\n",
    "Efficiently batch variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForLanguageModeling:\n",
    "    \"\"\"\n",
    "    Collate batch of examples with padding.\n",
    "    Based on Hugging Face's DataCollator.\n",
    "    \"\"\"\n",
    "    tokenizer: SimpleTokenizer\n",
    "    max_length: int = 512\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        # Find max length in batch\n",
    "        batch_max = max(len(ex['input_ids']) for ex in examples)\n",
    "        \n",
    "        # Optionally pad to multiple (for TPU efficiency)\n",
    "        if self.pad_to_multiple_of is not None:\n",
    "            batch_max = (\n",
    "                (batch_max + self.pad_to_multiple_of - 1) \n",
    "                // self.pad_to_multiple_of \n",
    "                * self.pad_to_multiple_of\n",
    "            )\n",
    "        \n",
    "        batch_max = min(batch_max, self.max_length)\n",
    "        \n",
    "        # Pad all sequences\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        attention_mask = []\n",
    "        \n",
    "        for ex in examples:\n",
    "            seq_len = len(ex['input_ids'])\n",
    "            padding_len = batch_max - seq_len\n",
    "            \n",
    "            # Pad input_ids\n",
    "            padded_input = torch.cat([\n",
    "                ex['input_ids'],\n",
    "                torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            input_ids.append(padded_input)\n",
    "            \n",
    "            # Pad labels (use -100 for padding tokens)\n",
    "            padded_labels = torch.cat([\n",
    "                ex['labels'],\n",
    "                torch.full((padding_len,), -100, dtype=torch.long)\n",
    "            ])\n",
    "            labels.append(padded_labels)\n",
    "            \n",
    "            # Attention mask (1 for real tokens, 0 for padding)\n",
    "            mask = torch.cat([\n",
    "                torch.ones(seq_len, dtype=torch.long),\n",
    "                torch.zeros(padding_len, dtype=torch.long)\n",
    "            ])\n",
    "            attention_mask.append(mask)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids),\n",
    "            'labels': torch.stack(labels),\n",
    "            'attention_mask': torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "\n",
    "# Test data collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, max_length=128, pad_to_multiple_of=8)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    pc_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=collator,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Testing Data Collator...\\n\")\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"\\nSample attention mask:\")\n",
    "print(batch['attention_mask'][0])\n",
    "print(\"\\n‚úÖ Data collator works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08068983",
   "metadata": {},
   "source": [
    "## 5. Data Quality Control üîç\n",
    "\n",
    "Filter and validate data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Check and filter data for quality issues.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: SimpleTokenizer,\n",
    "        min_length: int = 10,\n",
    "        max_length: int = 1024,\n",
    "        max_prompt_completion_ratio: float = 10.0\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.max_ratio = max_prompt_completion_ratio\n",
    "        \n",
    "        self.stats = {\n",
    "            'total': 0,\n",
    "            'too_short': 0,\n",
    "            'too_long': 0,\n",
    "            'bad_ratio': 0,\n",
    "            'passed': 0\n",
    "        }\n",
    "    \n",
    "    def check_example(self, prompt: str, completion: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Check single example. Returns (is_valid, reason)\"\"\"\n",
    "        self.stats['total'] += 1\n",
    "        \n",
    "        # Check lengths\n",
    "        prompt_len = len(self.tokenizer.encode(prompt, add_special_tokens=False))\n",
    "        completion_len = len(self.tokenizer.encode(completion, add_special_tokens=False))\n",
    "        total_len = prompt_len + completion_len\n",
    "        \n",
    "        if total_len < self.min_length:\n",
    "            self.stats['too_short'] += 1\n",
    "            return False, f\"Too short: {total_len} tokens\"\n",
    "        \n",
    "        if total_len > self.max_length:\n",
    "            self.stats['too_long'] += 1\n",
    "            return False, f\"Too long: {total_len} tokens\"\n",
    "        \n",
    "        # Check prompt/completion ratio (avoid extremely long prompts)\n",
    "        if completion_len > 0:\n",
    "            ratio = prompt_len / completion_len\n",
    "            if ratio > self.max_ratio:\n",
    "                self.stats['bad_ratio'] += 1\n",
    "                return False, f\"Bad ratio: {ratio:.1f}\"\n",
    "        \n",
    "        self.stats['passed'] += 1\n",
    "        return True, \"OK\"\n",
    "    \n",
    "    def filter_dataset(self, data: List[Dict], format_type: str = 'prompt_completion') -> List[Dict]:\n",
    "        \"\"\"Filter dataset and return valid examples\"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for item in data:\n",
    "            if format_type == 'prompt_completion':\n",
    "                is_valid, _ = self.check_example(item['prompt'], item['completion'])\n",
    "            elif format_type == 'instruction':\n",
    "                prompt = f\"{item['instruction']} {item['input']}\"\n",
    "                is_valid, _ = self.check_example(prompt, item['output'])\n",
    "            else:\n",
    "                is_valid = True\n",
    "            \n",
    "            if is_valid:\n",
    "                filtered.append(item)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print filtering statistics\"\"\"\n",
    "        print(\"\\nüìä Data Quality Statistics:\")\n",
    "        print(f\"  Total examples: {self.stats['total']}\")\n",
    "        print(f\"  Passed: {self.stats['passed']} ({self.stats['passed']/self.stats['total']*100:.1f}%)\")\n",
    "        print(f\"  Too short: {self.stats['too_short']}\")\n",
    "        print(f\"  Too long: {self.stats['too_long']}\")\n",
    "        print(f\"  Bad ratio: {self.stats['bad_ratio']}\")\n",
    "\n",
    "\n",
    "# Test quality checker\n",
    "checker = DataQualityChecker(tokenizer, min_length=5, max_length=200)\n",
    "\n",
    "# Check examples\n",
    "filtered_data = checker.filter_dataset(prompt_completion_data, 'prompt_completion')\n",
    "checker.print_stats()\n",
    "\n",
    "print(f\"\\n‚úÖ Filtered data: {len(filtered_data)}/{len(prompt_completion_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ccdb35",
   "metadata": {},
   "source": [
    "## 6. Data Visualization üìà\n",
    "\n",
    "Understand your data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee53383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(data: List[Dict], tokenizer: SimpleTokenizer, format_type: str = 'prompt_completion'):\n",
    "    \"\"\"\n",
    "    Analyze and visualize dataset statistics.\n",
    "    \"\"\"\n",
    "    prompt_lengths = []\n",
    "    completion_lengths = []\n",
    "    total_lengths = []\n",
    "    \n",
    "    for item in data:\n",
    "        if format_type == 'prompt_completion':\n",
    "            prompt = item['prompt']\n",
    "            completion = item['completion']\n",
    "        elif format_type == 'instruction':\n",
    "            prompt = f\"{item['instruction']} {item['input']}\"\n",
    "            completion = item['output']\n",
    "        \n",
    "        p_len = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "        c_len = len(tokenizer.encode(completion, add_special_tokens=False))\n",
    "        \n",
    "        prompt_lengths.append(p_len)\n",
    "        completion_lengths.append(c_len)\n",
    "        total_lengths.append(p_len + c_len)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Histogram of lengths\n",
    "    axes[0, 0].hist(prompt_lengths, bins=20, alpha=0.7, label='Prompt', color='blue')\n",
    "    axes[0, 0].hist(completion_lengths, bins=20, alpha=0.7, label='Completion', color='orange')\n",
    "    axes[0, 0].set_xlabel('Length (tokens)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Token Length Distribution', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    data_to_plot = [prompt_lengths, completion_lengths, total_lengths]\n",
    "    axes[0, 1].boxplot(data_to_plot, labels=['Prompt', 'Completion', 'Total'])\n",
    "    axes[0, 1].set_ylabel('Length (tokens)')\n",
    "    axes[0, 1].set_title('Length Statistics', fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: prompt vs completion length\n",
    "    axes[1, 0].scatter(prompt_lengths, completion_lengths, alpha=0.6, color='green')\n",
    "    axes[1, 0].set_xlabel('Prompt Length (tokens)')\n",
    "    axes[1, 0].set_ylabel('Completion Length (tokens)')\n",
    "    axes[1, 0].set_title('Prompt vs Completion Length', fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Statistics table\n",
    "    stats_data = {\n",
    "        'Metric': ['Mean', 'Median', 'Min', 'Max', 'Std Dev'],\n",
    "        'Prompt': [\n",
    "            np.mean(prompt_lengths),\n",
    "            np.median(prompt_lengths),\n",
    "            np.min(prompt_lengths),\n",
    "            np.max(prompt_lengths),\n",
    "            np.std(prompt_lengths)\n",
    "        ],\n",
    "        'Completion': [\n",
    "            np.mean(completion_lengths),\n",
    "            np.median(completion_lengths),\n",
    "            np.min(completion_lengths),\n",
    "            np.max(completion_lengths),\n",
    "            np.std(completion_lengths)\n",
    "        ],\n",
    "        'Total': [\n",
    "            np.mean(total_lengths),\n",
    "            np.median(total_lengths),\n",
    "            np.min(total_lengths),\n",
    "            np.max(total_lengths),\n",
    "            np.std(total_lengths)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(stats_data)\n",
    "    axes[1, 1].axis('tight')\n",
    "    axes[1, 1].axis('off')\n",
    "    table = axes[1, 1].table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "    axes[1, 1].set_title('Length Statistics', fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'prompt_lengths': prompt_lengths,\n",
    "        'completion_lengths': completion_lengths,\n",
    "        'total_lengths': total_lengths\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze datasets\n",
    "print(\"Analyzing Prompt-Completion Dataset...\")\n",
    "pc_stats = analyze_dataset(prompt_completion_data, tokenizer, 'prompt_completion')\n",
    "\n",
    "print(\"\\nAnalyzing Instruction Dataset...\")\n",
    "inst_stats = analyze_dataset(instruction_data, tokenizer, 'instruction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1cca2",
   "metadata": {},
   "source": [
    "## 7. Practical Tips & Best Practices üí°\n",
    "\n",
    "### Data Preparation Checklist:\n",
    "\n",
    "‚úÖ **Data Format**\n",
    "- Choose appropriate format (prompt-completion, instruction, chat)\n",
    "- Ensure consistent formatting across dataset\n",
    "- Include proper special tokens\n",
    "\n",
    "‚úÖ **Tokenization**\n",
    "- Use subword tokenization (BPE, WordPiece) in production\n",
    "- Handle special tokens correctly\n",
    "- Set appropriate `max_length`\n",
    "\n",
    "‚úÖ **Quality Control**\n",
    "- Filter too short/long examples\n",
    "- Check prompt/completion ratios\n",
    "- Remove duplicates\n",
    "- Validate data integrity\n",
    "\n",
    "‚úÖ **Batching**\n",
    "- Use efficient padding (pad to batch max, not global max)\n",
    "- Consider `pad_to_multiple_of` for TPU\n",
    "- Set appropriate batch size for GPU memory\n",
    "\n",
    "‚úÖ **Labels**\n",
    "- Use `-100` for tokens to ignore in loss\n",
    "- Only compute loss on completions (not prompts)\n",
    "- Handle padding in labels\n",
    "\n",
    "### Recommended Hyperparameters:\n",
    "\n",
    "```python\n",
    "# For 7B model on A100 (40GB)\n",
    "max_length = 2048\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "effective_batch_size = 16  # batch_size * grad_accum\n",
    "\n",
    "# For 13B model on A100 (80GB)\n",
    "max_length = 2048\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "effective_batch_size = 16\n",
    "```\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "1. **Use Hugging Face Datasets**: Better memory efficiency, caching\n",
    "2. **Streaming for large datasets**: Don't load everything into RAM\n",
    "3. **Data mixing**: Combine multiple datasets with sampling ratios\n",
    "4. **Packing**: Pack short sequences together to reduce padding waste\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4be58e",
   "metadata": {},
   "source": [
    "## 8. Summary üìù\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ Different data formats for fine-tuning  \n",
    "‚úÖ Tokenization strategies and special tokens  \n",
    "‚úÖ Dataset classes for different formats  \n",
    "‚úÖ Efficient batching with padding  \n",
    "‚úÖ Data quality control and filtering  \n",
    "‚úÖ Data analysis and visualization  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Format matters**: Choose the right format for your task\n",
    "2. **Quality over quantity**: Filter low-quality examples\n",
    "3. **Efficient batching**: Minimize padding waste\n",
    "4. **Label correctly**: Use `-100` for ignored tokens\n",
    "5. **Monitor statistics**: Understand your data distribution\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Tutorial 4**: Instruction tuning with LoRA\n",
    "- **Tutorial 5**: Evaluation metrics and model assessment\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "**Hugging Face:**\n",
    "- Datasets: https://huggingface.co/docs/datasets\n",
    "- Tokenizers: https://huggingface.co/docs/tokenizers\n",
    "\n",
    "**Datasets:**\n",
    "- Alpaca: https://github.com/tatsu-lab/stanford_alpaca\n",
    "- OpenAssistant: https://huggingface.co/datasets/OpenAssistant/oasst1\n",
    "- Dolly: https://huggingface.co/datasets/databricks/databricks-dolly-15k\n",
    "\n",
    "**Related:**\n",
    "- [transformer-foundation/01_embeddings_and_positional_encoding.ipynb](../transformer-foundation/01_embeddings_and_positional_encoding.ipynb)\n",
    "- [01_introduction_to_fine_tuning.ipynb](01_introduction_to_fine_tuning.ipynb)\n",
    "- [02_lora_implementation.ipynb](02_lora_implementation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to start training? Continue to Tutorial 4! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
