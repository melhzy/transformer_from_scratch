{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Tutorial 6: QLoRA - 4-bit Quantization for Efficient Fine-Tuning\n\n## Introduction\n\nWelcome to Tutorial 6 on QLoRA (Quantized LoRA)! This tutorial explores how 4-bit quantization enables training large language models (7B+ parameters) on consumer GPUs.\n\n### What is QLoRA?\n\n**QLoRA** combines:\n1. **4-bit Quantization**: Compress model weights from 32-bit to 4-bit\n2. **LoRA Adapters**: Train small adapter layers in FP16 precision\n\n### Memory Comparison (7B Model)\n\n| Method | Memory | Trainable Params | Performance |\n|--------|--------|------------------|-------------|\n| Full Fine-Tuning | 28 GB | 7B (100%) | 100% |\n| LoRA | 14 GB | 16M (0.2%) | 99% |\n| QLoRA | 5 GB | 16M (0.2%) | 99% |"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Google Colab Setup\nimport sys\nimport os\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print('Running in Google Colab')\n    if not os.path.exists('transformer_from_scratch'):\n        !git clone https://github.com/melhzy/transformer_from_scratch.git\n    os.chdir('transformer_from_scratch')\n    !pip install -q torch matplotlib seaborn numpy pandas\n    sys.path.insert(0, '/content/transformer_from_scratch')\nelse:\n    print('Running locally')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Import Required Libraries"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nprint(f'PyTorch version: {torch.__version__}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Understanding Quantization\n\nQuantization reduces precision to save memory:\n\n- **FP32**: 32 bits (4 bytes)\n- **FP16**: 16 bits (2 bytes) - 50% savings\n- **INT8**: 8 bits (1 byte) - 75% savings  \n- **4-bit**: 4 bits (0.5 bytes) - 87.5% savings!\n\n### Why 4-bit for QLoRA?\n\n- 7B model: 28GB → 3.5GB (8x reduction)\n- Maintains 99%+ performance\n- Only LoRA adapters train in FP16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Simple 4-bit Quantizer"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class Simple4BitQuantizer:\n    def __init__(self):\n        self.n_levels = 16  # 2^4\n    \n    def quantize(self, tensor):\n        min_val = tensor.min()\n        max_val = tensor.max()\n        scale = (max_val - min_val) / (self.n_levels - 1)\n        if scale == 0:\n            scale = 1.0\n        quantized = torch.round((tensor - min_val) / scale)\n        quantized = torch.clamp(quantized, 0, self.n_levels - 1).to(torch.uint8)\n        return quantized, scale, min_val\n    \n    def dequantize(self, quantized, scale, min_val):\n        return (quantized.float() * scale) + min_val\n\n# Demo\nweights = torch.randn(4, 4) * 0.5\nprint('Original:', weights.element_size() * weights.nelement(), 'bytes')\n\nquantizer = Simple4BitQuantizer()\nquant, scale, min_val = quantizer.quantize(weights)\nprint('Quantized:', quant.element_size() * quant.nelement(), 'bytes')\nprint('Savings:', (1 - quant.element_size() / weights.element_size()) * 100, '%')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. QLoRA Layer Implementation\n\nCombines quantized base weights (4-bit) with trainable LoRA adapters (FP16)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class QLoRALayer(nn.Module):\n    def __init__(self, in_features, out_features, rank=8):\n        super().__init__()\n        self.rank = rank\n        \n        # Quantize base weights\n        base_weight = torch.randn(out_features, in_features) * 0.02\n        quantizer = Simple4BitQuantizer()\n        self.quantized_weight, self.scale, self.min_val = quantizer.quantize(base_weight)\n        \n        # LoRA adapters (trainable)\n        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n        self.scaling = 1.0 / rank\n    \n    def forward(self, x):\n        # Dequantize base\n        quantizer = Simple4BitQuantizer()\n        base_weight = quantizer.dequantize(self.quantized_weight, self.scale, self.min_val)\n        base_output = F.linear(x, base_weight)\n        \n        # Add LoRA\n        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n        return base_output + lora_output\n\n# Demo\nlayer = QLoRALayer(512, 512, rank=8)\nx = torch.randn(2, 10, 512)\noutput = layer(x)\nprint(f'Input: {x.shape}')\nprint(f'Output: {output.shape}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Memory Comparison"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\n\ndef calc_memory(params_billions, method):\n    params = params_billions * 1e9\n    if method == 'full':\n        model = (params * 4) / 1e9\n        return {'Model': model, 'Gradients': model, 'Optimizer': model * 2, 'Total': model * 4}\n    elif method == 'lora':\n        base = (params * 2) / 1e9\n        lora = (params * 0.01 * 2) / 1e9\n        return {'Base': base, 'LoRA': lora * 3, 'Total': base + lora * 3}\n    else:  # qlora\n        base = (params * 0.5) / 1e9\n        lora = (params * 0.01 * 2) / 1e9\n        return {'Base': base, 'LoRA': lora * 3, 'Total': base + lora * 3}\n\nresults = [{'Method': m.upper(), **calc_memory(7, m)} for m in ['full', 'lora', 'qlora']]\ndf = pd.DataFrame(results)\nprint(df.to_string(index=False))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Production QLoRA with Unsloth\n\nFor production use, Unsloth provides optimized QLoRA:\n\n```python\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name='unsloth/Llama-3.2-1B-Instruct-bnb-4bit',\n    load_in_4bit=True\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n)\n```\n\n**Benefits:**\n- 2x faster training\n- 30% less memory\n- Pre-configured models"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Summary\n\n### Key Takeaways\n\n| Method | Memory (7B) | GPU Required |\n|--------|-------------|-------------|\n| Full | 28 GB | A100 40GB |\n| LoRA | 14 GB | RTX 3090 24GB |\n| QLoRA | 5 GB | RTX 3060 12GB |\n\n### When to Use QLoRA\n\n✅ Limited GPU memory (< 24GB)\n✅ Training large models (7B+)\n✅ Consumer GPUs\n\n### Resources\n\n- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n- [Tutorial 1: Introduction](01_introduction_to_fine_tuning.ipynb)\n- [Tutorial 2: LoRA Implementation](02_lora_implementation.ipynb)\n\n**Congratulations!** You can now train large models on consumer GPUs!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}