{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ded174a",
   "metadata": {},
   "source": [
    "# Tutorial 6: QLoRA - 4-bit Quantization for Efficient Fine-Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to **Tutorial 6** on **QLoRA (Quantized LoRA)**! This tutorial explores how 4-bit quantization enables training large language models (7B+ parameters) on consumer GPUs with limited VRAM.\n",
    "\n",
    "### What is QLoRA?\n",
    "\n",
    "**QLoRA** combines two powerful techniques:\n",
    "1. **4-bit Quantization**: Compress model weights from 32-bit floats to 4-bit integers\n",
    "2. **LoRA Adapters**: Train small adapter layers in FP16 precision\n",
    "\n",
    "This allows you to:\n",
    "- Train 7B models on GPUs with 6GB VRAM (vs 28GB for full fine-tuning)\n",
    "- Maintain 99% of full precision performance\n",
    "- Enable training on consumer hardware (RTX 3060, 3090, 4090)\n",
    "\n",
    "### Memory Comparison (7B Model)\n",
    "\n",
    "| Method | Memory Required | Trainable Params | Performance |\n",
    "|--------|----------------|------------------|-------------|\n",
    "| **Full Fine-Tuning** | ~28 GB | 7B (100%) | 100% |\n",
    "| **LoRA** | ~14 GB | ~16M (0.2%) | 99% |\n",
    "| **QLoRA** | ~5 GB | ~16M (0.2%) | 99% |\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. **Quantization Theory**: How 4-bit quantization works (NF4, FP4)\n",
    "2. **QLoRA Architecture**: Combining quantized base + FP16 adapters\n",
    "3. **Memory Analysis**: Why QLoRA saves so much memory\n",
    "4. **Implementation**: Build a simple quantizer from scratch\n",
    "5. **Production Use**: Using Unsloth for real QLoRA training\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Tutorial 1 (LoRA theory)\n",
    "- Tutorial 2 (LoRA implementation)\n",
    "- Basic understanding of floating-point representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"Installing dependencies...\")\n",
    "    !pip install -q torch matplotlib seaborn numpy pandas\n",
    "    print(\"Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"Setup complete! Ready to run the tutorial.\")\n",
    "else:\n",
    "    print(\"Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbbba3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for implementing QLoRA from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9171b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2561",
   "metadata": {},
   "source": [
    "## 2. Understanding Quantization Basics\n",
    "\n",
    "### What is Quantization?\n",
    "\n",
    "Quantization reduces the precision of numbers to save memory:\n",
    "\n",
    "- **FP32 (Full Precision)**: 32 bits per parameter (4 bytes)\n",
    "- **FP16 (Half Precision)**: 16 bits per parameter (2 bytes) - 50% memory savings\n",
    "- **INT8**: 8 bits per parameter (1 byte) - 75% memory savings\n",
    "- **4-bit**: 4 bits per parameter (0.5 bytes) - 87.5% memory savings!\n",
    "\n",
    "### How Does it Work?\n",
    "\n",
    "Quantization maps floating-point values to a smaller set of discrete values:\n",
    "\n",
    "```\n",
    "Original (FP32): [0.1, 0.5, 0.9, 1.2, 1.8]\n",
    "Quantized (4-bit): [0, 5, 9, 12, 15]  (16 possible values: 0-15)\n",
    "```\n",
    "\n",
    "The formula:\n",
    "```\n",
    "quantized = round((value - zero_point) / scale)\n",
    "dequantized = (quantized * scale) + zero_point\n",
    "```\n",
    "\n",
    "### Why 4-bit for QLoRA?\n",
    "\n",
    "- **Memory**: 7B model goes from 28GB to 3.5GB (8x reduction)\n",
    "- **Accuracy**: NF4 format preserves 99%+ of model performance\n",
    "- **Training**: Only LoRA adapters train in FP16, base stays quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64d897",
   "metadata": {},
   "source": [
    "## 3. Implementing 4-bit Quantization\n",
    "\n",
    "Let's build a simple 4-bit quantizer from scratch to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182962c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple4BitQuantizer:\n",
    "    \"\"\"\n",
    "    Simple 4-bit symmetric quantization.\n",
    "    Maps FP32 values to 16 discrete levels (0-15).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_levels = 16  # 2^4 = 16 possible values\n",
    "    \n",
    "    def quantize(self, tensor):\n",
    "        \"\"\"Quantize FP32 tensor to 4-bit representation.\"\"\"\n",
    "        # Find min and max for scale calculation\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        \n",
    "        # Calculate scale (range divided by number of levels)\n",
    "        scale = (max_val - min_val) / (self.n_levels - 1)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if scale == 0:\n",
    "            scale = 1.0\n",
    "        \n",
    "        # Quantize: map to 0-15 range\n",
    "        quantized = torch.round((tensor - min_val) / scale)\n",
    "        quantized = torch.clamp(quantized, 0, self.n_levels - 1).to(torch.uint8)\n",
    "        \n",
    "        return quantized, scale, min_val\n",
    "    \n",
    "    def dequantize(self, quantized, scale, min_val):\n",
    "        \"\"\"Dequantize back to FP32 for computation.\"\"\"\n",
    "        return (quantized.float() * scale) + min_val\n",
    "\n",
    "\n",
    "# Example: Quantize a weight matrix\n",
    "print(\"=== Simple 4-bit Quantization Demo ===\\n\")\n",
    "\n",
    "# Create a sample weight matrix (similar to transformer layer weights)\n",
    "original_weights = torch.randn(4, 4) * 0.5\n",
    "\n",
    "print(\"Original weights (FP32):\")\n",
    "print(original_weights)\n",
    "print(f\"Memory: {original_weights.element_size() * original_weights.nelement()} bytes\\n\")\n",
    "\n",
    "# Quantize\n",
    "quantizer = Simple4BitQuantizer()\n",
    "quant_weights, scale, min_val = quantizer.quantize(original_weights)\n",
    "\n",
    "print(\"Quantized weights (4-bit):\")\n",
    "print(quant_weights)\n",
    "print(f\"Memory: {quant_weights.element_size() * quant_weights.nelement()} bytes\")\n",
    "print(f\"Memory savings: {(1 - quant_weights.element_size() / original_weights.element_size()) * 100:.1f}%\\n\")\n",
    "\n",
    "# Dequantize\n",
    "dequant_weights = quantizer.dequantize(quant_weights, scale, min_val)\n",
    "\n",
    "print(\"Dequantized weights (back to FP32):\")\n",
    "print(dequant_weights)\n",
    "\n",
    "# Calculate quantization error\n",
    "error = (original_weights - dequant_weights).abs().mean()\n",
    "print(f\"\\nQuantization error (MAE): {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500afd46",
   "metadata": {},
   "source": [
    "## 4. QLoRA Architecture: Combining Quantized Base + LoRA Adapters\n",
    "\n",
    "Now let's implement the core idea of QLoRA: keep the base model weights in 4-bit, but add LoRA adapters in FP16.\n",
    "\n",
    "**Key Insight**: During forward pass:\n",
    "1. Dequantize 4-bit weights to FP32 (on-the-fly)\n",
    "2. Compute base output: `base_output = input @ dequantized_weights`\n",
    "3. Compute LoRA output: `lora_output = input @ A @ B` (in FP16)\n",
    "4. Combine: `final_output = base_output + lora_output`\n",
    "\n",
    "Only LoRA parameters (A and B) get gradients during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    QLoRA Layer: Quantized base weights + FP16 LoRA adapters\n",
    "    \n",
    "    Memory breakdown:\n",
    "    - Base weights: 4-bit (frozen, quantized)\n",
    "    - LoRA A, B: FP16 (trainable)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=8):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        \n",
    "        # Base weights (will be quantized and frozen)\n",
    "        base_weight = torch.randn(out_features, in_features) * 0.02\n",
    "        \n",
    "        # Quantize base weights\n",
    "        quantizer = Simple4BitQuantizer()\n",
    "        self.quantized_weight, self.scale, self.min_val = quantizer.quantize(base_weight)\n",
    "        \n",
    "        # LoRA adapters (trainable, FP16)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Scaling factor for LoRA\n",
    "        self.scaling = 1.0 / rank\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: dequantize base + add LoRA\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, in_features)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, out_features)\n",
    "        \"\"\"\n",
    "        # Dequantize base weights on-the-fly\n",
    "        quantizer = Simple4BitQuantizer()\n",
    "        base_weight = quantizer.dequantize(self.quantized_weight, self.scale, self.min_val)\n",
    "        \n",
    "        # Base output (frozen weights)\n",
    "        base_output = F.linear(x, base_weight)\n",
    "        \n",
    "        # LoRA output (trainable adapters)\n",
    "        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "        \n",
    "        # Combine both\n",
    "        return base_output + lora_output\n",
    "    \n",
    "    def memory_usage(self):\n",
    "        \"\"\"Calculate memory usage in MB.\"\"\"\n",
    "        # Quantized base: 4 bits = 0.5 bytes per parameter\n",
    "        base_mem = (self.quantized_weight.nelement() * 0.5) / 1e6\n",
    "        \n",
    "        # LoRA adapters: FP16 = 2 bytes per parameter\n",
    "        lora_mem = ((self.lora_A.nelement() + self.lora_B.nelement()) * 2) / 1e6\n",
    "        \n",
    "        return {\n",
    "            'base_4bit_MB': base_mem,\n",
    "            'lora_fp16_MB': lora_mem,\n",
    "            'total_MB': base_mem + lora_mem\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Create a QLoRA layer\n",
    "print(\"=== QLoRA Layer Demo ===\\n\")\n",
    "\n",
    "d_model = 512\n",
    "qlora_layer = QLoRALayer(d_model, d_model, rank=8)\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 10, d_model)  # (batch=2, seq_len=10, d_model=512)\n",
    "output = qlora_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\\n\")\n",
    "\n",
    "# Memory analysis\n",
    "mem = qlora_layer.memory_usage()\n",
    "print(\"Memory Usage:\")\n",
    "print(f\"  Base weights (4-bit): {mem['base_4bit_MB']:.2f} MB\")\n",
    "print(f\"  LoRA adapters (FP16): {mem['lora_fp16_MB']:.2f} MB\")\n",
    "print(f\"  Total: {mem['total_MB']:.2f} MB\\n\")\n",
    "\n",
    "# Compare to full precision\n",
    "full_precision_mem = (d_model * d_model * 4) / 1e6  # FP32 = 4 bytes\n",
    "print(f\"Full FP32 layer would be: {full_precision_mem:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - mem['total_MB'] / full_precision_mem) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60834f7d",
   "metadata": {},
   "source": [
    "## 5. Memory Comparison: Full vs LoRA vs QLoRA\n",
    "\n",
    "Let's calculate real memory requirements for a 7B parameter model using different fine-tuning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_memory(num_params_billions, method='full', lora_rank=16, lora_percentage=0.01):\n",
    "    \"\"\"\n",
    "    Calculate memory requirements for different fine-tuning methods.\n",
    "    \n",
    "    Args:\n",
    "        num_params_billions: Model size in billions (e.g., 7 for 7B model)\n",
    "        method: 'full', 'lora', or 'qlora'\n",
    "        lora_rank: Rank for LoRA adapters\n",
    "        lora_percentage: Percentage of layers with LoRA (default 1% of model)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory breakdown in GB\n",
    "    \"\"\"\n",
    "    num_params = num_params_billions * 1e9\n",
    "    \n",
    "    if method == 'full':\n",
    "        # Full fine-tuning: all params in FP32 + gradients + optimizer states\n",
    "        model_mem = (num_params * 4) / 1e9  # FP32 = 4 bytes\n",
    "        gradients_mem = model_mem  # Same size as model\n",
    "        optimizer_mem = model_mem * 2  # Adam: 2x model size (momentum + variance)\n",
    "        total = model_mem + gradients_mem + optimizer_mem\n",
    "        \n",
    "        return {\n",
    "            'Model': model_mem,\n",
    "            'Gradients': gradients_mem,\n",
    "            'Optimizer': optimizer_mem,\n",
    "            'Total': total\n",
    "        }\n",
    "    \n",
    "    elif method == 'lora':\n",
    "        # LoRA: frozen base in FP16 + trainable adapters in FP16\n",
    "        base_mem = (num_params * 2) / 1e9  # FP16 = 2 bytes\n",
    "        lora_params = num_params * lora_percentage  # ~1% of model\n",
    "        lora_mem = (lora_params * 2) / 1e9  # FP16\n",
    "        lora_gradients = lora_mem\n",
    "        lora_optimizer = lora_mem * 2\n",
    "        total = base_mem + lora_mem + lora_gradients + lora_optimizer\n",
    "        \n",
    "        return {\n",
    "            'Base (FP16)': base_mem,\n",
    "            'LoRA params': lora_mem,\n",
    "            'LoRA gradients': lora_gradients,\n",
    "            'LoRA optimizer': lora_optimizer,\n",
    "            'Total': total\n",
    "        }\n",
    "    \n",
    "    elif method == 'qlora':\n",
    "        # QLoRA: frozen base in 4-bit + trainable adapters in FP16\n",
    "        base_mem = (num_params * 0.5) / 1e9  # 4-bit = 0.5 bytes\n",
    "        lora_params = num_params * lora_percentage\n",
    "        lora_mem = (lora_params * 2) / 1e9  # FP16\n",
    "        lora_gradients = lora_mem\n",
    "        lora_optimizer = lora_mem * 2\n",
    "        total = base_mem + lora_mem + lora_gradients + lora_optimizer\n",
    "        \n",
    "        return {\n",
    "            'Base (4-bit)': base_mem,\n",
    "            'LoRA params': lora_mem,\n",
    "            'LoRA gradients': lora_gradients,\n",
    "            'LoRA optimizer': lora_optimizer,\n",
    "            'Total': total\n",
    "        }\n",
    "\n",
    "\n",
    "# Compare memory for 7B model\n",
    "print(\"=== Memory Comparison for 7B Model ===\\n\")\n",
    "\n",
    "methods = ['full', 'lora', 'qlora']\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    mem = calculate_memory(7, method=method)\n",
    "    results.append({'Method': method.upper(), **mem})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "full_mem = df[df['Method'] == 'FULL']['Total'].values[0]\n",
    "lora_mem = df[df['Method'] == 'LORA']['Total'].values[0]\n",
    "qlora_mem = df[df['Method'] == 'QLORA']['Total'].values[0]\n",
    "\n",
    "print(f\"LoRA saves: {(1 - lora_mem / full_mem) * 100:.1f}% vs Full Fine-Tuning\")\n",
    "print(f\"QLoRA saves: {(1 - qlora_mem / full_mem) * 100:.1f}% vs Full Fine-Tuning\")\n",
    "print(f\"QLoRA saves: {(1 - qlora_mem / lora_mem) * 100:.1f}% vs LoRA\")\n",
    "\n",
    "print(\"\\n=== GPU Requirements (7B Model) ===\")\n",
    "print(f\"Full Fine-Tuning: {full_mem:.1f} GB - Requires A100 40GB or 2x RTX 3090\")\n",
    "print(f\"LoRA: {lora_mem:.1f} GB - Fits on single RTX 3090 (24GB)\")\n",
    "print(f\"QLoRA: {qlora_mem:.1f} GB - Fits on RTX 3060 (12GB) or even RTX 4060 Ti (8GB)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d54f10",
   "metadata": {},
   "source": [
    "## 6. Production QLoRA with Unsloth\n",
    "\n",
    "Now that you understand how QLoRA works internally, let's see how to use it in production with **Unsloth** - a library optimized for efficient fine-tuning.\n",
    "\n",
    "### Why Unsloth?\n",
    "\n",
    "- **2x faster** than standard implementations\n",
    "- **30% less memory** through optimized kernels\n",
    "- **Pre-configured** 4-bit quantized models\n",
    "- **Easy to use** - just a few lines of code!\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install unsloth\n",
    "```\n",
    "\n",
    "### Example: Fine-tune Llama 3.2 1B with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production QLoRA Example (pseudo-code - requires unsloth installation)\n",
    "\n",
    "print(\"=== Production QLoRA with Unsloth ===\\n\")\n",
    "print(\"This is example code showing how to use Unsloth for QLoRA fine-tuning.\")\n",
    "print(\"To run this, install Unsloth: pip install unsloth\\n\")\n",
    "\n",
    "example_code = '''\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,  # Auto-detect\n",
    "    load_in_4bit = True,  # Enable 4-bit quantization\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True,\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:1000]\")\n",
    "\n",
    "def format_prompts(examples):\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(\n",
    "        examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
    "    ):\n",
    "        text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"lora_model\")\n",
    "\n",
    "# Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\"What is machine learning?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "'''\n",
    "\n",
    "print(example_code)\n",
    "\n",
    "print(\"\\n=== Key Configuration Parameters ===\")\n",
    "print(\"- load_in_4bit=True: Loads model in 4-bit quantization\")\n",
    "print(\"- r=16: LoRA rank (higher = more capacity, more memory)\")\n",
    "print(\"- lora_alpha=16: LoRA scaling factor\")\n",
    "print(\"- target_modules: Which layers get LoRA adapters\")\n",
    "print(\"- use_gradient_checkpointing: Save memory during backprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14538568",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Quantization Fundamentals**\n",
    "   - How to reduce precision from FP32 → 4-bit\n",
    "   - Trade-offs between memory and accuracy\n",
    "   - Simple quantization implementation\n",
    "\n",
    "2. **QLoRA Architecture**\n",
    "   - Combining 4-bit base weights with FP16 LoRA adapters\n",
    "   - Memory savings: 5-6x compared to LoRA, 8x compared to full fine-tuning\n",
    "   - Only LoRA adapters train, base stays frozen and quantized\n",
    "\n",
    "3. **Practical Benefits**\n",
    "   - Train 7B models on consumer GPUs (6-12GB VRAM)\n",
    "   - Maintain 99% of full precision performance\n",
    "   - Enable AI research on affordable hardware\n",
    "\n",
    "4. **Production Usage**\n",
    "   - Unsloth library for optimized QLoRA\n",
    "   - 2x speed improvements\n",
    "   - Pre-configured 4-bit quantized models\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Aspect | Full Fine-Tuning | LoRA | QLoRA |\n",
    "|--------|------------------|------|-------|\n",
    "| **Memory (7B)** | 28 GB | 14 GB | 5 GB |\n",
    "| **Trainable Params** | 7B (100%) | 16M (0.2%) | 16M (0.2%) |\n",
    "| **Performance** | 100% | 99% | 99% |\n",
    "| **GPU Required** | A100 40GB | RTX 3090 24GB | RTX 3060 12GB |\n",
    "| **Training Speed** | 1x | 1.2x | 1x |\n",
    "\n",
    "### When to Use QLoRA?\n",
    "\n",
    "✅ **Use QLoRA when:**\n",
    "- Limited GPU memory (< 24GB)\n",
    "- Training large models (7B+)\n",
    "- Budget constraints (consumer GPUs)\n",
    "- Quick experimentation\n",
    "\n",
    "❌ **Consider alternatives when:**\n",
    "- Memory is not a constraint (A100 available)\n",
    "- Training small models (< 1B parameters)\n",
    "- Need absolute maximum performance\n",
    "- Have access to multiple GPUs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Try fine-tuning with Unsloth on your own dataset\n",
    "2. **Experiment**: Test different ranks (r=8, 16, 32, 64)\n",
    "3. **Optimize**: Use gradient checkpointing for even lower memory\n",
    "4. **Deploy**: Merge LoRA weights and quantize for inference\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **QLoRA Paper**: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "- **Unsloth**: [GitHub Repository](https://github.com/unslothai/unsloth)\n",
    "- **Unsloth Notebooks**: [100+ Examples](https://github.com/unslothai/notebooks)\n",
    "- **bitsandbytes**: [Quantization Library](https://github.com/TimDettmers/bitsandbytes)\n",
    "\n",
    "**Related Tutorials:**\n",
    "- [Tutorial 1: Introduction to Fine-Tuning](01_introduction_to_fine_tuning.ipynb)\n",
    "- [Tutorial 2: LoRA Implementation](02_lora_implementation.ipynb)\n",
    "- [Tutorial 4: Instruction Tuning](04_instruction_tuning.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand QLoRA and can train large models on consumer GPUs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
