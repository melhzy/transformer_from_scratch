{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a556c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup (run this cell only if you're in Colab)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy pandas transformers datasets peft\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete! Ready to run the tutorial.\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "if not IN_COLAB:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our transformer implementation\n",
    "from src.transformer import Transformer\n",
    "from src.modules.embeddings import TokenEmbedding\n",
    "from src.modules.encoder import TransformerEncoder\n",
    "from src.modules.decoder import TransformerDecoder\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5a0fb",
   "metadata": {},
   "source": [
    "## 1. What is Fine-Tuning? üéØ\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Fine-tuning** is the process of taking a pre-trained model and adapting it to a specific task or domain by continuing training on task-specific data.\n",
    "\n",
    "### Why Fine-Tune?\n",
    "\n",
    "1. **Pre-training is expensive**: Training a model from scratch requires massive compute (millions of GPU hours)\n",
    "2. **Transfer learning works**: Models learn general language understanding that transfers to specific tasks\n",
    "3. **Customization**: Adapt models to your specific use case, style, or domain\n",
    "4. **Data efficiency**: Achieve good performance with relatively small datasets\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "```\n",
    "PRE-TRAINING:\n",
    "‚îú‚îÄ‚îÄ Data: Massive unlabeled text (trillions of tokens)\n",
    "‚îú‚îÄ‚îÄ Task: Next-token prediction / Masked language modeling\n",
    "‚îú‚îÄ‚îÄ Time: Weeks to months\n",
    "‚îú‚îÄ‚îÄ Cost: $1M - $100M+\n",
    "‚îî‚îÄ‚îÄ Result: General-purpose language model\n",
    "\n",
    "FINE-TUNING:\n",
    "‚îú‚îÄ‚îÄ Data: Task-specific labeled data (thousands to millions of examples)\n",
    "‚îú‚îÄ‚îÄ Task: Specific (classification, QA, summarization, chat, etc.)\n",
    "‚îú‚îÄ‚îÄ Time: Hours to days\n",
    "‚îú‚îÄ‚îÄ Cost: $10 - $10,000\n",
    "‚îî‚îÄ‚îÄ Result: Specialized model for your task\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee97cf",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning Strategies üõ†Ô∏è\n",
    "\n",
    "### Strategy 1: Full Fine-Tuning\n",
    "\n",
    "**What**: Update all model parameters during training\n",
    "\n",
    "**Pros:**\n",
    "- Maximum adaptation to your task\n",
    "- Best performance potential\n",
    "- Full control over model behavior\n",
    "\n",
    "**Cons:**\n",
    "- High memory requirements (need to store gradients for all parameters)\n",
    "- Risk of catastrophic forgetting\n",
    "- Expensive (GPU memory and time)\n",
    "\n",
    "**When to use:**\n",
    "- Small models (<1B parameters)\n",
    "- Large dataset available\n",
    "- Maximum performance needed\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 2: Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "**What**: Update only a small subset of parameters\n",
    "\n",
    "**Techniques:**\n",
    "1. **Adapter Layers**: Insert small trainable modules between frozen layers\n",
    "2. **Prefix Tuning**: Add trainable prefix tokens to input\n",
    "3. **Prompt Tuning**: Learn soft prompts (continuous embeddings)\n",
    "4. **LoRA** (Low-Rank Adaptation): Most popular - explained below\n",
    "\n",
    "**Pros:**\n",
    "- Much lower memory requirements\n",
    "- Faster training\n",
    "- Can maintain multiple task-specific adapters\n",
    "- Less catastrophic forgetting\n",
    "\n",
    "**Cons:**\n",
    "- Slightly lower performance than full fine-tuning\n",
    "- More complex implementation\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 3: LoRA (Low-Rank Adaptation) ‚≠ê\n",
    "\n",
    "**Key Insight**: Weight updates during fine-tuning have low intrinsic rank\n",
    "\n",
    "Instead of updating full weight matrix $W \\in \\mathbb{R}^{d \\times k}$:\n",
    "\n",
    "$$W' = W + \\Delta W$$\n",
    "\n",
    "Decompose update into low-rank matrices:\n",
    "\n",
    "$$W' = W + BA$$\n",
    "\n",
    "Where:\n",
    "- $W$ is frozen (pre-trained weights)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d, k)$ (rank is much smaller, typically r=8, 16, 32)\n",
    "\n",
    "**Parameters to train**: $d \\times r + r \\times k$ instead of $d \\times k$\n",
    "\n",
    "**Example**: For d=4096, k=4096, r=16:\n",
    "- Full: 16,777,216 parameters\n",
    "- LoRA: 131,072 parameters (0.78% of full!)\n",
    "\n",
    "**Pros:**\n",
    "- Extremely memory efficient\n",
    "- No additional inference latency (can merge weights)\n",
    "- Easy to switch between tasks\n",
    "- Surprisingly good performance\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 4: QLoRA (Quantized LoRA) üöÄ\n",
    "\n",
    "**What**: LoRA + 4-bit quantization of base model\n",
    "\n",
    "**Key Innovation** (by Tim Dettmers et al.):\n",
    "1. Quantize base model to 4-bit (NF4 - Normal Float 4)\n",
    "2. Use double quantization for quantization constants\n",
    "3. Paged optimizers to handle memory spikes\n",
    "4. Train LoRA adapters in 16-bit\n",
    "\n",
    "**Memory Savings**:\n",
    "- 16-bit model: 30GB for 7B model\n",
    "- 4-bit + LoRA: 5-6GB for 7B model!\n",
    "\n",
    "**When to use:**\n",
    "- Limited GPU memory (single consumer GPU)\n",
    "- Large models (7B, 13B, 70B parameters)\n",
    "- Quick iteration and experimentation\n",
    "\n",
    "**This is what Unsloth AI optimizes!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ba71d",
   "metadata": {},
   "source": [
    "## 3. Comparison Table üìä\n",
    "\n",
    "| Strategy | Trainable Params | Memory | Training Time | Performance | Use Case |\n",
    "|----------|------------------|---------|---------------|-------------|----------|\n",
    "| **Full Fine-Tuning** | 100% | Very High | Slow | Best | Small models, unlimited resources |\n",
    "| **PEFT (Adapters)** | 1-5% | Medium | Medium | Good | Multiple tasks, moderate resources |\n",
    "| **LoRA** | 0.1-1% | Low | Fast | Very Good | Most common choice, practical |\n",
    "| **QLoRA** | 0.1-1% | Very Low | Fast | Very Good | Large models, limited GPU |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the parameter efficiency\n",
    "import pandas as pd\n",
    "\n",
    "# Example: 7B parameter model\n",
    "total_params = 7_000_000_000\n",
    "\n",
    "strategies = {\n",
    "    'Full Fine-Tuning': 1.0,\n",
    "    'Adapter Layers': 0.03,\n",
    "    'LoRA (r=8)': 0.005,\n",
    "    'LoRA (r=16)': 0.01,\n",
    "    'LoRA (r=32)': 0.02,\n",
    "}\n",
    "\n",
    "data = []\n",
    "for strategy, ratio in strategies.items():\n",
    "    trainable = int(total_params * ratio)\n",
    "    frozen = total_params - trainable\n",
    "    data.append({\n",
    "        'Strategy': strategy,\n",
    "        'Trainable (M)': trainable / 1_000_000,\n",
    "        'Frozen (M)': frozen / 1_000_000,\n",
    "        'Percentage': f\"{ratio * 100:.2f}%\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nüìä Parameter Efficiency Comparison (7B Model)\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "strategies_list = [d['Strategy'] for d in data]\n",
    "trainable = [d['Trainable (M)'] for d in data]\n",
    "frozen = [d['Frozen (M)'] for d in data]\n",
    "\n",
    "x = np.arange(len(strategies_list))\n",
    "width = 0.6\n",
    "\n",
    "ax.bar(x, frozen, width, label='Frozen Parameters', color='lightblue', alpha=0.7)\n",
    "ax.bar(x, trainable, width, label='Trainable Parameters', color='orange', bottom=frozen)\n",
    "\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_title('Parameter Efficiency: Fine-Tuning Strategies (7B Model)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(strategies_list, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: LoRA trains <1% of parameters but maintains performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e911f",
   "metadata": {},
   "source": [
    "## 4. Understanding LoRA Implementation üîß\n",
    "\n",
    "Let's implement a simple LoRA layer to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-Rank Adaptation (LoRA) Layer\n",
    "    \n",
    "    Adds trainable low-rank matrices to frozen pre-trained weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Frozen pre-trained weight (simulated)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n",
    "        \n",
    "        # LoRA low-rank matrices (trainable)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features))  # (r, in)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))  # (out, r)\n",
    "        \n",
    "        # Initialize A with Kaiming uniform, B with zeros\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original frozen path\n",
    "        result = F.linear(x, self.weight)\n",
    "        \n",
    "        # LoRA path: x @ A^T @ B^T * scaling\n",
    "        lora_result = F.linear(F.linear(x, self.lora_A), self.lora_B)\n",
    "        lora_result = lora_result * self.scaling\n",
    "        \n",
    "        return result + lora_result\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into base weights for inference (optional)\"\"\"\n",
    "        merged = self.weight + (self.lora_B @ self.lora_A) * self.scaling\n",
    "        return merged\n",
    "\n",
    "\n",
    "# Test LoRA layer\n",
    "d_model = 512\n",
    "lora_rank = 8\n",
    "\n",
    "lora_layer = LoRALayer(d_model, d_model, rank=lora_rank)\n",
    "\n",
    "# Count parameters\n",
    "frozen_params = sum(p.numel() for p in lora_layer.parameters() if not p.requires_grad)\n",
    "trainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "total_params = frozen_params + trainable_params\n",
    "\n",
    "print(\"\\nüìä LoRA Layer Analysis:\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"Trainable parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable ratio: {trainable_params / total_params * 100:.2f}%\")\n",
    "print(f\"\\nMemory savings: {frozen_params / trainable_params:.1f}x fewer trainable params!\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = lora_layer(x)\n",
    "print(f\"\\n‚úÖ Input shape: {x.shape}\")\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2827f",
   "metadata": {},
   "source": [
    "## 5. Where to Apply LoRA? üéØ\n",
    "\n",
    "In Transformers, you can apply LoRA to different components:\n",
    "\n",
    "### Common Choices:\n",
    "\n",
    "1. **Query & Value matrices (Q, V)** - Most common, good balance\n",
    "2. **All attention matrices (Q, K, V, O)** - More parameters, better adaptation\n",
    "3. **Attention + FFN** - Maximum adaptation\n",
    "\n",
    "### Reference to transformer-foundation:\n",
    "\n",
    "From `transformer-foundation/03_multi_head_attention.ipynb`:\n",
    "```python\n",
    "class MultiHeadAttention:\n",
    "    self.W_q = nn.Linear(d_model, d_model)  # ‚Üê Apply LoRA here\n",
    "    self.W_k = nn.Linear(d_model, d_model)  # ‚Üê Optional\n",
    "    self.W_v = nn.Linear(d_model, d_model)  # ‚Üê Apply LoRA here\n",
    "    self.W_o = nn.Linear(d_model, d_model)  # ‚Üê Optional\n",
    "```\n",
    "\n",
    "From `transformer-foundation/04_feed_forward_networks.ipynb`:\n",
    "```python\n",
    "class PositionWiseFeedForward:\n",
    "    self.fc1 = nn.Linear(d_model, d_ff)  # ‚Üê Can apply LoRA\n",
    "    self.fc2 = nn.Linear(d_ff, d_model)  # ‚Üê Can apply LoRA\n",
    "```\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "- **Fewer modules**: Faster, less memory, slightly lower performance\n",
    "- **More modules**: Better adaptation, more memory, slower training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter counts for different LoRA configurations\n",
    "d_model = 4096  # Llama-2 7B size\n",
    "d_ff = 11008\n",
    "n_layers = 32\n",
    "n_heads = 32\n",
    "lora_rank = 16\n",
    "\n",
    "configs = {\n",
    "    'Query + Value only': [\n",
    "        ('Q', d_model, d_model),\n",
    "        ('V', d_model, d_model),\n",
    "    ],\n",
    "    'All Attention (Q,K,V,O)': [\n",
    "        ('Q', d_model, d_model),\n",
    "        ('K', d_model, d_model),\n",
    "        ('V', d_model, d_model),\n",
    "        ('O', d_model, d_model),\n",
    "    ],\n",
    "    'Attention + FFN': [\n",
    "        ('Q', d_model, d_model),\n",
    "        ('K', d_model, d_model),\n",
    "        ('V', d_model, d_model),\n",
    "        ('O', d_model, d_model),\n",
    "        ('FFN_up', d_model, d_ff),\n",
    "        ('FFN_down', d_ff, d_model),\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"\\nüìä LoRA Parameter Counts (per layer):\\n\")\n",
    "for config_name, modules in configs.items():\n",
    "    total_lora_params = 0\n",
    "    for name, in_dim, out_dim in modules:\n",
    "        # LoRA params: A (r √ó in) + B (out √ó r)\n",
    "        lora_params = (lora_rank * in_dim) + (out_dim * lora_rank)\n",
    "        total_lora_params += lora_params\n",
    "    \n",
    "    total_for_model = total_lora_params * n_layers\n",
    "    print(f\"{config_name}:\")\n",
    "    print(f\"  Per layer: {total_lora_params:,} params\")\n",
    "    print(f\"  Full model ({n_layers} layers): {total_for_model / 1e6:.2f}M params\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüí° Recommendation: Start with 'Query + Value only' for fastest iteration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42c063",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning Pipeline üîÑ\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "```\n",
    "1. DATA PREPARATION\n",
    "   ‚îú‚îÄ‚îÄ Collect task-specific data\n",
    "   ‚îú‚îÄ‚îÄ Format (instruction, input, output)\n",
    "   ‚îú‚îÄ‚îÄ Tokenize\n",
    "   ‚îî‚îÄ‚îÄ Create DataLoader\n",
    "\n",
    "2. MODEL SETUP\n",
    "   ‚îú‚îÄ‚îÄ Load pre-trained model\n",
    "   ‚îú‚îÄ‚îÄ Add LoRA adapters (or freeze layers)\n",
    "   ‚îú‚îÄ‚îÄ Configure optimizer\n",
    "   ‚îî‚îÄ‚îÄ Set hyperparameters\n",
    "\n",
    "3. TRAINING\n",
    "   ‚îú‚îÄ‚îÄ Forward pass\n",
    "   ‚îú‚îÄ‚îÄ Compute loss\n",
    "   ‚îú‚îÄ‚îÄ Backward pass (only LoRA params)\n",
    "   ‚îú‚îÄ‚îÄ Update weights\n",
    "   ‚îî‚îÄ‚îÄ Monitor metrics\n",
    "\n",
    "4. EVALUATION\n",
    "   ‚îú‚îÄ‚îÄ Test on validation set\n",
    "   ‚îú‚îÄ‚îÄ Compare with base model\n",
    "   ‚îú‚îÄ‚îÄ Check for overfitting\n",
    "   ‚îî‚îÄ‚îÄ Measure task performance\n",
    "\n",
    "5. DEPLOYMENT\n",
    "   ‚îú‚îÄ‚îÄ Merge LoRA weights (optional)\n",
    "   ‚îú‚îÄ‚îÄ Quantize for inference (optional)\n",
    "   ‚îú‚îÄ‚îÄ Test inference speed\n",
    "   ‚îî‚îÄ‚îÄ Deploy to production\n",
    "```\n",
    "\n",
    "**Next tutorials will cover each step in detail!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8224fb8",
   "metadata": {},
   "source": [
    "## 7. Connection to Our Transformer Implementation üîó\n",
    "\n",
    "### From `src/transformer.py`:\n",
    "\n",
    "Our implementation provides the foundation. To add LoRA:\n",
    "\n",
    "```python\n",
    "# Original (from src/transformer.py)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        self.encoder = TransformerEncoder(...)  # From src/modules/encoder.py\n",
    "        self.decoder = TransformerDecoder(...)  # From src/modules/decoder.py\n",
    "\n",
    "# With LoRA (next tutorial)\n",
    "def add_lora_to_transformer(model, rank=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if 'W_q' in name or 'W_v' in name:\n",
    "                # Replace with LoRA layer\n",
    "                pass\n",
    "```\n",
    "\n",
    "### Key Modules to Understand:\n",
    "\n",
    "1. **`src/modules/embeddings.py`** - Usually frozen during fine-tuning\n",
    "2. **`src/modules/attention.py`** - Where LoRA is most effective\n",
    "3. **`src/modules/feed_forward.py`** - Optional LoRA application\n",
    "4. **`src/modules/encoder.py` & `src/modules/decoder.py`** - Container modules\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Production-Ready Alternative: Unsloth AI\n",
    "\n",
    "Now that you understand the theory and manual implementation, here's how professionals do it in production:\n",
    "\n",
    "### Why Unsloth for Production?\n",
    "\n",
    "After learning the fundamentals above, **Unsloth AI** provides:\n",
    "- ‚ö° **2x faster training** with optimized CUDA kernels\n",
    "- üíæ **30% less memory** usage (fit larger models)\n",
    "- üì¶ **Pre-configured setups** for Llama, Gemma, Mistral, Qwen, etc.\n",
    "- üîß **Production-tested** code used by thousands of developers\n",
    "\n",
    "### Quick Example with Unsloth\n",
    "\n",
    "```python\n",
    "# Install Unsloth (in production environment)\n",
    "# pip install unsloth\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model with LoRA in 3 lines\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/tinyllama-bnb-4bit\",  # 4-bit quantized model\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect best dtype\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    ")\n",
    "\n",
    "# Add LoRA adapters automatically\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# That's it! Ready to train with standard HuggingFace Trainer\n",
    "```\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "1. ‚úÖ **This Tutorial** - Understand fundamentals (pre-training, fine-tuning, LoRA theory)\n",
    "2. ‚û°Ô∏è **Tutorials 2-5** - Implement from scratch (LoRA, datasets, training, evaluation)\n",
    "3. üöÄ **Then Use Unsloth** - Production deployment with optimizations\n",
    "\n",
    "**Why this order?** Understanding how LoRA works (next tutorials) helps you debug issues, customize implementations, and make informed decisions when using production tools like Unsloth.\n",
    "\n",
    "---\n",
    "\n",
    "Ready to implement LoRA from scratch? Continue to **Tutorial 2**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51614f5",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps üìù\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ Fine-tuning adapts pre-trained models to specific tasks  \n",
    "‚úÖ Full fine-tuning updates all parameters (expensive)  \n",
    "‚úÖ PEFT updates only subset of parameters (efficient)  \n",
    "‚úÖ LoRA decomposes weight updates into low-rank matrices  \n",
    "‚úÖ QLoRA adds quantization for extreme efficiency  \n",
    "‚úÖ Different strategies trade off performance vs. resources  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **LoRA is the practical choice** for most use cases\n",
    "2. **Start simple**: Query + Value matrices with rank=8 or 16\n",
    "3. **Iterate quickly**: Low memory means faster experiments\n",
    "4. **Know your constraints**: GPU memory, training time, performance needs\n",
    "\n",
    "### Next Tutorials:\n",
    "\n",
    "1. **02_lora_implementation.ipynb** - Implement LoRA from scratch\n",
    "2. **03_data_preparation.ipynb** - Prepare datasets for fine-tuning\n",
    "3. **04_instruction_tuning.ipynb** - Fine-tune for instruction following\n",
    "4. **05_evaluation_metrics.ipynb** - Measure fine-tuning success\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Papers:\n",
    "- **LoRA**: \"LoRA: Low-Rank Adaptation of Large Language Models\" (Hu et al., 2021)\n",
    "- **QLoRA**: \"QLoRA: Efficient Finetuning of Quantized LLMs\" (Dettmers et al., 2023)\n",
    "- **PEFT Survey**: \"Parameter-Efficient Fine-Tuning Methods\" (Lialin et al., 2023)\n",
    "\n",
    "### Libraries:\n",
    "- **Hugging Face PEFT**: https://github.com/huggingface/peft\n",
    "- **Unsloth AI**: https://github.com/unslothai/unsloth (optimized fine-tuning)\n",
    "- **bitsandbytes**: https://github.com/TimDettmers/bitsandbytes (quantization)\n",
    "\n",
    "### Related Tutorials:\n",
    "- [transformer-foundation/](../transformer-foundation/) - Build understanding from scratch\n",
    "- [papers/DeepSeek-R1-paper.pdf](../papers/DeepSeek-R1-paper.pdf) - Modern architecture\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to implement LoRA? Continue to Tutorial 2! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}