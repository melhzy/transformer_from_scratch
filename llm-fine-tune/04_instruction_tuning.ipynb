{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
    "    if not os.path.exists('transformer_from_scratch'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/melhzy/transformer_from_scratch.git\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    os.chdir('transformer_from_scratch')\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -q torch torchvision matplotlib seaborn numpy pandas tqdm tensorboard\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "    if '/content/transformer_from_scratch' not in sys.path:\n",
    "        sys.path.insert(0, '/content/transformer_from_scratch')\n",
    "    print(\"‚úÖ Setup complete!\")\n",
    "else:\n",
    "    print(\"üíª Running locally - no setup needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ce632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "if not IN_COLAB:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our implementations\n",
    "from src.transformer import Transformer\n",
    "from src.modules.embeddings import TokenEmbedding, PositionalEncoding\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d594637",
   "metadata": {},
   "source": [
    "## 1. Setup: Model, Data, and LoRA üõ†Ô∏è\n",
    "\n",
    "We'll use simplified versions of our implementations for faster training on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe85afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use LoRA implementations from Tutorial 2\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"LoRA layer from Tutorial 2\"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features), requires_grad=False)\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = F.linear(x, self.weight, self.bias)\n",
    "        x_lora = self.dropout(x)\n",
    "        lora_result = F.linear(F.linear(x_lora, self.lora_A), self.lora_B)\n",
    "        return result + lora_result * self.scaling\n",
    "\n",
    "\n",
    "# Simple tokenizer from Tutorial 3\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple tokenizer from Tutorial 3\"\"\"\n",
    "    def __init__(self, vocab_size: int = 10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        \n",
    "        self.token2id = {self.pad_token: 0, self.unk_token: 1, self.bos_token: 2, self.eos_token: 3}\n",
    "        self.id2token = {v: k for k, v in self.token2id.items()}\n",
    "        self.next_id = 4\n",
    "        \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        for word, _ in word_freq.most_common(self.vocab_size - 4):\n",
    "            if word not in self.token2id:\n",
    "                self.token2id[word] = self.next_id\n",
    "                self.id2token[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        words = text.lower().split()\n",
    "        ids = [self.token2id.get(w, self.token2id[self.unk_token]) for w in words]\n",
    "        if add_special_tokens:\n",
    "            ids = [self.token2id[self.bos_token]] + ids + [self.token2id[self.eos_token]]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        special_ids = {self.token2id[t] for t in [self.pad_token, self.bos_token, self.eos_token]}\n",
    "        words = []\n",
    "        for id in ids:\n",
    "            if skip_special_tokens and id in special_ids:\n",
    "                continue\n",
    "            words.append(self.id2token.get(id, self.unk_token))\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.token2id[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        return self.token2id[self.eos_token]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper classes loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b4cbe",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data üìö\n",
    "\n",
    "Create a small instruction dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4388756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction dataset (expanded for better training)\n",
    "instruction_data = [\n",
    "    {\"instruction\": \"Translate to French\", \"input\": \"Hello\", \"output\": \"Bonjour\"},\n",
    "    {\"instruction\": \"Translate to French\", \"input\": \"Thank you\", \"output\": \"Merci\"},\n",
    "    {\"instruction\": \"Translate to Spanish\", \"input\": \"Good morning\", \"output\": \"Buenos d√≠as\"},\n",
    "    {\"instruction\": \"Translate to Spanish\", \"input\": \"How are you\", \"output\": \"C√≥mo est√°s\"},\n",
    "    {\"instruction\": \"Answer the question\", \"input\": \"What is 2+2?\", \"output\": \"2+2 equals 4\"},\n",
    "    {\"instruction\": \"Answer the question\", \"input\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris\"},\n",
    "    {\"instruction\": \"Summarize\", \"input\": \"The quick brown fox jumps over the lazy dog\", \"output\": \"A fox jumps over a dog\"},\n",
    "    {\"instruction\": \"Complete the sentence\", \"input\": \"The weather today is\", \"output\": \"The weather today is sunny and warm\"},\n",
    "    {\"instruction\": \"Explain\", \"input\": \"What is AI?\", \"output\": \"AI stands for Artificial Intelligence, which enables machines to learn and perform tasks\"},\n",
    "    {\"instruction\": \"Explain\", \"input\": \"What is Python?\", \"output\": \"Python is a popular programming language known for its simplicity and versatility\"},\n",
    "]\n",
    "\n",
    "# Build tokenizer\n",
    "all_texts = []\n",
    "for item in instruction_data:\n",
    "    all_texts.extend([item['instruction'], item['input'], item['output']])\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=500)\n",
    "tokenizer.build_vocab(all_texts)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer built: {len(tokenizer.token2id)} tokens\")\n",
    "print(f\"‚úÖ Training examples: {len(instruction_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class from Tutorial 3\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict], tokenizer: SimpleTokenizer, max_length: int = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = f\"Instruction: {item['instruction']} Input: {item['input']} Output:\"\n",
    "        completion = item['output']\n",
    "        \n",
    "        prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        completion_ids = self.tokenizer.encode(completion, add_special_tokens=False)\n",
    "        \n",
    "        input_ids = (\n",
    "            [self.tokenizer.token2id[self.tokenizer.bos_token]] +\n",
    "            prompt_ids + completion_ids +\n",
    "            [self.tokenizer.token2id[self.tokenizer.eos_token]]\n",
    "        )\n",
    "        \n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "        \n",
    "        labels = ([-100] * (len(prompt_ids) + 1) + completion_ids + [self.tokenizer.eos_token_id])\n",
    "        if len(labels) > self.max_length:\n",
    "            labels = labels[:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    tokenizer: SimpleTokenizer\n",
    "    max_length: int = 128\n",
    "    \n",
    "    def __call__(self, examples: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        batch_max = min(max(len(ex['input_ids']) for ex in examples), self.max_length)\n",
    "        \n",
    "        input_ids, labels, attention_mask = [], [], []\n",
    "        for ex in examples:\n",
    "            seq_len = len(ex['input_ids'])\n",
    "            padding_len = batch_max - seq_len\n",
    "            \n",
    "            padded_input = torch.cat([\n",
    "                ex['input_ids'],\n",
    "                torch.full((padding_len,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "            input_ids.append(padded_input)\n",
    "            \n",
    "            padded_labels = torch.cat([\n",
    "                ex['labels'],\n",
    "                torch.full((padding_len,), -100, dtype=torch.long)\n",
    "            ])\n",
    "            labels.append(padded_labels)\n",
    "            \n",
    "            mask = torch.cat([\n",
    "                torch.ones(seq_len, dtype=torch.long),\n",
    "                torch.zeros(padding_len, dtype=torch.long)\n",
    "            ])\n",
    "            attention_mask.append(mask)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids),\n",
    "            'labels': torch.stack(labels),\n",
    "            'attention_mask': torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = InstructionDataset(instruction_data, tokenizer, max_length=128)\n",
    "collator = DataCollator(tokenizer, max_length=128)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=collator, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ DataLoader created: {len(train_dataloader)} batches\")\n",
    "\n",
    "# Test batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"  Batch input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  Batch labels shape: {sample_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e055a1a",
   "metadata": {},
   "source": [
    "## 3. Create Model with LoRA ü§ñ\n",
    "\n",
    "We'll create a small Transformer and apply LoRA to its attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small model configuration for Colab\n",
    "config = {\n",
    "    'vocab_size': len(tokenizer.token2id),\n",
    "    'd_model': 128,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 2,\n",
    "    'd_ff': 512,\n",
    "    'dropout': 0.1,\n",
    "    'max_seq_len': 128,\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "model = Transformer(\n",
    "    src_vocab_size=config['vocab_size'],\n",
    "    tgt_vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=config['dropout'],\n",
    "    max_seq_len=config['max_seq_len']\n",
    ").to(device)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Apply LoRA to attention projections\n",
    "lora_rank = 8\n",
    "lora_alpha = 16.0\n",
    "\n",
    "def apply_lora_to_attention(model, rank=8, alpha=16.0):\n",
    "    \"\"\"Replace attention projections with LoRA layers\"\"\"\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'W_q') and isinstance(module.W_q, nn.Linear):\n",
    "            # Replace Q and V projections with LoRA\n",
    "            d_model = module.W_q.in_features\n",
    "            \n",
    "            # Copy weights to LoRA layers\n",
    "            lora_q = LoRALayer(d_model, d_model, rank, alpha)\n",
    "            lora_q.weight.data = module.W_q.weight.data.clone()\n",
    "            if module.W_q.bias is not None:\n",
    "                lora_q.bias.data = module.W_q.bias.data.clone()\n",
    "            module.W_q = lora_q\n",
    "            \n",
    "            lora_v = LoRALayer(d_model, d_model, rank, alpha)\n",
    "            lora_v.weight.data = module.W_v.weight.data.clone()\n",
    "            if module.W_v.bias is not None:\n",
    "                lora_v.bias.data = module.W_v.bias.data.clone()\n",
    "            module.W_v = lora_v\n",
    "\n",
    "apply_lora_to_attention(model, lora_rank, lora_alpha)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model created and LoRA applied!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dd91d",
   "metadata": {},
   "source": [
    "## 4. Training Configuration üéì\n",
    "\n",
    "Set up optimizer, scheduler, and training hyperparameters.\n",
    "\n",
    "### Learning Rate Schedule:\n",
    "\n",
    "We use **warmup + cosine decay** as recommended in the Transformer paper:\n",
    "\n",
    "$$\\text{lr}(t) = \\begin{cases}\n",
    "\\text{lr}_{\\text{max}} \\cdot \\frac{t}{T_{\\text{warmup}}} & t < T_{\\text{warmup}} \\\\\n",
    "\\text{lr}_{\\text{min}} + (\\text{lr}_{\\text{max}} - \\text{lr}_{\\text{min}}) \\cdot \\frac{1 + \\cos(\\pi \\cdot \\frac{t - T_{\\text{warmup}}}{T_{\\text{total}} - T_{\\text{warmup}}})}{2} & t \\geq T_{\\text{warmup}}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 20,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'log_interval': 5,\n",
    "}\n",
    "\n",
    "# Optimizer (only LoRA parameters)\n",
    "optimizer = AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=training_config['learning_rate'],\n",
    "    weight_decay=training_config['weight_decay'],\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + cosine)\n",
    "total_steps = len(train_dataloader) * training_config['num_epochs']\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=training_config['warmup_steps']\n",
    ")\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - training_config['warmup_steps'],\n",
    "    eta_min=training_config['learning_rate'] * 0.1\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[training_config['warmup_steps']]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training configuration:\")\n",
    "print(f\"  Epochs: {training_config['num_epochs']}\")\n",
    "print(f\"  Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"  Warmup steps: {training_config['warmup_steps']}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Gradient accumulation: {training_config['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddeaa7",
   "metadata": {},
   "source": [
    "## 5. Training Loop üöÄ\n",
    "\n",
    "Implement complete training loop with:\n",
    "- Gradient accumulation\n",
    "- Gradient clipping\n",
    "- Learning rate scheduling\n",
    "- Loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfe62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, config, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass (language modeling)\n",
    "        # For decoder-only: use input_ids as both src and tgt\n",
    "        src = input_ids[:, :-1]  # Remove last token\n",
    "        tgt = input_ids[:, :-1]  # Same as src for decoder-only\n",
    "        \n",
    "        logits = model(src, tgt)\n",
    "        \n",
    "        # Compute loss\n",
    "        logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "        labels_flat = labels[:, 1:].reshape(-1)  # Shift labels\n",
    "        \n",
    "        loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=-100)\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / config['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every N steps\n",
    "        if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad],\n",
    "                config['max_grad_norm']\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * config['gradient_accumulation_steps']\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (step + 1) % config['log_interval'] == 0:\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{current_lr:.6f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "history = {'loss': [], 'lr': []}\n",
    "\n",
    "for epoch in range(training_config['num_epochs']):\n",
    "    epoch_loss = train_epoch(model, train_dataloader, optimizer, scheduler, training_config, epoch)\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{training_config['num_epochs']} - Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377eacb",
   "metadata": {},
   "source": [
    "## 6. Visualize Training üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "epochs = range(1, len(history['loss']) + 1)\n",
    "ax1.plot(epochs, history['loss'], marker='o', linewidth=2, markersize=6, color='blue')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "ax2.plot(epochs, history['lr'], marker='o', linewidth=2, markersize=6, color='orange')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.ticklabel_format(axis='y', style='scientific', scilimits=(0,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {history['loss'][-1]:.4f}\")\n",
    "print(f\"Loss reduction: {(1 - history['loss'][-1]/history['loss'][0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727581a",
   "metadata": {},
   "source": [
    "## 7. Generate Text with Fine-Tuned Model üé®\n",
    "\n",
    "Test our fine-tuned model with greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 50,\n",
    "    temperature: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned transformer\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Input prompt\n",
    "        max_length: Maximum generation length\n",
    "        temperature: Sampling temperature (1.0 = greedy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Prepare input\n",
    "            src = input_tensor[:, :-1]\n",
    "            tgt = input_tensor[:, :-1]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(src, tgt)\n",
    "            \n",
    "            # Get next token prediction\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            \n",
    "            # Stop at EOS\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated.append(next_token_id)\n",
    "            input_tensor = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Decode\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test generation on training examples\n",
    "print(\"\\nüé® Testing generation...\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Instruction: Translate to French Input: Hello Output:\",\n",
    "    \"Instruction: Answer the question Input: What is 2+2? Output:\",\n",
    "    \"Instruction: Translate to Spanish Input: Good morning Output:\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=20)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dac4b",
   "metadata": {},
   "source": [
    "## 8. Save and Load Checkpoints üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94826f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_checkpoint(model, optimizer, epoch, loss, path='lora_checkpoint.pt'):\n",
    "    \"\"\"\n",
    "    Save only LoRA parameters (much smaller than full model).\n",
    "    \"\"\"\n",
    "    # Extract LoRA parameters\n",
    "    lora_state_dict = {\n",
    "        k: v for k, v in model.state_dict().items()\n",
    "        if 'lora' in k.lower()\n",
    "    }\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'lora_state_dict': lora_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"‚úÖ Checkpoint saved: {path}\")\n",
    "    \n",
    "    # Print size comparison\n",
    "    full_size = sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024  # MB\n",
    "    lora_size = sum(v.numel() for v in lora_state_dict.values()) * 4 / 1024 / 1024  # MB\n",
    "    print(f\"  Full model: {full_size:.2f} MB\")\n",
    "    print(f\"  LoRA only: {lora_size:.2f} MB ({lora_size/full_size*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def load_lora_checkpoint(model, optimizer, path='lora_checkpoint.pt'):\n",
    "    \"\"\"\n",
    "    Load LoRA parameters from checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Load LoRA parameters (strict=False to ignore missing keys)\n",
    "    model.load_state_dict(checkpoint['lora_state_dict'], strict=False)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint loaded: {path}\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "\n",
    "# Save checkpoint\n",
    "save_lora_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    epoch=training_config['num_epochs'],\n",
    "    loss=history['loss'][-1],\n",
    "    path='lora_instruction_tuned.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47155fdd",
   "metadata": {},
   "source": [
    "## 9. Summary & Best Practices üìù\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ Complete instruction tuning pipeline  \n",
    "‚úÖ Training with LoRA for parameter efficiency  \n",
    "‚úÖ Learning rate scheduling (warmup + cosine)  \n",
    "‚úÖ Gradient accumulation for larger effective batch sizes  \n",
    "‚úÖ Gradient clipping for training stability  \n",
    "‚úÖ Text generation with fine-tuned model  \n",
    "‚úÖ Checkpoint saving (LoRA-only for efficiency)  \n",
    "\n",
    "### Training Best Practices:\n",
    "\n",
    "1. **Learning Rate**\n",
    "   - LoRA: 1e-4 to 1e-3 (higher than full fine-tuning)\n",
    "   - Full fine-tuning: 1e-5 to 5e-5\n",
    "   - Always use warmup (10-20% of training)\n",
    "\n",
    "2. **Batch Size**\n",
    "   - Effective batch size: 16-64 for most tasks\n",
    "   - Use gradient accumulation if limited GPU memory\n",
    "   - Formula: `effective_batch_size = batch_size √ó grad_accum_steps √ó num_gpus`\n",
    "\n",
    "3. **Gradient Clipping**\n",
    "   - Essential for training stability\n",
    "   - Max norm: 0.5-1.0\n",
    "\n",
    "4. **Checkpointing**\n",
    "   - Save LoRA weights only (much smaller)\n",
    "   - Save every N epochs or best validation loss\n",
    "   - Include optimizer state for resuming training\n",
    "\n",
    "5. **Monitoring**\n",
    "   - Track loss, learning rate, gradient norms\n",
    "   - Use TensorBoard or Weights & Biases\n",
    "   - Validate on held-out set regularly\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "- **Loss not decreasing**: Learning rate too high/low, check data quality\n",
    "- **NaN loss**: Gradient exploding (use gradient clipping), learning rate too high\n",
    "- **Overfitting**: Reduce model size, increase regularization, add more data\n",
    "- **OOM errors**: Reduce batch size, enable gradient checkpointing, use smaller model\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Tutorial 5**: Evaluation metrics and model assessment\n",
    "- Scale to larger models (7B, 13B) with DeepSpeed/FSDP\n",
    "- Try different LoRA configurations\n",
    "- Experiment with QLoRA for even lower memory\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "**Papers:**\n",
    "- [papers/DeepSeek-R1-paper.pdf](../papers/DeepSeek-R1-paper.pdf) - Training methodology\n",
    "- Instruction Tuning: https://arxiv.org/abs/2109.01652\n",
    "- LoRA: https://arxiv.org/abs/2106.09685\n",
    "\n",
    "**Code:**\n",
    "- Our implementation: [src/transformer.py](../src/transformer.py)\n",
    "- Hugging Face Transformers: https://github.com/huggingface/transformers\n",
    "- Hugging Face PEFT: https://github.com/huggingface/peft\n",
    "\n",
    "**Related:**\n",
    "- [transformer-foundation/06_complete_transformer.ipynb](../transformer-foundation/06_complete_transformer.ipynb)\n",
    "- [02_lora_implementation.ipynb](02_lora_implementation.ipynb)\n",
    "- [03_data_preparation.ipynb](03_data_preparation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to evaluate your model? Continue to Tutorial 5! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
